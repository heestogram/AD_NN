{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===============================\n",
    "# 0) simplex vertices \n",
    "# ===============================\n",
    "def simplex_vertices(K: int) -> torch.Tensor:\n",
    "    w_list = []\n",
    "    for j in range(1, K + 1):\n",
    "        if j == 1:\n",
    "\n",
    "            w_j = (1.0 / (K - 1) ** 0.5) * torch.ones(K - 1)\n",
    "        else:\n",
    "\n",
    "            term1 = -((1.0 + K ** 0.5) / ((K - 1) ** 1.5)) * torch.ones(K - 1)\n",
    "            e = torch.zeros(K - 1)\n",
    "            e[j - 2] = (K / (K - 1)) ** 0.5\n",
    "            w_j = term1 + e\n",
    "        w_list.append(w_j)\n",
    "    return torch.stack(w_list)  # (K, K-1)\n",
    "\n",
    "# ===============================\n",
    "# 1) 시나리오별 delta 정의\n",
    "# ===============================\n",
    "def delta_scenario(X, scenario, K):\n",
    "    \"\"\"\n",
    "    X: (n, p) with columns X1,...,Xp\n",
    "    return: deltas (n, K=4)   each column k = δ_k(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, p = X.shape\n",
    "    d = np.zeros((n, K))\n",
    "    x1, x2, x3, x4 = X[:,0], X[:,1], X[:,2], X[:,3]\n",
    "    # 여분 변수도 쓸 수 있음\n",
    "    if p >= 6:\n",
    "        x5, x6 = X[:,4], X[:,5]\n",
    "    else:\n",
    "        x5 = np.zeros(n); x6 = np.zeros(n)\n",
    "\n",
    "    if scenario == 1:\n",
    "        # Linear (논문 식)\n",
    "        d[:,0] = 1 + x1 + x2 + x3 + x4           \n",
    "        d[:,1] = 1 + x1 - x2 - x3 + x4            \n",
    "        d[:,2] = 1 + x1 - x2 + x3 - x4           \n",
    "        d[:,3] = 1 - x1 - x2 + x3 + x4         \n",
    "\n",
    "    elif scenario == 2:\n",
    "        # Tree type (indicator)\n",
    "        d[:,0] = 3.0 * ((x1 <= 0.5).astype(float) * ((x2 > -0.6).astype(float) - 1.0))                      \n",
    "        d[:,1] = ((x3 <= 1.0).astype(float)) * (2.0*(x4 <= -0.3).astype(float) - 1.0)                     \n",
    "        d[:,2] = (4.0*(x5 <= 0.0).astype(float) - 2.0)                                           \n",
    "        d[:,3] = (4.0*(x6 <= 0.0).astype(float) - 2.0)                                                            \n",
    "\n",
    "    elif scenario == 3:\n",
    "        # Poly type (두 변수 +, 두 변수 - : 자연스러운 순환 패턴)\n",
    "        d[:,0] = 0.2 + x1**2 + x2**2 - x3**2 - x4**2      \n",
    "        d[:,1] = 0.2 + x2**2 + x3**2 - x1**2 - x4**2   \n",
    "        d[:,2] = 0.2 + x3**2 + x4**2 - x1**2 - x2**2      \n",
    "        d[:,3] = 0.2 + x1**2 + x4**2 - x2**2 - x3**2   \n",
    "    else:\n",
    "        raise ValueError(\"scenario must be 1, 2, or 3\")\n",
    "\n",
    "    return d  # (n,4)\n",
    "\n",
    "# ===============================\n",
    "# 2) 데이터 생성\n",
    "# ===============================\n",
    "\n",
    "def mu_default(X):\n",
    "    \"\"\"\n",
    "    논문 기본설정: μ(x) = 1 + X1 + X2  (X는 (n,p), X1=X[:,0], X2=X[:,1])\n",
    "    나머지 시나리오에서도 mu가 동일하게 적용되는지 모르겠음. appendix에서 알려준대놓고 안 알려줌;;\n",
    "    \"\"\"\n",
    "    return 1.0 + X[:, 0] + X[:, 1]\n",
    "\n",
    "def mu_poly(X):\n",
    "    x1, x2 = X[:,0], X[:,1]\n",
    "    return 1.0 + 0.6*x1 + 0.6*x2 + 0.4*(x1**2) - 0.3*(x2**2) + 0.2*(x1*x2)\n",
    "\n",
    "\n",
    "def simulate_data(n=2000, p=10, K=4, scenario=1, seed=0,\n",
    "                  main_effect=mu_default, sigma_eps=1.0, uniform_low=-1.0, uniform_high=1.0):\n",
    "    \"\"\"\n",
    "    논문 5.1과 동일한 형태:\n",
    "      - X_ij ~ Uniform[-1,1]\n",
    "      - ε_i ~ N(0, 1) (sigma_eps로 조정 가능)\n",
    "      - R = μ(X) + δ_A(X) + ε\n",
    "    반환: X, A, R, true_opt  (true_opt은 argmax_k δ_k(X))\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # X ~ Unif[-1,1]\n",
    "    X = rng.uniform(uniform_low, uniform_high, size=(n, p))\n",
    "\n",
    "    # δ_k(x)들 (n,K)\n",
    "    deltas = delta_scenario(X, scenario, K=K) \n",
    "\n",
    "    mu = main_effect(X)                     \n",
    "\n",
    "    # 치료법은 무작위배정\n",
    "    A = rng.integers(0, K, size=n)\n",
    "\n",
    "    eps = rng.normal(0.0, sigma_eps, size=n)\n",
    "    R = mu + deltas[np.arange(n), A] + eps\n",
    "\n",
    "    # 최적 치료 (Ground truth): main effect 제외\n",
    "    true_opt = np.argmax(deltas, axis=1)\n",
    "\n",
    "    return X, A, R, true_opt\n",
    "\n",
    "# ===============================\n",
    "# 3) 선형 AD-learning (원 논문식)\n",
    "# ===============================\n",
    "def ad_linear(X, A, R, K=4, alpha=1.0):\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).numpy()                   # (K, K-1)\n",
    "    Y = np.zeros((n, K-1))\n",
    "    for i in range(n):\n",
    "        Y[i,:] = K * R[i] * V[A[i]]                   # 반응: K * R * w_A\n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=False)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    def predict(X_new):\n",
    "        f = model.predict(X_new)                      # (m, K-1)\n",
    "        scores = f @ V.T                              # (m, K)\n",
    "        return scores.argmax(axis=1)\n",
    "    return model, V, predict\n",
    "\n",
    "# ===============================\n",
    "# 4) 신경망 AD-learning \n",
    "# ===============================\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, p, out_dim, hidden=[128,128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = p\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def ad_nn(X, A, R, K=4, epochs=60, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).to(device)                # (K, K-1)\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    A_t = torch.tensor(A, dtype=torch.long, device=device)\n",
    "    R_t = torch.tensor(R, dtype=torch.float32, device=device)\n",
    "\n",
    "    model = ADNet(p, K-1).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        f = model(X_t)                                # (n, K-1)\n",
    "        wA = V[A_t]                                   # (n, K-1)\n",
    "        pred = (f*wA).sum(1)                          # <f, w_A>\n",
    "        loss = ((R_t - pred)**2).mean()\n",
    "        loss.backward(); opt.step()\n",
    "        if (ep+1) % 10 == 0:\n",
    "            print(f\"[NN] epoch {ep+1}/{epochs} loss={loss.item():.4f}\")\n",
    "\n",
    "    def predict(X_new):\n",
    "        X_new_t = torch.tensor(X_new, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            f = model(X_new_t)                        # (m, K-1)\n",
    "            scores = f @ V.T                          # (m, K)\n",
    "            return scores.argmax(1).cpu().numpy()\n",
    "    return model, V.cpu().numpy(), predict\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5) 평가 (Error rate & IPW Value)\n",
    "# ===============================\n",
    "def evaluate_policy(predict_fn, X, A, R, K, true_opt=None):\n",
    "    pred = predict_fn(X)\n",
    "    error_rate = None if true_opt is None else np.mean(pred != true_opt)\n",
    "    # IPW value (균등배정 → P(A|X)=1/K)\n",
    "    value = np.mean(R * (pred == A) * K)\n",
    "    return error_rate, value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(scenario, seed, K=4, p=10, n=2000, nn_epochs=60, nn_lr=1e-3):\n",
    "    # 재현성\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # 데이터 생성 & 분할\n",
    "    X, A, R, opt = simulate_data(n=n, p=p, K=K, scenario=scenario, seed=seed)\n",
    "    X_tr, X_te, A_tr, A_te, R_tr, R_te, opt_tr, opt_te = \\\n",
    "        train_test_split(X, A, R, opt, test_size=0.3, random_state=seed+999)\n",
    "\n",
    "    # Linear AD\n",
    "    _, _, lin_pred = ad_linear(X_tr, A_tr, R_tr, K=K, alpha=1.0)\n",
    "    err_lin, val_lin = evaluate_policy(lin_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "    # NN AD\n",
    "    _, _, nn_pred = ad_nn(X_tr, A_tr, R_tr, K=K, epochs=nn_epochs, lr=nn_lr)\n",
    "    err_nn, val_nn = evaluate_policy(nn_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "    return (err_lin, val_lin, err_nn, val_nn)\n",
    "\n",
    "def mean_var(xs):\n",
    "    xs = np.asarray(xs, dtype=float)\n",
    "    mean = xs.mean()\n",
    "    var = xs.var(ddof=1) if xs.size > 1 else 0.0\n",
    "    return mean, var\n",
    "\n",
    "def run_experiments(n_trials=20, scenarios=(1,2,3), base_seed=12345, K=4, p=10, n=2000,\n",
    "                    nn_epochs=60, nn_lr=1e-3):\n",
    "    results = {}  # scenario -> dict of lists\n",
    "    for sc in scenarios:\n",
    "        lin_errs, lin_vals, nn_errs, nn_vals = [], [], [], []\n",
    "        for t in range(n_trials):\n",
    "            seed = base_seed + 1000*sc + t  # 시나리오별로 다른 시드 스트림\n",
    "            e_lin, v_lin, e_nn, v_nn = run_once(sc, seed, K=K, p=p, n=n,\n",
    "                                                nn_epochs=nn_epochs, nn_lr=nn_lr)\n",
    "            lin_errs.append(e_lin); lin_vals.append(v_lin)\n",
    "            nn_errs.append(e_nn);  nn_vals.append(v_nn)\n",
    "\n",
    "        \n",
    "        lin_err_mean, lin_err_var = mean_var(lin_errs)\n",
    "        lin_val_mean, lin_val_var = mean_var(lin_vals)\n",
    "        nn_err_mean,  nn_err_var  = mean_var(nn_errs)\n",
    "        nn_val_mean,  nn_val_var  = mean_var(nn_vals)\n",
    "\n",
    "        results[sc] = {\n",
    "            \"lin_err\": lin_errs, \"lin_val\": lin_vals,\n",
    "            \"nn_err\": nn_errs,   \"nn_val\": nn_vals,\n",
    "            \"summary\": {\n",
    "                \"Linear\": {\"Error_mean\": lin_err_mean, \"Error_var\": lin_err_var,\n",
    "                           \"Value_mean\": lin_val_mean, \"Value_var\": lin_val_var},\n",
    "                \"NN\":     {\"Error_mean\": nn_err_mean,  \"Error_var\": nn_err_var,\n",
    "                           \"Value_mean\": nn_val_mean,  \"Value_var\": nn_val_var},\n",
    "            }\n",
    "        }\n",
    "\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*12 + f\" Scenario {sc} ({n_trials} runs) \" + \"=\"*12)\n",
    "        print(f\"[Linear-AD]  Error mean={lin_err_mean:.4f}, var={lin_err_var:.6f} | \"\n",
    "              f\"Value mean={lin_val_mean:.4f}, var={lin_val_var:.6f}\")\n",
    "        print(f\"[NN-AD]      Error mean={nn_err_mean:.4f}, var={nn_err_var:.6f} | \"\n",
    "              f\"Value mean={nn_val_mean:.4f}, var={nn_val_var:.6f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] epoch 10/60 loss=4.8385\n",
      "[NN] epoch 20/60 loss=4.4549\n",
      "[NN] epoch 30/60 loss=4.1032\n",
      "[NN] epoch 40/60 loss=3.7122\n",
      "[NN] epoch 50/60 loss=3.3597\n",
      "[NN] epoch 60/60 loss=3.0563\n",
      "[NN] epoch 10/60 loss=4.6813\n",
      "[NN] epoch 20/60 loss=4.3766\n",
      "[NN] epoch 30/60 loss=4.1076\n",
      "[NN] epoch 40/60 loss=3.7993\n",
      "[NN] epoch 50/60 loss=3.4915\n",
      "[NN] epoch 60/60 loss=3.2016\n",
      "[NN] epoch 10/60 loss=4.9224\n",
      "[NN] epoch 20/60 loss=4.5766\n",
      "[NN] epoch 30/60 loss=4.2588\n",
      "[NN] epoch 40/60 loss=3.9042\n",
      "[NN] epoch 50/60 loss=3.5835\n",
      "[NN] epoch 60/60 loss=3.3006\n",
      "[NN] epoch 10/60 loss=4.8184\n",
      "[NN] epoch 20/60 loss=4.4834\n",
      "[NN] epoch 30/60 loss=4.1283\n",
      "[NN] epoch 40/60 loss=3.7371\n",
      "[NN] epoch 50/60 loss=3.3809\n",
      "[NN] epoch 60/60 loss=3.0856\n",
      "[NN] epoch 10/60 loss=4.7564\n",
      "[NN] epoch 20/60 loss=4.3786\n",
      "[NN] epoch 30/60 loss=4.0659\n",
      "[NN] epoch 40/60 loss=3.7597\n",
      "[NN] epoch 50/60 loss=3.4818\n",
      "[NN] epoch 60/60 loss=3.2346\n",
      "[NN] epoch 10/60 loss=4.8220\n",
      "[NN] epoch 20/60 loss=4.4792\n",
      "[NN] epoch 30/60 loss=4.1031\n",
      "[NN] epoch 40/60 loss=3.7017\n",
      "[NN] epoch 50/60 loss=3.3659\n",
      "[NN] epoch 60/60 loss=3.0835\n",
      "[NN] epoch 10/60 loss=4.6666\n",
      "[NN] epoch 20/60 loss=4.3902\n",
      "[NN] epoch 30/60 loss=4.1169\n",
      "[NN] epoch 40/60 loss=3.7992\n",
      "[NN] epoch 50/60 loss=3.4687\n",
      "[NN] epoch 60/60 loss=3.1624\n",
      "[NN] epoch 10/60 loss=5.0400\n",
      "[NN] epoch 20/60 loss=4.6445\n",
      "[NN] epoch 30/60 loss=4.3203\n",
      "[NN] epoch 40/60 loss=3.9905\n",
      "[NN] epoch 50/60 loss=3.6980\n",
      "[NN] epoch 60/60 loss=3.4198\n",
      "[NN] epoch 10/60 loss=4.5393\n",
      "[NN] epoch 20/60 loss=4.2008\n",
      "[NN] epoch 30/60 loss=3.8696\n",
      "[NN] epoch 40/60 loss=3.5152\n",
      "[NN] epoch 50/60 loss=3.1836\n",
      "[NN] epoch 60/60 loss=2.8891\n",
      "[NN] epoch 10/60 loss=4.5967\n",
      "[NN] epoch 20/60 loss=4.2785\n",
      "[NN] epoch 30/60 loss=3.9480\n",
      "[NN] epoch 40/60 loss=3.6232\n",
      "[NN] epoch 50/60 loss=3.3261\n",
      "[NN] epoch 60/60 loss=3.0444\n",
      "[NN] epoch 10/60 loss=4.7407\n",
      "[NN] epoch 20/60 loss=4.4256\n",
      "[NN] epoch 30/60 loss=4.1283\n",
      "[NN] epoch 40/60 loss=3.8221\n",
      "[NN] epoch 50/60 loss=3.5300\n",
      "[NN] epoch 60/60 loss=3.2421\n",
      "[NN] epoch 10/60 loss=5.0744\n",
      "[NN] epoch 20/60 loss=4.7233\n",
      "[NN] epoch 30/60 loss=4.3896\n",
      "[NN] epoch 40/60 loss=4.0286\n",
      "[NN] epoch 50/60 loss=3.7000\n",
      "[NN] epoch 60/60 loss=3.4064\n",
      "[NN] epoch 10/60 loss=4.8770\n",
      "[NN] epoch 20/60 loss=4.5208\n",
      "[NN] epoch 30/60 loss=4.1711\n",
      "[NN] epoch 40/60 loss=3.7943\n",
      "[NN] epoch 50/60 loss=3.4548\n",
      "[NN] epoch 60/60 loss=3.1461\n",
      "[NN] epoch 10/60 loss=5.0256\n",
      "[NN] epoch 20/60 loss=4.6433\n",
      "[NN] epoch 30/60 loss=4.2311\n",
      "[NN] epoch 40/60 loss=3.8224\n",
      "[NN] epoch 50/60 loss=3.4653\n",
      "[NN] epoch 60/60 loss=3.1462\n",
      "[NN] epoch 10/60 loss=4.6875\n",
      "[NN] epoch 20/60 loss=4.3629\n",
      "[NN] epoch 30/60 loss=4.0260\n",
      "[NN] epoch 40/60 loss=3.6582\n",
      "[NN] epoch 50/60 loss=3.3519\n",
      "[NN] epoch 60/60 loss=3.1030\n",
      "[NN] epoch 10/60 loss=5.0104\n",
      "[NN] epoch 20/60 loss=4.6104\n",
      "[NN] epoch 30/60 loss=4.2740\n",
      "[NN] epoch 40/60 loss=3.8959\n",
      "[NN] epoch 50/60 loss=3.5329\n",
      "[NN] epoch 60/60 loss=3.2298\n",
      "[NN] epoch 10/60 loss=5.0479\n",
      "[NN] epoch 20/60 loss=4.7374\n",
      "[NN] epoch 30/60 loss=4.3944\n",
      "[NN] epoch 40/60 loss=3.9852\n",
      "[NN] epoch 50/60 loss=3.5771\n",
      "[NN] epoch 60/60 loss=3.2132\n",
      "[NN] epoch 10/60 loss=4.8245\n",
      "[NN] epoch 20/60 loss=4.5180\n",
      "[NN] epoch 30/60 loss=4.2177\n",
      "[NN] epoch 40/60 loss=3.8797\n",
      "[NN] epoch 50/60 loss=3.5649\n",
      "[NN] epoch 60/60 loss=3.2973\n",
      "[NN] epoch 10/60 loss=4.9856\n",
      "[NN] epoch 20/60 loss=4.6947\n",
      "[NN] epoch 30/60 loss=4.3707\n",
      "[NN] epoch 40/60 loss=4.0190\n",
      "[NN] epoch 50/60 loss=3.6836\n",
      "[NN] epoch 60/60 loss=3.3666\n",
      "[NN] epoch 10/60 loss=5.0521\n",
      "[NN] epoch 20/60 loss=4.6761\n",
      "[NN] epoch 30/60 loss=4.2699\n",
      "[NN] epoch 40/60 loss=3.8369\n",
      "[NN] epoch 50/60 loss=3.4905\n",
      "[NN] epoch 60/60 loss=3.2081\n",
      "\n",
      "============ Scenario 1 (20 runs) ============\n",
      "[Linear-AD]  Error mean=0.4423, var=0.004715 | Value mean=2.6150, var=0.268899\n",
      "[NN-AD]      Error mean=0.3658, var=0.003239 | Value mean=2.7846, var=0.254796\n",
      "[NN] epoch 10/60 loss=3.4678\n",
      "[NN] epoch 20/60 loss=3.1823\n",
      "[NN] epoch 30/60 loss=2.8364\n",
      "[NN] epoch 40/60 loss=2.5032\n",
      "[NN] epoch 50/60 loss=2.2428\n",
      "[NN] epoch 60/60 loss=2.0219\n",
      "[NN] epoch 10/60 loss=3.9604\n",
      "[NN] epoch 20/60 loss=3.6187\n",
      "[NN] epoch 30/60 loss=3.1999\n",
      "[NN] epoch 40/60 loss=2.7699\n",
      "[NN] epoch 50/60 loss=2.4419\n",
      "[NN] epoch 60/60 loss=2.1900\n",
      "[NN] epoch 10/60 loss=3.7533\n",
      "[NN] epoch 20/60 loss=3.3917\n",
      "[NN] epoch 30/60 loss=2.9722\n",
      "[NN] epoch 40/60 loss=2.6455\n",
      "[NN] epoch 50/60 loss=2.4346\n",
      "[NN] epoch 60/60 loss=2.2496\n",
      "[NN] epoch 10/60 loss=4.3280\n",
      "[NN] epoch 20/60 loss=3.9541\n",
      "[NN] epoch 30/60 loss=3.5455\n",
      "[NN] epoch 40/60 loss=3.1644\n",
      "[NN] epoch 50/60 loss=2.8675\n",
      "[NN] epoch 60/60 loss=2.6282\n",
      "[NN] epoch 10/60 loss=3.6265\n",
      "[NN] epoch 20/60 loss=3.3198\n",
      "[NN] epoch 30/60 loss=2.9446\n",
      "[NN] epoch 40/60 loss=2.5764\n",
      "[NN] epoch 50/60 loss=2.2664\n",
      "[NN] epoch 60/60 loss=2.0137\n",
      "[NN] epoch 10/60 loss=3.9405\n",
      "[NN] epoch 20/60 loss=3.6064\n",
      "[NN] epoch 30/60 loss=3.2127\n",
      "[NN] epoch 40/60 loss=2.8707\n",
      "[NN] epoch 50/60 loss=2.6238\n",
      "[NN] epoch 60/60 loss=2.4071\n",
      "[NN] epoch 10/60 loss=3.6257\n",
      "[NN] epoch 20/60 loss=3.3069\n",
      "[NN] epoch 30/60 loss=2.9298\n",
      "[NN] epoch 40/60 loss=2.6126\n",
      "[NN] epoch 50/60 loss=2.3855\n",
      "[NN] epoch 60/60 loss=2.1934\n",
      "[NN] epoch 10/60 loss=3.6243\n",
      "[NN] epoch 20/60 loss=3.3516\n",
      "[NN] epoch 30/60 loss=3.0023\n",
      "[NN] epoch 40/60 loss=2.6302\n",
      "[NN] epoch 50/60 loss=2.3317\n",
      "[NN] epoch 60/60 loss=2.0926\n",
      "[NN] epoch 10/60 loss=3.5328\n",
      "[NN] epoch 20/60 loss=3.2292\n",
      "[NN] epoch 30/60 loss=2.8432\n",
      "[NN] epoch 40/60 loss=2.4648\n",
      "[NN] epoch 50/60 loss=2.1856\n",
      "[NN] epoch 60/60 loss=1.9718\n",
      "[NN] epoch 10/60 loss=4.2753\n",
      "[NN] epoch 20/60 loss=3.9487\n",
      "[NN] epoch 30/60 loss=3.5490\n",
      "[NN] epoch 40/60 loss=3.1495\n",
      "[NN] epoch 50/60 loss=2.8320\n",
      "[NN] epoch 60/60 loss=2.5665\n",
      "[NN] epoch 10/60 loss=3.9549\n",
      "[NN] epoch 20/60 loss=3.6209\n",
      "[NN] epoch 30/60 loss=3.1968\n",
      "[NN] epoch 40/60 loss=2.7901\n",
      "[NN] epoch 50/60 loss=2.5009\n",
      "[NN] epoch 60/60 loss=2.2726\n",
      "[NN] epoch 10/60 loss=3.9300\n",
      "[NN] epoch 20/60 loss=3.5466\n",
      "[NN] epoch 30/60 loss=3.1114\n",
      "[NN] epoch 40/60 loss=2.7420\n",
      "[NN] epoch 50/60 loss=2.5108\n",
      "[NN] epoch 60/60 loss=2.3118\n",
      "[NN] epoch 10/60 loss=4.1300\n",
      "[NN] epoch 20/60 loss=3.7912\n",
      "[NN] epoch 30/60 loss=3.3733\n",
      "[NN] epoch 40/60 loss=2.9325\n",
      "[NN] epoch 50/60 loss=2.5807\n",
      "[NN] epoch 60/60 loss=2.3143\n",
      "[NN] epoch 10/60 loss=3.5491\n",
      "[NN] epoch 20/60 loss=3.2429\n",
      "[NN] epoch 30/60 loss=2.8531\n",
      "[NN] epoch 40/60 loss=2.4566\n",
      "[NN] epoch 50/60 loss=2.1831\n",
      "[NN] epoch 60/60 loss=1.9791\n",
      "[NN] epoch 10/60 loss=4.0779\n",
      "[NN] epoch 20/60 loss=3.7890\n",
      "[NN] epoch 30/60 loss=3.4349\n",
      "[NN] epoch 40/60 loss=3.0628\n",
      "[NN] epoch 50/60 loss=2.7356\n",
      "[NN] epoch 60/60 loss=2.4521\n",
      "[NN] epoch 10/60 loss=3.4502\n",
      "[NN] epoch 20/60 loss=3.1829\n",
      "[NN] epoch 30/60 loss=2.8656\n",
      "[NN] epoch 40/60 loss=2.5600\n",
      "[NN] epoch 50/60 loss=2.3253\n",
      "[NN] epoch 60/60 loss=2.1217\n",
      "[NN] epoch 10/60 loss=3.7902\n",
      "[NN] epoch 20/60 loss=3.4288\n",
      "[NN] epoch 30/60 loss=2.9772\n",
      "[NN] epoch 40/60 loss=2.5642\n",
      "[NN] epoch 50/60 loss=2.3150\n",
      "[NN] epoch 60/60 loss=2.1291\n",
      "[NN] epoch 10/60 loss=3.7935\n",
      "[NN] epoch 20/60 loss=3.5008\n",
      "[NN] epoch 30/60 loss=3.1471\n",
      "[NN] epoch 40/60 loss=2.7796\n",
      "[NN] epoch 50/60 loss=2.4757\n",
      "[NN] epoch 60/60 loss=2.2419\n",
      "[NN] epoch 10/60 loss=3.5818\n",
      "[NN] epoch 20/60 loss=3.2949\n",
      "[NN] epoch 30/60 loss=2.9321\n",
      "[NN] epoch 40/60 loss=2.5799\n",
      "[NN] epoch 50/60 loss=2.3432\n",
      "[NN] epoch 60/60 loss=2.1422\n",
      "[NN] epoch 10/60 loss=3.8501\n",
      "[NN] epoch 20/60 loss=3.5265\n",
      "[NN] epoch 30/60 loss=3.1347\n",
      "[NN] epoch 40/60 loss=2.7902\n",
      "[NN] epoch 50/60 loss=2.5315\n",
      "[NN] epoch 60/60 loss=2.3135\n",
      "\n",
      "============ Scenario 2 (20 runs) ============\n",
      "[Linear-AD]  Error mean=0.4794, var=0.004581 | Value mean=2.1108, var=0.270975\n",
      "[NN-AD]      Error mean=0.4787, var=0.004775 | Value mean=2.1354, var=0.281555\n",
      "[NN] epoch 10/60 loss=3.0202\n",
      "[NN] epoch 20/60 loss=2.8398\n",
      "[NN] epoch 30/60 loss=2.6403\n",
      "[NN] epoch 40/60 loss=2.4363\n",
      "[NN] epoch 50/60 loss=2.2222\n",
      "[NN] epoch 60/60 loss=1.9892\n",
      "[NN] epoch 10/60 loss=2.9459\n",
      "[NN] epoch 20/60 loss=2.8115\n",
      "[NN] epoch 30/60 loss=2.6393\n",
      "[NN] epoch 40/60 loss=2.4304\n",
      "[NN] epoch 50/60 loss=2.1888\n",
      "[NN] epoch 60/60 loss=1.9191\n",
      "[NN] epoch 10/60 loss=3.1206\n",
      "[NN] epoch 20/60 loss=2.9586\n",
      "[NN] epoch 30/60 loss=2.7683\n",
      "[NN] epoch 40/60 loss=2.5522\n",
      "[NN] epoch 50/60 loss=2.3187\n",
      "[NN] epoch 60/60 loss=2.0707\n",
      "[NN] epoch 10/60 loss=2.8913\n",
      "[NN] epoch 20/60 loss=2.7339\n",
      "[NN] epoch 30/60 loss=2.5600\n",
      "[NN] epoch 40/60 loss=2.3631\n",
      "[NN] epoch 50/60 loss=2.1441\n",
      "[NN] epoch 60/60 loss=1.9075\n",
      "[NN] epoch 10/60 loss=3.1565\n",
      "[NN] epoch 20/60 loss=3.0108\n",
      "[NN] epoch 30/60 loss=2.8399\n",
      "[NN] epoch 40/60 loss=2.6403\n",
      "[NN] epoch 50/60 loss=2.4112\n",
      "[NN] epoch 60/60 loss=2.1493\n",
      "[NN] epoch 10/60 loss=3.2543\n",
      "[NN] epoch 20/60 loss=3.0874\n",
      "[NN] epoch 30/60 loss=2.8861\n",
      "[NN] epoch 40/60 loss=2.6556\n",
      "[NN] epoch 50/60 loss=2.4022\n",
      "[NN] epoch 60/60 loss=2.1241\n",
      "[NN] epoch 10/60 loss=2.6693\n",
      "[NN] epoch 20/60 loss=2.5210\n",
      "[NN] epoch 30/60 loss=2.3489\n",
      "[NN] epoch 40/60 loss=2.1677\n",
      "[NN] epoch 50/60 loss=1.9763\n",
      "[NN] epoch 60/60 loss=1.7684\n",
      "[NN] epoch 10/60 loss=3.0513\n",
      "[NN] epoch 20/60 loss=2.8962\n",
      "[NN] epoch 30/60 loss=2.7065\n",
      "[NN] epoch 40/60 loss=2.4895\n",
      "[NN] epoch 50/60 loss=2.2535\n",
      "[NN] epoch 60/60 loss=1.9997\n",
      "[NN] epoch 10/60 loss=2.9418\n",
      "[NN] epoch 20/60 loss=2.7855\n",
      "[NN] epoch 30/60 loss=2.5891\n",
      "[NN] epoch 40/60 loss=2.3559\n",
      "[NN] epoch 50/60 loss=2.1049\n",
      "[NN] epoch 60/60 loss=1.8441\n",
      "[NN] epoch 10/60 loss=3.1838\n",
      "[NN] epoch 20/60 loss=3.0252\n",
      "[NN] epoch 30/60 loss=2.8382\n",
      "[NN] epoch 40/60 loss=2.6330\n",
      "[NN] epoch 50/60 loss=2.4175\n",
      "[NN] epoch 60/60 loss=2.1799\n",
      "[NN] epoch 10/60 loss=3.0099\n",
      "[NN] epoch 20/60 loss=2.8520\n",
      "[NN] epoch 30/60 loss=2.6718\n",
      "[NN] epoch 40/60 loss=2.4687\n",
      "[NN] epoch 50/60 loss=2.2466\n",
      "[NN] epoch 60/60 loss=2.0013\n",
      "[NN] epoch 10/60 loss=3.2478\n",
      "[NN] epoch 20/60 loss=3.0893\n",
      "[NN] epoch 30/60 loss=2.8964\n",
      "[NN] epoch 40/60 loss=2.6712\n",
      "[NN] epoch 50/60 loss=2.4203\n",
      "[NN] epoch 60/60 loss=2.1486\n",
      "[NN] epoch 10/60 loss=3.0117\n",
      "[NN] epoch 20/60 loss=2.8750\n",
      "[NN] epoch 30/60 loss=2.7081\n",
      "[NN] epoch 40/60 loss=2.5170\n",
      "[NN] epoch 50/60 loss=2.3065\n",
      "[NN] epoch 60/60 loss=2.0717\n",
      "[NN] epoch 10/60 loss=2.9662\n",
      "[NN] epoch 20/60 loss=2.8005\n",
      "[NN] epoch 30/60 loss=2.5989\n",
      "[NN] epoch 40/60 loss=2.3688\n",
      "[NN] epoch 50/60 loss=2.1273\n",
      "[NN] epoch 60/60 loss=1.8765\n",
      "[NN] epoch 10/60 loss=2.9050\n",
      "[NN] epoch 20/60 loss=2.7653\n",
      "[NN] epoch 30/60 loss=2.5988\n",
      "[NN] epoch 40/60 loss=2.4112\n",
      "[NN] epoch 50/60 loss=2.2026\n",
      "[NN] epoch 60/60 loss=1.9671\n",
      "[NN] epoch 10/60 loss=2.8507\n",
      "[NN] epoch 20/60 loss=2.6916\n",
      "[NN] epoch 30/60 loss=2.5023\n",
      "[NN] epoch 40/60 loss=2.2944\n",
      "[NN] epoch 50/60 loss=2.0682\n",
      "[NN] epoch 60/60 loss=1.8270\n",
      "[NN] epoch 10/60 loss=3.1832\n",
      "[NN] epoch 20/60 loss=3.0109\n",
      "[NN] epoch 30/60 loss=2.8140\n",
      "[NN] epoch 40/60 loss=2.5984\n",
      "[NN] epoch 50/60 loss=2.3663\n",
      "[NN] epoch 60/60 loss=2.1102\n",
      "[NN] epoch 10/60 loss=3.1409\n",
      "[NN] epoch 20/60 loss=2.9727\n",
      "[NN] epoch 30/60 loss=2.7820\n",
      "[NN] epoch 40/60 loss=2.5659\n",
      "[NN] epoch 50/60 loss=2.3345\n",
      "[NN] epoch 60/60 loss=2.0853\n",
      "[NN] epoch 10/60 loss=2.6321\n",
      "[NN] epoch 20/60 loss=2.4813\n",
      "[NN] epoch 30/60 loss=2.3060\n",
      "[NN] epoch 40/60 loss=2.1116\n",
      "[NN] epoch 50/60 loss=1.9013\n",
      "[NN] epoch 60/60 loss=1.6764\n",
      "[NN] epoch 10/60 loss=2.8444\n",
      "[NN] epoch 20/60 loss=2.7061\n",
      "[NN] epoch 30/60 loss=2.5491\n",
      "[NN] epoch 40/60 loss=2.3714\n",
      "[NN] epoch 50/60 loss=2.1688\n",
      "[NN] epoch 60/60 loss=1.9417\n",
      "\n",
      "============ Scenario 3 (20 runs) ============\n",
      "[Linear-AD]  Error mean=0.8488, var=0.001305 | Value mean=1.0216, var=0.089555\n",
      "[NN-AD]      Error mean=0.8063, var=0.002070 | Value mean=1.0977, var=0.088823\n"
     ]
    }
   ],
   "source": [
    "# 스크립트로 실행 시\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_experiments(\n",
    "        n_trials=20,\n",
    "        scenarios=(1,2,3),\n",
    "        base_seed=12345,\n",
    "        K=7, p=10, n=800,\n",
    "        nn_epochs=60, nn_lr=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이스라인 벡터(KRW) 손실함수 vs 신경망 벡터(KRW) 손실함수\n",
    "\n",
    "이게 정석이긴 한데, 결과는 좀 잘 안나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ad_nn_vector_target(X, A, R, K=4, epochs=60, lr=1e-3, hidden=[128,128]):\n",
    "    \"\"\"\n",
    "    AD-NN (vector regression version, target = K*R*w_A)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n, p = X.shape\n",
    "    V = torch.tensor(simplex_vertices(K), dtype=torch.float32, device=device)  # (K, K-1)\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    A_t = torch.tensor(A, dtype=torch.long, device=device)\n",
    "    R_t = torch.tensor(R, dtype=torch.float32, device=device)\n",
    "\n",
    "    # 타깃: Y = K * R * w_A\n",
    "    Y_t = (K * R_t[:, None]) * V[A_t]   # (n, K-1)\n",
    "\n",
    "    model = ADNet(p=p, out_dim=K-1, hidden=hidden).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        f = model(X_t)                 # (n, K-1)\n",
    "        loss = (((Y_t - f) ** 2)/(K-1)).mean() # 벡터 회귀\n",
    "        loss.backward(); opt.step()\n",
    "        if (ep+1) % 10 == 0:\n",
    "            print(f\"[NN-vec] epoch {ep+1}/{epochs} loss={loss.item():.4f}\")\n",
    "\n",
    "    def predict(X_new):\n",
    "        X_new_t = torch.tensor(X_new, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            f = model(X_new_t)          # (m, K-1)\n",
    "            scores = f @ V.T            # (m, K)  Ridge와 동일\n",
    "            return scores.argmax(1).cpu().numpy()\n",
    "\n",
    "    return model, V.cpu().numpy(), predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Scenario 1 ============\n",
      "[NN-vec] epoch 10/60 loss=11.5962\n",
      "[NN-vec] epoch 20/60 loss=11.4315\n",
      "[NN-vec] epoch 30/60 loss=11.2396\n",
      "[NN-vec] epoch 40/60 loss=11.0833\n",
      "[NN-vec] epoch 50/60 loss=10.9879\n",
      "[NN-vec] epoch 60/60 loss=10.9070\n",
      "Linear-AD : Error=0.210  |  IPW-Value=3.309\n",
      "NN-AD     : Error=0.307  |  IPW-Value=3.203\n",
      "\n",
      "============ Scenario 2 ============\n",
      "[NN-vec] epoch 10/60 loss=8.5146\n",
      "[NN-vec] epoch 20/60 loss=8.3324\n",
      "[NN-vec] epoch 30/60 loss=8.0953\n",
      "[NN-vec] epoch 40/60 loss=7.8605\n",
      "[NN-vec] epoch 50/60 loss=7.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/ns0p72dd663b344v05p7gz080000gn/T/ipykernel_79817/654617832.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V = torch.tensor(simplex_vertices(K), dtype=torch.float32, device=device)  # (K, K-1)\n",
      "/var/folders/cm/ns0p72dd663b344v05p7gz080000gn/T/ipykernel_79817/654617832.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V = torch.tensor(simplex_vertices(K), dtype=torch.float32, device=device)  # (K, K-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN-vec] epoch 60/60 loss=7.6389\n",
      "Linear-AD : Error=0.278  |  IPW-Value=2.459\n",
      "NN-AD     : Error=0.285  |  IPW-Value=2.555\n",
      "\n",
      "============ Scenario 3 ============\n",
      "[NN-vec] epoch 10/60 loss=5.5696\n",
      "[NN-vec] epoch 20/60 loss=5.5306\n",
      "[NN-vec] epoch 30/60 loss=5.4832\n",
      "[NN-vec] epoch 40/60 loss=5.4251\n",
      "[NN-vec] epoch 50/60 loss=5.3532\n",
      "[NN-vec] epoch 60/60 loss=5.2660\n",
      "Linear-AD : Error=0.755  |  IPW-Value=1.221\n",
      "NN-AD     : Error=0.693  |  IPW-Value=1.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/ns0p72dd663b344v05p7gz080000gn/T/ipykernel_79817/654617832.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V = torch.tensor(simplex_vertices(K), dtype=torch.float32, device=device)  # (K, K-1)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 6) 시나리오별 실행 데모\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    K, p, n =4, 10, 2000\n",
    "    for scenario in [1, 2, 3]:\n",
    "        print(\"\\n\" + \"=\"*12, f\"Scenario {scenario}\", \"=\"*12)\n",
    "        X, A, R, opt = simulate_data(K=K, n=n, p=p, scenario=scenario, seed=42)\n",
    "        X_tr, X_te, A_tr, A_te, R_tr, R_te, opt_tr, opt_te = \\\n",
    "            train_test_split(X, A, R, opt, test_size=0.3, random_state=123)\n",
    "\n",
    "        # Linear AD\n",
    "        lin_model, V_lin, lin_pred = ad_linear(X_tr, A_tr, R_tr, K=K, alpha=1.0)\n",
    "        err_lin, val_lin = evaluate_policy(lin_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "        # NN AD\n",
    "        nn_model, V_nn, nn_pred = ad_nn_vector_target(X_tr, A_tr, R_tr, K=K, epochs=60, lr=1e-3)\n",
    "        err_nn, val_nn = evaluate_policy(nn_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "        print(f\"Linear-AD : Error={err_lin:.3f}  |  IPW-Value={val_lin:.3f}\")\n",
    "        print(f\"NN-AD     : Error={err_nn:.3f}  |  IPW-Value={val_nn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 논문 베이스라인도 스칼라 손실함수 vs 신경망도 스칼라 손실함수\n",
    "\n",
    "베이스라인 벡터 손실함수 vs 신경망 스칼라 손실함수와 거의 동일한 경과.\n",
    "수학적으로 동치라고 하는데 괜찮은건지 잘 모르겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ad_linear_scalar(X, A, R, K=4, alpha=1.0):\n",
    "    \"\"\"\n",
    "    AD-learning (scalar target version).\n",
    "    X: (n, p)\n",
    "    A: (n,) ∈ {0,...,K-1}\n",
    "    R: (n,)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).numpy()   # (K, K-1)\n",
    "\n",
    "    # target = R_i\n",
    "    y = R\n",
    "\n",
    "    # feature map: phi(x,a) = kron(x, w_a)\n",
    "    # 즉, x와 w_a의 크로네커 곱을 feature로 사용\n",
    "    X_feat = np.zeros((n, p * (K - 1)))\n",
    "    for i in range(n):\n",
    "        w = V[A[i]]                    # (K-1,)\n",
    "        # Kronecker product: x ⊗ w\n",
    "        X_feat[i, :] = np.kron(X[i], w)\n",
    "\n",
    "    # Ridge 회귀: f는 (p*(K-1)) 차원 파라미터로 학습됨\n",
    "    model = Ridge(alpha=alpha, fit_intercept=False)\n",
    "    model.fit(X_feat, y)\n",
    "\n",
    "    def predict(X_new):\n",
    "        m = X_new.shape[0]\n",
    "        scores = np.zeros((m, K))\n",
    "        for k in range(K):\n",
    "            w = V[k]                    # (K-1,)\n",
    "            # weight 벡터를 (p,(K-1)) 행렬로 reshape\n",
    "            W = model.coef_.reshape(p, K-1)\n",
    "            fX = X_new @ W              # (m, K-1)\n",
    "            scores[:, k] = fX @ w       # <f(X), w_k>\n",
    "        return scores.argmax(axis=1)\n",
    "\n",
    "    return model, V, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Scenario 1 ============\n",
      "[NN] epoch 10/60 loss=5.9688\n",
      "[NN] epoch 20/60 loss=4.6991\n",
      "[NN] epoch 30/60 loss=3.1325\n",
      "[NN] epoch 40/60 loss=1.9910\n",
      "[NN] epoch 50/60 loss=1.5484\n",
      "[NN] epoch 60/60 loss=1.1909\n",
      "Linear-AD : Error=0.542  |  IPW-Value=2.710\n",
      "NN-AD     : Error=0.254  |  IPW-Value=3.789\n",
      "\n",
      "============ Scenario 2 ============\n",
      "[NN] epoch 10/60 loss=4.5168\n",
      "[NN] epoch 20/60 loss=3.8654\n",
      "[NN] epoch 30/60 loss=3.0995\n",
      "[NN] epoch 40/60 loss=2.4207\n",
      "[NN] epoch 50/60 loss=1.9438\n",
      "[NN] epoch 60/60 loss=1.5955\n",
      "Linear-AD : Error=0.562  |  IPW-Value=2.003\n",
      "NN-AD     : Error=0.237  |  IPW-Value=2.703\n",
      "\n",
      "============ Scenario 3 ============\n",
      "[NN] epoch 10/60 loss=2.8026\n",
      "[NN] epoch 20/60 loss=2.2433\n",
      "[NN] epoch 30/60 loss=1.6896\n",
      "[NN] epoch 40/60 loss=1.4043\n",
      "[NN] epoch 50/60 loss=1.2139\n",
      "[NN] epoch 60/60 loss=1.0577\n",
      "Linear-AD : Error=0.863  |  IPW-Value=1.333\n",
      "NN-AD     : Error=0.729  |  IPW-Value=1.761\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 6) 시나리오별 실행 데모\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    K, p, n = 5, 10, 800\n",
    "    for scenario in [1, 2, 3]:\n",
    "        print(\"\\n\" + \"=\"*12, f\"Scenario {scenario}\", \"=\"*12)\n",
    "        X, A, R, opt = simulate_data(n=n, p=p, scenario=scenario, seed=42)\n",
    "        X_tr, X_te, A_tr, A_te, R_tr, R_te, opt_tr, opt_te = \\\n",
    "            train_test_split(X, A, R, opt, test_size=0.3, random_state=123)\n",
    "\n",
    "        # Linear AD\n",
    "        lin_model, V_lin, lin_pred = ad_linear_scalar(X_tr, A_tr, R_tr, K=K, alpha=1.0)\n",
    "        err_lin, val_lin = evaluate_policy(lin_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "        # NN AD\n",
    "        nn_model, V_nn, nn_pred = ad_nn(X_tr, A_tr, R_tr, K=K, epochs=60, lr=1e-3)\n",
    "        err_nn, val_nn = evaluate_policy(nn_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "        print(f\"Linear-AD : Error={err_lin:.3f}  |  IPW-Value={val_lin:.3f}\")\n",
    "        print(f\"NN-AD     : Error={err_nn:.3f}  |  IPW-Value={val_nn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이어 개수 그리드 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] epoch 10/60  loss=6.9073\n",
      "[NN] epoch 20/60  loss=6.8074\n",
      "[NN] epoch 30/60  loss=6.7103\n",
      "[NN] epoch 40/60  loss=6.6122\n",
      "[NN] epoch 50/60  loss=6.5112\n",
      "[NN] epoch 60/60  loss=6.4075\n",
      "[scen1] width=64, layers=1  -->  error=0.311, value=3.050\n",
      "[NN] epoch 10/60  loss=6.8949\n",
      "[NN] epoch 20/60  loss=6.7457\n",
      "[NN] epoch 30/60  loss=6.5473\n",
      "[NN] epoch 40/60  loss=6.3098\n",
      "[NN] epoch 50/60  loss=6.0726\n",
      "[NN] epoch 60/60  loss=5.8880\n",
      "[scen1] width=64, layers=2  -->  error=0.178, value=3.028\n",
      "[NN] epoch 10/60  loss=6.9384\n",
      "[NN] epoch 20/60  loss=6.8010\n",
      "[NN] epoch 30/60  loss=6.5463\n",
      "[NN] epoch 40/60  loss=6.1954\n",
      "[NN] epoch 50/60  loss=5.9036\n",
      "[NN] epoch 60/60  loss=5.7497\n",
      "[scen1] width=64, layers=3  -->  error=0.199, value=3.048\n",
      "[NN] epoch 10/60  loss=6.8722\n",
      "[NN] epoch 20/60  loss=6.7159\n",
      "[NN] epoch 30/60  loss=6.5685\n",
      "[NN] epoch 40/60  loss=6.4235\n",
      "[NN] epoch 50/60  loss=6.2807\n",
      "[NN] epoch 60/60  loss=6.1444\n",
      "[scen1] width=128, layers=1  -->  error=0.223, value=3.069\n",
      "[NN] epoch 10/60  loss=6.7403\n",
      "[NN] epoch 20/60  loss=6.3796\n",
      "[NN] epoch 30/60  loss=6.0144\n",
      "[NN] epoch 40/60  loss=5.7904\n",
      "[NN] epoch 50/60  loss=5.6851\n",
      "[NN] epoch 60/60  loss=5.5864\n",
      "[scen1] width=128, layers=2  -->  error=0.190, value=3.013\n",
      "[NN] epoch 10/60  loss=6.7979\n",
      "[NN] epoch 20/60  loss=6.3121\n",
      "[NN] epoch 30/60  loss=5.8490\n",
      "[NN] epoch 40/60  loss=5.6928\n",
      "[NN] epoch 50/60  loss=5.5230\n",
      "[NN] epoch 60/60  loss=5.3469\n",
      "[scen1] width=128, layers=3  -->  error=0.236, value=3.035\n",
      "\n",
      "=== 정렬 (Error 오름차순) ===\n",
      "layers=2, width=64 | error=0.178, value=3.028\n",
      "layers=2, width=128 | error=0.190, value=3.013\n",
      "layers=3, width=64 | error=0.199, value=3.048\n",
      "layers=1, width=128 | error=0.223, value=3.069\n",
      "layers=3, width=128 | error=0.236, value=3.035\n",
      "layers=1, width=64 | error=0.311, value=3.050\n",
      "\n",
      "=== 정렬 (Value 내림차순) ===\n",
      "layers=1, width=128 | error=0.223, value=3.069\n",
      "layers=1, width=64 | error=0.311, value=3.050\n",
      "layers=3, width=64 | error=0.199, value=3.048\n",
      "layers=3, width=128 | error=0.236, value=3.035\n",
      "layers=2, width=64 | error=0.178, value=3.028\n",
      "layers=2, width=128 | error=0.190, value=3.013\n",
      "\n",
      "Best by Error: {'metric': 0.17777777777777778, 'config': {'width': 64, 'layers': 2}}\n",
      "Best by Value: {'metric': 3.069066420528356, 'config': {'width': 128, 'layers': 1}}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 0) 신경망 정의 (은닉층 수/너비를 파라미터로)\n",
    "# ----------------------------------------------------\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, hidden_width=128, num_layers=2):\n",
    "        \"\"\"\n",
    "        input_dim: p\n",
    "        out_dim: K-1 (simplex 차원)\n",
    "        hidden_width: 64 또는 128\n",
    "        num_layers: 1, 2, 3 (은닉층 수)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers += [nn.Linear(d, hidden_width), nn.ReLU()]\n",
    "            d = hidden_width\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # (batch, K-1)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) AD-NN 학습 루틴 (균등 설계 → 가중치 상수 → 평범한 MSE)\n",
    "# ----------------------------------------------------\n",
    "def train_ad_nn(X, A, R, K=4, hidden_width=128, num_layers=2, epochs=60, lr=1e-3, verbose=False, device=None):\n",
    "    \"\"\"\n",
    "    균등 배정 가정 (pi = 1/K): 목적식 상수배 → 가중치 생략 OK\n",
    "    X: (n, p) numpy\n",
    "    A: (n,) numpy, {0,...,K-1}\n",
    "    R: (n,) numpy\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n, p = X.shape\n",
    "    V = torch.tensor(generate_simplex_coding(K), dtype=torch.float32, device=device)  # (K, K-1)\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    A_t = torch.tensor(A, dtype=torch.long, device=device)\n",
    "    R_t = torch.tensor(R, dtype=torch.float32, device=device)\n",
    "\n",
    "    model = ADNet(input_dim=p, out_dim=K-1, hidden_width=hidden_width, num_layers=num_layers).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        f = model(X_t)                 # (n, K-1)\n",
    "        wA = V[A_t]                    # (n, K-1)\n",
    "        pred = (f * wA).sum(1)         # <f(X), w_A>\n",
    "        loss = ((R_t - pred) ** 2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if verbose and (ep + 1) % 10 == 0:\n",
    "            print(f\"[NN] epoch {ep+1}/{epochs}  loss={loss.item():.4f}\")\n",
    "\n",
    "    # 예측 함수\n",
    "    def predict(X_new):\n",
    "        X_new_t = torch.tensor(X_new, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            f = model(X_new_t)         # (m, K-1)\n",
    "            scores = f @ V.T           # (m, K)\n",
    "            return scores.argmax(1).cpu().numpy()\n",
    "\n",
    "    return model, predict\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) 평가 지표 (Error rate & IPW Value)\n",
    "# ----------------------------------------------------\n",
    "def evaluate_policy(predict_fn, X, A, R, K, true_opt=None):\n",
    "    pred = predict_fn(X)\n",
    "    error_rate = None if true_opt is None else np.mean(pred != true_opt)\n",
    "    # 균등 무작위 배정 → P(A|X)=1/K → IPW value = E[ R * I(d(X)=A) * K ]\n",
    "    value = np.mean(R * (pred == A) * K)\n",
    "    return error_rate, value\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) 그리드 서치 실행기\n",
    "# ----------------------------------------------------\n",
    "def grid_search_nn(\n",
    "    scenario=1,                  # delta 시나리오 (1=linear, 2=tree, 3=poly 같은 식으로 당신 코드와 맞춰주세요)\n",
    "    main_effect=None,                  # main effect 함수 (없으면 μ(x)=1+X1+X2 가정하는 simulate_data에 기본값으로)\n",
    "    n=3000, p=10, K=4,\n",
    "    test_size=0.3, seed=42,\n",
    "    epochs=60, lr=1e-3,\n",
    "    widths=(64, 128),\n",
    "    layers=(1, 2, 3),\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    delta_scenario, simulate_data, generate_simplex_coding 이 이미 정의되어 있다고 가정.\n",
    "    simulate_data는 다음 형태:\n",
    "      X~Unif[-1,1], R = μ(X) + δ_A(X) + ε, true_opt = argmax_k δ_k(X)\n",
    "    \"\"\"\n",
    "    # ----- 데이터 생성 -----\n",
    "    X, A, R, true_opt = simulate_data(n=n, p=p, K=K, scenario=scenario, seed=seed,\n",
    "                                      main_effect=mu_default,\n",
    "                                      sigma_eps=1.0)\n",
    "\n",
    "    Xtr, Xte, Atr, Ate, Rtr, Rte, opttr, optte = train_test_split(\n",
    "        X, A, R, true_opt, test_size=test_size, random_state=123\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    best = {\"err\": np.inf, \"val\": -np.inf, \"cfg_err\": None, \"cfg_val\": None}\n",
    "\n",
    "    for w in widths:\n",
    "        for L in layers:\n",
    "            # ----- 학습 -----\n",
    "            _, predict_fn = train_ad_nn(\n",
    "                Xtr, Atr, Rtr, K=K,\n",
    "                hidden_width=w, num_layers=L,\n",
    "                epochs=epochs, lr=lr, verbose=verbose\n",
    "            )\n",
    "            # ----- 평가 -----\n",
    "            err, val = evaluate_policy(predict_fn, Xte, Ate, Rte, K, true_opt=optte)\n",
    "            results.append({\"width\": w, \"layers\": L, \"error\": err, \"value\": val})\n",
    "            if verbose:\n",
    "                print(f\"[scen{scenario}] width={w}, layers={L}  -->  error={err:.3f}, value={val:.3f}\")\n",
    "\n",
    "            # best by error (작을수록 좋음)\n",
    "            if err is not None and err < best[\"err\"]:\n",
    "                best[\"err\"] = err\n",
    "                best[\"cfg_err\"] = (w, L)\n",
    "            # best by value (클수록 좋음)\n",
    "            if val > best[\"val\"]:\n",
    "                best[\"val\"] = val\n",
    "                best[\"cfg_val\"] = (w, L)\n",
    "\n",
    "    # 정렬된 요약 출력용\n",
    "    results_sorted_err = sorted(results, key=lambda d: d[\"error\"])\n",
    "    results_sorted_val = sorted(results, key=lambda d: -d[\"value\"])\n",
    "\n",
    "    return {\n",
    "        \"all\": results,\n",
    "        \"rank_by_error\": results_sorted_err,\n",
    "        \"rank_by_value\": results_sorted_val,\n",
    "        \"best_error\": {\"metric\": best[\"err\"], \"config\": {\"width\": best[\"cfg_err\"][0], \"layers\": best[\"cfg_err\"][1]}},\n",
    "        \"best_value\": {\"metric\": best[\"val\"], \"config\": {\"width\": best[\"cfg_val\"][0], \"layers\": best[\"cfg_val\"][1]}},\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) 실행 예시\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 예: scenario=1 (linear δ), μ 기본(1+X1+X2), n=3000\n",
    "    out = grid_search_nn(\n",
    "        scenario=1, n=3000, p=10, K=4,\n",
    "        widths=(64, 128), layers=(1, 2, 3),\n",
    "        epochs=60, lr=1e-3, verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== 정렬 (Error 오름차순) ===\")\n",
    "    for r in out[\"rank_by_error\"]:\n",
    "        print(f\"layers={r['layers']}, width={r['width']} | error={r['error']:.3f}, value={r['value']:.3f}\")\n",
    "\n",
    "    print(\"\\n=== 정렬 (Value 내림차순) ===\")\n",
    "    for r in out[\"rank_by_value\"]:\n",
    "        print(f\"layers={r['layers']}, width={r['width']} | error={r['error']:.3f}, value={r['value']:.3f}\")\n",
    "\n",
    "    print(\"\\nBest by Error:\", out[\"best_error\"])\n",
    "    print(\"Best by Value:\", out[\"best_value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새롭게 짠 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def delta_scenario(X, scenario: int, K: int,\n",
    "                       smooth: bool = False,\n",
    "                       temp: float = 0.25,    \n",
    "                       amp: float = 1.0):       \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    d = np.zeros((n, K))\n",
    "    xs = [X[:, j % p] for j in range(max(p, 6))]\n",
    "    x1,x2,x3,x4,x5,x6 = xs[:6]\n",
    "\n",
    "    def _sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
    "    def _phi(u): return np.tanh(u)\n",
    "\n",
    "    # ---------- Scenario 1 : Linear ----------\n",
    "    if scenario == 1:\n",
    "\n",
    "        if K >= 1: d[:,0] = 1 + x1 + x2 + x3 + x4\n",
    "        if K >= 2: d[:,1] = 1 + x1 - x2 - x3 + x4\n",
    "        if K >= 3: d[:,2] = 1 + x1 - x2 + x3 - x4\n",
    "        if K >= 4: d[:,3] = 1 - x1 - x2 + x3 + x4\n",
    "\n",
    "\n",
    "    # ---------- Scenario 2 : Tree ----------\n",
    "    elif scenario == 2:\n",
    "        if not smooth:\n",
    "            if K >= 1: d[:,0] = 3.0 * ((x1 <= 0.5).astype(float) * ((x2 > -0.6).astype(float) - 1.0))\n",
    "            if K >= 2: d[:,1] = ((x3 <= 1.0).astype(float)) * (2.0*(x4 <= -0.3).astype(float) - 1.0)\n",
    "            if K >= 3: d[:,2] = (4.0*(x5 <= 0.0).astype(float) - 2.0)\n",
    "            if K >= 4: d[:,3] = (4.0*(x6 <= 0.0).astype(float) - 2.0)\n",
    "        else:\n",
    "            s1 = _sigmoid((0.5 - x1)/temp)   \n",
    "            s2 = _sigmoid((x2 + 0.6)/temp)    \n",
    "            s3 = _sigmoid((1.0 - x3)/temp) \n",
    "            s4 = _sigmoid((-0.3 - x4)/temp)   \n",
    "            s5 = _sigmoid((0.0 - x5)/temp)    \n",
    "            s6 = _sigmoid((0.0 - x6)/temp)  \n",
    "            if K >= 1: d[:,0] = amp * 3.0 * s1 * (2.0*s2 - 1.0)\n",
    "            if K >= 2: d[:,1] = amp * s3 * (2.0*s4 - 1.0)\n",
    "            if K >= 3: d[:,2] = amp * (4.0*s5 - 2.0)\n",
    "            if K >= 4: d[:,3] = amp * (4.0*s6 - 2.0)\n",
    "\n",
    "    # ---------- Scenario 3 : Polynomial ----------\n",
    "    elif scenario == 3:\n",
    "        if not smooth:\n",
    "            if K >= 1: d[:,0] = 0.2 + x1**2 + x2**2 - x3**2 - x4**2\n",
    "            if K >= 2: d[:,1] = 0.2 + x2**2 + x3**2 - x1**2 - x4**2\n",
    "            if K >= 3: d[:,2] = 0.2 + x3**2 + x4**2 - x1**2 - x2**2\n",
    "            if K >= 4: d[:,3] = 0.2 + x1**2 + x4**2 - x2**2 - x3**2\n",
    "        else:\n",
    "            # 교차항 + 합성 비선형\n",
    "            c12 = _phi(0.8*x1*x2); c23 = _phi(0.8*x2*x3)\n",
    "            c34 = _phi(0.8*x3*x4); c14 = _phi(0.8*x1*x4)\n",
    "            h1  = _phi(0.7*x1 + 0.5*x2 - 0.4*x3 + 0.3*x4)\n",
    "            h2  = _phi(-0.6*x1 + 0.7*x2 + 0.5*x3 - 0.4*x4)\n",
    "            if K >= 1: d[:,0] = amp*(0.2 + x1**2 + x2**2 - x3**2 - x4**2 + 0.6*c12 + 0.4*h1)\n",
    "            if K >= 2: d[:,1] = amp*(0.2 + x2**2 + x3**2 - x1**2 - x4**2 + 0.6*c23 + 0.4*h2)\n",
    "            if K >= 3: d[:,2] = amp*(0.2 + x3**2 + x4**2 - x1**2 - x2**2 + 0.6*c34 + 0.4*h1)\n",
    "            if K >= 4: d[:,3] = amp*(0.2 + x1**2 + x4**2 - x2**2 - x3**2 + 0.6*c14 + 0.4*h2)\n",
    "    else:\n",
    "        raise ValueError(\"scenario must be 1, 2, or 3\")\n",
    "\n",
    "    if K <= 4:\n",
    "        return d\n",
    "\n",
    "    def lin_arm(j):\n",
    "        P = [j % p, (j+1) % p]\n",
    "        M = [(j+2) % p, (j+3) % p]\n",
    "\n",
    "        return 1.0 + X[:,P[0]] + X[:,P[1]] - X[:,M[0]] - X[:,M[1]]\n",
    "\n",
    "\n",
    "    def poly_arm(j):\n",
    "        P = [j % p, (j+1) % p]; M = [(j+2) % p, (j+3) % p]\n",
    "        if not smooth:\n",
    "            return 0.2 + X[:,P[0]]**2 + X[:,P[1]]**2 - X[:,M[0]]**2 - X[:,M[1]]**2\n",
    "        cP = _phi(0.8*X[:,P[0]]*X[:,P[1]])\n",
    "        h  = _phi(0.7*X[:,P[0]] - 0.6*X[:,P[1]] + 0.4*X[:,M[0]] - 0.3*X[:,M[1]])\n",
    "        base = 0.2 + X[:,P[0]]**2 + X[:,P[1]]**2 - X[:,M[0]]**2 - X[:,M[1]]**2\n",
    "        return amp*(base + 0.6*cP + 0.4*h)\n",
    "\n",
    "    def tree_tmpl(t, j):\n",
    "        jp = j % p; jn1 = (j+1) % p\n",
    "        if not smooth:\n",
    "            if t == 0:\n",
    "                return 3.0 * ((X[:,jp] <= 0.5).astype(float) * ((X[:,jn1] > -0.6).astype(float) - 1.0))\n",
    "            elif t == 1:\n",
    "                return ((X[:,jp] <= 1.0).astype(float)) * (2.0*(X[:,jn1] <= -0.3).astype(float) - 1.0)\n",
    "            else:\n",
    "                return (4.0*(X[:,jp] <= 0.0).astype(float) - 2.0)\n",
    "        if t == 0:\n",
    "            sA = _sigmoid((0.5 - X[:,jp])/temp); sB = _sigmoid((X[:,jn1] + 0.6)/temp)\n",
    "            return amp * (3.0 * sA * (2.0*sB - 1.0))\n",
    "        elif t == 1:\n",
    "            sA = _sigmoid((1.0 - X[:,jp])/temp); sB = _sigmoid((-0.3 - X[:,jn1])/temp)\n",
    "            return amp * (sA * (2.0*sB - 1.0))\n",
    "        else:\n",
    "            sA = _sigmoid((0.0 - X[:,jp])/temp)\n",
    "            return amp * (4.0*sA - 2.0)\n",
    "\n",
    "    start_j = 0\n",
    "    for k in range(4, K):\n",
    "        j = (start_j + 2*(k-4))\n",
    "        if scenario == 1:\n",
    "            d[:,k] = lin_arm(j)\n",
    "        elif scenario == 2:\n",
    "            t = (k-4) % 4\n",
    "            d[:,k] = tree_tmpl(t, j)\n",
    "        elif scenario == 3:\n",
    "            d[:,k] = poly_arm(j)\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===============================\n",
    "# 0) simplex vertices \n",
    "# ===============================\n",
    "def simplex_vertices(K: int) -> torch.Tensor:\n",
    "    w_list = []\n",
    "    for j in range(1, K + 1):\n",
    "        if j == 1:\n",
    "\n",
    "            w_j = (1.0 / (K - 1) ** 0.5) * torch.ones(K - 1)\n",
    "        else:\n",
    "\n",
    "            term1 = -((1.0 + K ** 0.5) / ((K - 1) ** 1.5)) * torch.ones(K - 1)\n",
    "            e = torch.zeros(K - 1)\n",
    "            e[j - 2] = (K / (K - 1)) ** 0.5\n",
    "            w_j = term1 + e\n",
    "        w_list.append(w_j)\n",
    "    return torch.stack(w_list)  # (K, K-1)\n",
    "\n",
    "\n",
    "def delta_scenario_past(X, scenario: int, K: int):\n",
    "    \"\"\"\n",
    "    원 논문의 시나리오 1~3을 구현해놓음.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    d = np.zeros((n, K))\n",
    "\n",
    "    x = [X[:, j % p] for j in range(max(p, 6))]  \n",
    "    x1,x2,x3,x4,x5,x6 = x[0],x[1],x[2],x[3],x[4],x[5]\n",
    "\n",
    "    if scenario == 1:  # linear\n",
    "        if K >= 1: d[:,0] = 1 + x1 + x2 + x3 + x4\n",
    "        if K >= 2: d[:,1] = 1 + x1 - x2 - x3 + x4\n",
    "        if K >= 3: d[:,2] = 1 + x1 - x2 + x3 - x4\n",
    "        if K >= 4: d[:,3] = 1 - x1 - x2 + x3 + x4\n",
    "\n",
    "    elif scenario == 2:  # tree \n",
    "        if K >= 1: d[:,0] = 3.0 * ((x1 <= 0.5).astype(float) * ((x2 > -0.6).astype(float) - 1.0))\n",
    "        if K >= 2: d[:,1] = ((x3 <= 1.0).astype(float)) * (2.0*(x4 <= -0.3).astype(float) - 1.0)\n",
    "        if K >= 3: d[:,2] = (4.0*(x5 <= 0.0).astype(float) - 2.0)\n",
    "        if K >= 4: d[:,3] = (4.0*(x6 <= 0.0).astype(float) - 2.0)\n",
    "\n",
    "    elif scenario == 3:  # polynomial\n",
    "        if K >= 1: d[:,0] = 0.2 + x1**2 + x2**2 - x3**2 - x4**2\n",
    "        if K >= 2: d[:,1] = 0.2 + x2**2 + x3**2 - x1**2 - x4**2\n",
    "        if K >= 3: d[:,2] = 0.2 + x3**2 + x4**2 - x1**2 - x2**2\n",
    "        if K >= 4: d[:,3] = 0.2 + x1**2 + x4**2 - x2**2 - x3**2\n",
    "    else:\n",
    "        raise ValueError(\"scenario must be 1, 2, or 3\")\n",
    "\n",
    "    if K <= 4:\n",
    "        return d\n",
    "\n",
    "\n",
    "    def lin_arm(j):\n",
    "        P = [j % p, (j+1) % p]\n",
    "        M = [(j+2) % p, (j+3) % p]\n",
    "        return 1.0 + X[:,P[0]] + X[:,P[1]] - X[:,M[0]] - X[:,M[1]]\n",
    "\n",
    "    def poly_arm(j):\n",
    "        P = [j % p, (j+1) % p]\n",
    "        M = [(j+2) % p, (j+3) % p]\n",
    "        return 0.2 + X[:,P[0]]**2 + X[:,P[1]]**2 - X[:,M[0]]**2 - X[:,M[1]]**2\n",
    "\n",
    "\n",
    "    def tree_tmpl(t, j):\n",
    "        jp = j % p; jn1 = (j+1) % p\n",
    "        if t == 0:\n",
    "            return 3.0 * ((X[:,jp] <= 0.5).astype(float) * ((X[:,jn1] > -0.6).astype(float) - 1.0))\n",
    "        elif t == 1:\n",
    "            return ((X[:,jp] <= 1.0).astype(float)) * (2.0*(X[:,jn1] <= -0.3).astype(float) - 1.0)\n",
    "        elif t == 2:\n",
    "            return (4.0*(X[:,jp] <= 0.0).astype(float) - 2.0)\n",
    "        else:\n",
    "            return (4.0*(X[:,jp] <= 0.0).astype(float) - 2.0)\n",
    "\n",
    "    # k=5부터 채우기\n",
    "    start_j = 0  \n",
    "    for k in range(4, K):\n",
    "        j = (start_j + 2*(k-4))  \n",
    "        if scenario == 1:\n",
    "            d[:,k] = lin_arm(j)\n",
    "        elif scenario == 2:\n",
    "            t = (k-4) % 4       \n",
    "            d[:,k] = tree_tmpl(t, j)\n",
    "        elif scenario == 3:\n",
    "            d[:,k] = poly_arm(j)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2) 데이터 생성\n",
    "# ===============================\n",
    "\n",
    "def mu_default(X):\n",
    "    \"\"\"\n",
    "    논문 기본설정: μ(x) = 1 + X1 + X2  (X는 (n,p), X1=X[:,0], X2=X[:,1])\n",
    "    나머지 시나리오에서도 mu가 동일하게 적용되는지 모르겠음. appendix에서 알려준대놓고 안 알려줌;;\n",
    "    \"\"\"\n",
    "    return 1.0 + X[:, 0] + X[:, 1]\n",
    "\n",
    "# def mu_poly(X):\n",
    "#     x1, x2 = X[:,0], X[:,1]\n",
    "#     return 1.0 + 0.6*x1 + 0.6*x2 + 0.4*(x1**2) - 0.3*(x2**2) + 0.2*(x1*x2)\n",
    "\n",
    "\n",
    "def simulate_data(K, n=2000, p=10, scenario=1, seed=0,\n",
    "                  main_effect=mu_default, sigma_eps=1.0,\n",
    "                  uniform_low=-1.0, uniform_high=1.0, smooth=False):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # X ~ Unif[-1, 1]\n",
    "    X = rng.uniform(uniform_low, uniform_high, size=(n, p))\n",
    "\n",
    "    deltas = delta_scenario(X, scenario, K, smooth)\n",
    "    mu = main_effect(X)\n",
    "\n",
    "    # A를 가능한 균등하게 분배\n",
    "    base = n // K\n",
    "    counts = np.full(K, base, dtype=int)\n",
    "    counts[: (n % K)] += 1       \n",
    "    A = np.repeat(np.arange(K), counts)\n",
    "    rng.shuffle(A)                \n",
    "\n",
    "\n",
    "    eps = rng.normal(0.0, sigma_eps, size=n)\n",
    "    R = mu + deltas[np.arange(n), A] + eps\n",
    "\n",
    "    # 최적 치료 (Ground truth): main effect 제외\n",
    "    true_opt = np.argmax(deltas, axis=1)\n",
    "\n",
    "    return X, A, R, true_opt\n",
    "\n",
    "# ===============================\n",
    "# 3) 선형 AD-learning\n",
    "# Ridge로 간단하게 구현한 것.\n",
    "# ===============================\n",
    "def ad_linear(X, A, R, K, alpha=1.0):\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).numpy()           \n",
    "    Y = np.zeros((n, K-1))\n",
    "    for i in range(n):\n",
    "        Y[i,:] = K * R[i] * V[A[i]]                  \n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=False)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    def predict(X_new):\n",
    "        f = model.predict(X_new)               \n",
    "        scores = f @ V.T                            \n",
    "        return scores.argmax(axis=1)\n",
    "    return model, V, predict\n",
    "\n",
    "# ===============================\n",
    "# 4) 신경망 AD-learning \n",
    "# ===============================\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, p, out_dim, hidden=[128,128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = p\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def ad_nn(X, A, R, K, epochs=50, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).to(device)     \n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    A_t = torch.tensor(A, dtype=torch.long, device=device)\n",
    "    R_t = torch.tensor(R, dtype=torch.float32, device=device)\n",
    "\n",
    "    model = ADNet(p, K-1).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        f = model(X_t)                        \n",
    "        wA = V[A_t]                            \n",
    "        pred = (f*wA).sum(1)                       \n",
    "        loss = ((R_t - pred)**2).mean()\n",
    "\n",
    "        loss.backward(); opt.step()\n",
    "        if (ep+1) % 10 == 0:\n",
    "            print(f\"[NN] epoch {ep+1}/{epochs} loss={loss.item():.4f}\")\n",
    "\n",
    "    def predict(X_new):\n",
    "        X_new_t = torch.tensor(X_new, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            f = model(X_new_t)                  \n",
    "            scores = f @ V.T                 \n",
    "            return scores.argmax(1).cpu().numpy()\n",
    "    return model, V.cpu().numpy(), predict\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5) 평가 (Error rate & IPW Value)\n",
    "# ===============================\n",
    "def evaluate_policy(predict_fn, X, A, R, K, true_opt=None):\n",
    "    pred = predict_fn(X)\n",
    "    error_rate = None if true_opt is None else np.mean(pred != true_opt)\n",
    "    value = np.mean(R * (pred == A) * K)\n",
    "    return error_rate, value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _group_soft_threshold_rowwise(B, tau):\n",
    "    \"\"\"\n",
    "    group sparsity\n",
    "    \"\"\"\n",
    "    B_new = B.copy()\n",
    "    norms = np.linalg.norm(B_new, axis=1)             # (p,)\n",
    "    scale = np.maximum(0.0, 1.0 - tau / (norms + 1e-12))\n",
    "    B_new = scale[:, None] * B_new\n",
    "    return B_new\n",
    "\n",
    "def ad_linear_group(\n",
    "    X, A, R, K,\n",
    "    lam=0.05,            \n",
    "    step=1e-2,      \n",
    "    max_iter=1000,\n",
    "    tol=1e-6,\n",
    "    pi=None,\n",
    "    alpha=1.0\n",
    "):\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).numpy() \n",
    "    if pi is None:\n",
    "        pi = np.full(n, 1.0 / K, dtype=float)\n",
    "\n",
    "    Y = np.zeros((n, K-1), dtype=float)\n",
    "    for i in range(n):\n",
    "        Y[i, :] = (R[i] / pi[i]) * V[A[i]]\n",
    "\n",
    "    # 초기값\n",
    "    B = np.zeros((p, K-1), dtype=float)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        XB = X @ B\n",
    "        # 그래디언트\n",
    "        grad = (2.0 / n) * (X.T @ (XB - Y))\n",
    "        # gradient step\n",
    "        B_next = B - step * grad\n",
    "        # group sparsity\n",
    "        B_next = _group_soft_threshold_rowwise(B_next, tau=step * lam)\n",
    "\n",
    "        # 수렴 체크\n",
    "        denom = max(1.0, np.linalg.norm(B, ord='fro'))\n",
    "        if np.linalg.norm(B_next - B, ord='fro') <= tol * denom:\n",
    "            B = B_next\n",
    "            break\n",
    "        B = B_next\n",
    "\n",
    "\n",
    "    def predict(X_new):\n",
    "        F = X_new @ B            \n",
    "        scores = F @ V.T       \n",
    "        return scores.argmax(axis=1)\n",
    "\n",
    "\n",
    "    model = {\"B\": B}\n",
    "    return model, V, predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x 표준화 추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _group_soft_threshold_rowwise(B, tau):\n",
    "    \"\"\"행(특징) 단위 L2 소프트임계: b_j <- (1 - tau/||b_j||)_+ * b_j\"\"\"\n",
    "    norms = np.linalg.norm(B, axis=1)              # (p,)\n",
    "    scale = np.maximum(0.0, 1.0 - tau / (norms + 1e-12))\n",
    "    return scale[:, None] * B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ad_linear_group_standardized(\n",
    "    X, A, R, K,\n",
    "    lam=1.0,          # λ * ||B||_{2,1}\n",
    "    step=1e-3,        # 고정 step size\n",
    "    max_iter=2000,\n",
    "    tol=1e-6,\n",
    "    pi=None,          # propensity P(A|X); None이면 균등(=1/K)\n",
    "    V=None,\n",
    "    alpha=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    센터링 + 표준화 버전.\n",
    "    목적:  min_B (1/n)∑ ||Y_i - B^T ((x_i - μ)/σ)||_2^2 + lam * ||B||_{2,1}\n",
    "      - Y_i = (R_i / π_i) w_{A_i}; 균등배정이면 π_i=1/K → Y_i = K R_i w_{A_i}\n",
    "    예측:  argmax_k  ((B^T ((x - μ)/σ)) · w_k)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    if V is None:\n",
    "        V = simplex_vertices(K).numpy()  # (K, K-1)\n",
    "    if pi is None:\n",
    "        pi = np.full(n, 1.0 / K, dtype=float)\n",
    "\n",
    "    # Target Y ∈ R^{n×(K-1)}\n",
    "    Y = np.zeros((n, K-1), dtype=float)\n",
    "    for i in range(n):\n",
    "        Y[i, :] = (R[i] / pi[i]) * V[A[i]]\n",
    "\n",
    "    # -------- 표준화 --------\n",
    "    X_mean = X.mean(axis=0, keepdims=True)       # (1, p)\n",
    "    X_std = X.std(axis=0, keepdims=True) + 1e-12 # (1, p)\n",
    "    Xs = (X - X_mean) / X_std                    # (n, p)\n",
    "    # ------------------------\n",
    "\n",
    "    # 초기값\n",
    "    B = np.zeros((p, K-1), dtype=float)\n",
    "\n",
    "    # ISTA (prox-gradient)\n",
    "    for _ in range(max_iter):\n",
    "        XB = Xs @ B                                   # (n, K-1)\n",
    "        grad = (2.0 / n) * (Xs.T @ (XB - Y))          # (p, K-1)\n",
    "        B_next = B - step * grad\n",
    "        B_next = _group_soft_threshold_rowwise(B_next, tau=step * lam)\n",
    "\n",
    "        if np.linalg.norm(B_next - B, ord='fro') <= tol * max(1.0, np.linalg.norm(B, ord='fro')):\n",
    "            B = B_next\n",
    "            break\n",
    "        B = B_next\n",
    "\n",
    "    def predict(X_new):\n",
    "        Xs_new = (X_new - X_mean) / X_std\n",
    "        F = Xs_new @ B\n",
    "        scores = F @ V.T\n",
    "        return scores.argmax(axis=1)\n",
    "\n",
    "\n",
    "    model = {\"B\": B, \"X_mean\": X_mean, \"X_std\": X_std}\n",
    "    return model, V, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_experiments(K, p=10, n=2000, n_test=10000, scenario_list=[1,2,3],\n",
    "                    n_repeats=20, epochs=60, lr=1e-3, alpha=1.0, smooth=False):\n",
    "    \"\"\"\n",
    "    학습: 크기 n의 학습 데이터 전체 사용\n",
    "    평가: 크기 n_test의 독립 테스트 데이터로만 평가\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for scenario in scenario_list:\n",
    "        err_lin_list, val_lin_list = [], []\n",
    "        err_nn_list, val_nn_list = [], []\n",
    "\n",
    "        for rep in range(n_repeats):\n",
    "            print(f\"[Scenario {scenario}] Repeat {rep+1}/{n_repeats}\")\n",
    "\n",
    "            # 1) Train set (size = n)\n",
    "            train_seed = 42 + rep\n",
    "            X_tr, A_tr, R_tr, opt_tr = simulate_data(\n",
    "                K=K, n=n, p=p, scenario=scenario, seed=train_seed, smooth=smooth\n",
    "            )\n",
    "\n",
    "\n",
    "            # 2) Fit models on train set\n",
    "            # Linear AD\n",
    "            lin_model, V_lin, lin_pred = ad_linear_group(X_tr, A_tr, R_tr, K=K, alpha=alpha)\n",
    "\n",
    "            # NN AD\n",
    "            nn_model, V_nn, nn_pred = ad_nn(X_tr, A_tr, R_tr, K=K, epochs=epochs, lr=lr)\n",
    "\n",
    "\n",
    "            # 3) Independent Test set (size = n_test)\n",
    "            test_seed = 1_000_000 + train_seed\n",
    "            X_te, A_te, R_te, opt_te = simulate_data(\n",
    "                K=K, n=n_test, p=p, scenario=scenario, seed=test_seed, smooth=smooth\n",
    "            )\n",
    "\n",
    "            # 4) Evaluate on test only\n",
    "            err_lin, val_lin = evaluate_policy(lin_pred, X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "            err_nn,  val_nn  = evaluate_policy(nn_pred,  X_te, A_te, R_te, K, true_opt=opt_te)\n",
    "\n",
    "            err_lin_list.append(err_lin); val_lin_list.append(val_lin)\n",
    "            err_nn_list.append(err_nn);   val_nn_list.append(val_nn)\n",
    "\n",
    "        results[scenario] = {\n",
    "            \"Linear\": {\n",
    "                \"Error_mean\": float(np.mean(err_lin_list)),\n",
    "                \"Error_var\":  float(np.var(err_lin_list, ddof=1)),\n",
    "                \"Value_mean\": float(np.mean(val_lin_list)),\n",
    "                \"Value_var\":  float(np.var(val_lin_list, ddof=1)),\n",
    "            },\n",
    "            \"NN\": {\n",
    "                \"Error_mean\": float(np.mean(err_nn_list)),\n",
    "                \"Error_var\":  float(np.var(err_nn_list, ddof=1)),\n",
    "                \"Value_mean\": float(np.mean(val_nn_list)),\n",
    "                \"Value_var\":  float(np.var(val_nn_list, ddof=1)),\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] epoch 10/50 loss=6.6621\n",
      "[NN] epoch 20/50 loss=6.3100\n",
      "[NN] epoch 30/50 loss=5.9645\n",
      "[NN] epoch 40/50 loss=5.7794\n",
      "[NN] epoch 50/50 loss=5.6779\n",
      "[NN] epoch 10/50 loss=6.5740\n",
      "[NN] epoch 20/50 loss=6.2032\n",
      "[NN] epoch 30/50 loss=5.8408\n",
      "[NN] epoch 40/50 loss=5.6350\n",
      "[NN] epoch 50/50 loss=5.5351\n",
      "[NN] epoch 10/50 loss=6.6033\n",
      "[NN] epoch 20/50 loss=6.2389\n",
      "[NN] epoch 30/50 loss=5.8843\n",
      "[NN] epoch 40/50 loss=5.6797\n",
      "[NN] epoch 50/50 loss=5.5554\n",
      "[NN] epoch 10/50 loss=6.7905\n",
      "[NN] epoch 20/50 loss=6.4154\n",
      "[NN] epoch 30/50 loss=6.0476\n",
      "[NN] epoch 40/50 loss=5.8654\n",
      "[NN] epoch 50/50 loss=5.7681\n",
      "[NN] epoch 10/50 loss=6.7626\n",
      "[NN] epoch 20/50 loss=6.3904\n",
      "[NN] epoch 30/50 loss=6.0177\n",
      "[NN] epoch 40/50 loss=5.7854\n",
      "[NN] epoch 50/50 loss=5.6585\n",
      "[NN] epoch 10/50 loss=6.7750\n",
      "[NN] epoch 20/50 loss=6.3989\n",
      "[NN] epoch 30/50 loss=6.0003\n",
      "[NN] epoch 40/50 loss=5.7534\n",
      "[NN] epoch 50/50 loss=5.6547\n",
      "[NN] epoch 10/50 loss=6.6756\n",
      "[NN] epoch 20/50 loss=6.3127\n",
      "[NN] epoch 30/50 loss=5.9676\n",
      "[NN] epoch 40/50 loss=5.7791\n",
      "[NN] epoch 50/50 loss=5.6753\n",
      "[NN] epoch 10/50 loss=6.9523\n",
      "[NN] epoch 20/50 loss=6.6060\n",
      "[NN] epoch 30/50 loss=6.2225\n",
      "[NN] epoch 40/50 loss=5.9838\n",
      "[NN] epoch 50/50 loss=5.8812\n",
      "[NN] epoch 10/50 loss=6.7082\n",
      "[NN] epoch 20/50 loss=6.3448\n",
      "[NN] epoch 30/50 loss=5.9335\n",
      "[NN] epoch 40/50 loss=5.6198\n",
      "[NN] epoch 50/50 loss=5.4665\n",
      "[NN] epoch 10/50 loss=6.5182\n",
      "[NN] epoch 20/50 loss=6.1325\n",
      "[NN] epoch 30/50 loss=5.7395\n",
      "[NN] epoch 40/50 loss=5.5090\n",
      "[NN] epoch 50/50 loss=5.3943\n",
      "[NN] epoch 10/50 loss=7.0677\n",
      "[NN] epoch 20/50 loss=6.7374\n",
      "[NN] epoch 30/50 loss=6.3566\n",
      "[NN] epoch 40/50 loss=6.0754\n",
      "[NN] epoch 50/50 loss=5.9552\n",
      "[NN] epoch 10/50 loss=6.4099\n",
      "[NN] epoch 20/50 loss=6.0421\n",
      "[NN] epoch 30/50 loss=5.7344\n",
      "[NN] epoch 40/50 loss=5.5771\n",
      "[NN] epoch 50/50 loss=5.4690\n",
      "[NN] epoch 10/50 loss=6.7106\n",
      "[NN] epoch 20/50 loss=6.3349\n",
      "[NN] epoch 30/50 loss=5.9469\n",
      "[NN] epoch 40/50 loss=5.7191\n",
      "[NN] epoch 50/50 loss=5.6206\n",
      "[NN] epoch 10/50 loss=6.5413\n",
      "[NN] epoch 20/50 loss=6.2139\n",
      "[NN] epoch 30/50 loss=5.8765\n",
      "[NN] epoch 40/50 loss=5.6896\n",
      "[NN] epoch 50/50 loss=5.5897\n",
      "[NN] epoch 10/50 loss=6.7799\n",
      "[NN] epoch 20/50 loss=6.4319\n",
      "[NN] epoch 30/50 loss=6.1049\n",
      "[NN] epoch 40/50 loss=5.9406\n",
      "[NN] epoch 50/50 loss=5.8430\n",
      "[NN] epoch 10/50 loss=6.7142\n",
      "[NN] epoch 20/50 loss=6.3329\n",
      "[NN] epoch 30/50 loss=5.9466\n",
      "[NN] epoch 40/50 loss=5.7134\n",
      "[NN] epoch 50/50 loss=5.5860\n",
      "[NN] epoch 10/50 loss=6.7833\n",
      "[NN] epoch 20/50 loss=6.4451\n",
      "[NN] epoch 30/50 loss=6.0939\n",
      "[NN] epoch 40/50 loss=5.8960\n",
      "[NN] epoch 50/50 loss=5.7854\n",
      "[NN] epoch 10/50 loss=6.9439\n",
      "[NN] epoch 20/50 loss=6.6112\n",
      "[NN] epoch 30/50 loss=6.2771\n",
      "[NN] epoch 40/50 loss=6.0855\n",
      "[NN] epoch 50/50 loss=5.9814\n",
      "[NN] epoch 10/50 loss=6.7513\n",
      "[NN] epoch 20/50 loss=6.3735\n",
      "[NN] epoch 30/50 loss=5.9959\n",
      "[NN] epoch 40/50 loss=5.7601\n",
      "[NN] epoch 50/50 loss=5.6379\n",
      "[NN] epoch 10/50 loss=6.6011\n",
      "[NN] epoch 20/50 loss=6.2399\n",
      "[NN] epoch 30/50 loss=5.8658\n",
      "[NN] epoch 40/50 loss=5.6541\n",
      "[NN] epoch 50/50 loss=5.5694\n",
      "[NN] epoch 10/50 loss=4.8158\n",
      "[NN] epoch 20/50 loss=4.4228\n",
      "[NN] epoch 30/50 loss=4.0483\n",
      "[NN] epoch 40/50 loss=3.7655\n",
      "[NN] epoch 50/50 loss=3.6566\n",
      "[NN] epoch 10/50 loss=4.6457\n",
      "[NN] epoch 20/50 loss=4.3251\n",
      "[NN] epoch 30/50 loss=3.9962\n",
      "[NN] epoch 40/50 loss=3.7199\n",
      "[NN] epoch 50/50 loss=3.5926\n",
      "[NN] epoch 10/50 loss=4.9116\n",
      "[NN] epoch 20/50 loss=4.5518\n",
      "[NN] epoch 30/50 loss=4.1819\n",
      "[NN] epoch 40/50 loss=3.8970\n",
      "[NN] epoch 50/50 loss=3.7721\n",
      "[NN] epoch 10/50 loss=4.9050\n",
      "[NN] epoch 20/50 loss=4.5617\n",
      "[NN] epoch 30/50 loss=4.1820\n",
      "[NN] epoch 40/50 loss=3.8719\n",
      "[NN] epoch 50/50 loss=3.7371\n",
      "[NN] epoch 10/50 loss=5.1190\n",
      "[NN] epoch 20/50 loss=4.6841\n",
      "[NN] epoch 30/50 loss=4.3214\n",
      "[NN] epoch 40/50 loss=3.9803\n",
      "[NN] epoch 50/50 loss=3.7921\n",
      "[NN] epoch 10/50 loss=4.8787\n",
      "[NN] epoch 20/50 loss=4.5195\n",
      "[NN] epoch 30/50 loss=4.1704\n",
      "[NN] epoch 40/50 loss=3.8990\n",
      "[NN] epoch 50/50 loss=3.7770\n",
      "[NN] epoch 10/50 loss=4.7096\n",
      "[NN] epoch 20/50 loss=4.3828\n",
      "[NN] epoch 30/50 loss=4.0512\n",
      "[NN] epoch 40/50 loss=3.7843\n",
      "[NN] epoch 50/50 loss=3.6410\n",
      "[NN] epoch 10/50 loss=4.9353\n",
      "[NN] epoch 20/50 loss=4.5775\n",
      "[NN] epoch 30/50 loss=4.2410\n",
      "[NN] epoch 40/50 loss=3.9649\n",
      "[NN] epoch 50/50 loss=3.7993\n",
      "[NN] epoch 10/50 loss=4.8006\n",
      "[NN] epoch 20/50 loss=4.4576\n",
      "[NN] epoch 30/50 loss=4.1662\n",
      "[NN] epoch 40/50 loss=3.9243\n",
      "[NN] epoch 50/50 loss=3.7883\n",
      "[NN] epoch 10/50 loss=4.6518\n",
      "[NN] epoch 20/50 loss=4.3061\n",
      "[NN] epoch 30/50 loss=3.9765\n",
      "[NN] epoch 40/50 loss=3.7421\n",
      "[NN] epoch 50/50 loss=3.6367\n",
      "[NN] epoch 10/50 loss=4.9420\n",
      "[NN] epoch 20/50 loss=4.5839\n",
      "[NN] epoch 30/50 loss=4.2528\n",
      "[NN] epoch 40/50 loss=3.9665\n",
      "[NN] epoch 50/50 loss=3.8165\n",
      "[NN] epoch 10/50 loss=4.5994\n",
      "[NN] epoch 20/50 loss=4.2458\n",
      "[NN] epoch 30/50 loss=3.9061\n",
      "[NN] epoch 40/50 loss=3.6854\n",
      "[NN] epoch 50/50 loss=3.5808\n",
      "[NN] epoch 10/50 loss=4.9577\n",
      "[NN] epoch 20/50 loss=4.5725\n",
      "[NN] epoch 30/50 loss=4.1673\n",
      "[NN] epoch 40/50 loss=3.8604\n",
      "[NN] epoch 50/50 loss=3.7449\n",
      "[NN] epoch 10/50 loss=4.7718\n",
      "[NN] epoch 20/50 loss=4.4358\n",
      "[NN] epoch 30/50 loss=4.0818\n",
      "[NN] epoch 40/50 loss=3.8206\n",
      "[NN] epoch 50/50 loss=3.7202\n",
      "[NN] epoch 10/50 loss=5.0574\n",
      "[NN] epoch 20/50 loss=4.6944\n",
      "[NN] epoch 30/50 loss=4.3084\n",
      "[NN] epoch 40/50 loss=4.0083\n",
      "[NN] epoch 50/50 loss=3.8938\n",
      "[NN] epoch 10/50 loss=4.7378\n",
      "[NN] epoch 20/50 loss=4.4346\n",
      "[NN] epoch 30/50 loss=4.0918\n",
      "[NN] epoch 40/50 loss=3.8397\n",
      "[NN] epoch 50/50 loss=3.7349\n",
      "[NN] epoch 10/50 loss=4.8188\n",
      "[NN] epoch 20/50 loss=4.4484\n",
      "[NN] epoch 30/50 loss=4.1072\n",
      "[NN] epoch 40/50 loss=3.8862\n",
      "[NN] epoch 50/50 loss=3.7998\n",
      "[NN] epoch 10/50 loss=4.8339\n",
      "[NN] epoch 20/50 loss=4.5047\n",
      "[NN] epoch 30/50 loss=4.1628\n",
      "[NN] epoch 40/50 loss=3.9060\n",
      "[NN] epoch 50/50 loss=3.8091\n",
      "[NN] epoch 10/50 loss=4.7882\n",
      "[NN] epoch 20/50 loss=4.4595\n",
      "[NN] epoch 30/50 loss=4.1476\n",
      "[NN] epoch 40/50 loss=3.8290\n",
      "[NN] epoch 50/50 loss=3.6422\n",
      "[NN] epoch 10/50 loss=4.6018\n",
      "[NN] epoch 20/50 loss=4.2616\n",
      "[NN] epoch 30/50 loss=3.9003\n",
      "[NN] epoch 40/50 loss=3.6187\n",
      "[NN] epoch 50/50 loss=3.5038\n",
      "[NN] epoch 10/50 loss=3.5711\n",
      "[NN] epoch 20/50 loss=3.4909\n",
      "[NN] epoch 30/50 loss=3.4075\n",
      "[NN] epoch 40/50 loss=3.3087\n",
      "[NN] epoch 50/50 loss=3.1881\n",
      "[NN] epoch 10/50 loss=3.4474\n",
      "[NN] epoch 20/50 loss=3.3606\n",
      "[NN] epoch 30/50 loss=3.2696\n",
      "[NN] epoch 40/50 loss=3.1603\n",
      "[NN] epoch 50/50 loss=3.0303\n",
      "[NN] epoch 10/50 loss=3.5744\n",
      "[NN] epoch 20/50 loss=3.4874\n",
      "[NN] epoch 30/50 loss=3.3970\n",
      "[NN] epoch 40/50 loss=3.2914\n",
      "[NN] epoch 50/50 loss=3.1635\n",
      "[NN] epoch 10/50 loss=3.6669\n",
      "[NN] epoch 20/50 loss=3.5832\n",
      "[NN] epoch 30/50 loss=3.4979\n",
      "[NN] epoch 40/50 loss=3.3954\n",
      "[NN] epoch 50/50 loss=3.2704\n",
      "[NN] epoch 10/50 loss=3.6544\n",
      "[NN] epoch 20/50 loss=3.5722\n",
      "[NN] epoch 30/50 loss=3.4784\n",
      "[NN] epoch 40/50 loss=3.3604\n",
      "[NN] epoch 50/50 loss=3.2154\n",
      "[NN] epoch 10/50 loss=3.4671\n",
      "[NN] epoch 20/50 loss=3.4001\n",
      "[NN] epoch 30/50 loss=3.3285\n",
      "[NN] epoch 40/50 loss=3.2392\n",
      "[NN] epoch 50/50 loss=3.1288\n",
      "[NN] epoch 10/50 loss=3.5227\n",
      "[NN] epoch 20/50 loss=3.4441\n",
      "[NN] epoch 30/50 loss=3.3605\n",
      "[NN] epoch 40/50 loss=3.2612\n",
      "[NN] epoch 50/50 loss=3.1411\n",
      "[NN] epoch 10/50 loss=3.6438\n",
      "[NN] epoch 20/50 loss=3.5756\n",
      "[NN] epoch 30/50 loss=3.4946\n",
      "[NN] epoch 40/50 loss=3.3920\n",
      "[NN] epoch 50/50 loss=3.2657\n",
      "[NN] epoch 10/50 loss=3.5219\n",
      "[NN] epoch 20/50 loss=3.4437\n",
      "[NN] epoch 30/50 loss=3.3516\n",
      "[NN] epoch 40/50 loss=3.2338\n",
      "[NN] epoch 50/50 loss=3.0879\n",
      "[NN] epoch 10/50 loss=3.3506\n",
      "[NN] epoch 20/50 loss=3.2776\n",
      "[NN] epoch 30/50 loss=3.2010\n",
      "[NN] epoch 40/50 loss=3.1124\n",
      "[NN] epoch 50/50 loss=3.0040\n",
      "[NN] epoch 10/50 loss=3.7059\n",
      "[NN] epoch 20/50 loss=3.6259\n",
      "[NN] epoch 30/50 loss=3.5323\n",
      "[NN] epoch 40/50 loss=3.4176\n",
      "[NN] epoch 50/50 loss=3.2743\n",
      "[NN] epoch 10/50 loss=3.4990\n",
      "[NN] epoch 20/50 loss=3.4012\n",
      "[NN] epoch 30/50 loss=3.3087\n",
      "[NN] epoch 40/50 loss=3.2007\n",
      "[NN] epoch 50/50 loss=3.0715\n",
      "[NN] epoch 10/50 loss=3.5086\n",
      "[NN] epoch 20/50 loss=3.4294\n",
      "[NN] epoch 30/50 loss=3.3413\n",
      "[NN] epoch 40/50 loss=3.2358\n",
      "[NN] epoch 50/50 loss=3.1112\n",
      "[NN] epoch 10/50 loss=3.5267\n",
      "[NN] epoch 20/50 loss=3.4474\n",
      "[NN] epoch 30/50 loss=3.3581\n",
      "[NN] epoch 40/50 loss=3.2473\n",
      "[NN] epoch 50/50 loss=3.1147\n",
      "[NN] epoch 10/50 loss=3.7228\n",
      "[NN] epoch 20/50 loss=3.6399\n",
      "[NN] epoch 30/50 loss=3.5571\n",
      "[NN] epoch 40/50 loss=3.4569\n",
      "[NN] epoch 50/50 loss=3.3352\n",
      "[NN] epoch 10/50 loss=3.6425\n",
      "[NN] epoch 20/50 loss=3.5590\n",
      "[NN] epoch 30/50 loss=3.4731\n",
      "[NN] epoch 40/50 loss=3.3701\n",
      "[NN] epoch 50/50 loss=3.2465\n",
      "[NN] epoch 10/50 loss=3.5266\n",
      "[NN] epoch 20/50 loss=3.4414\n",
      "[NN] epoch 30/50 loss=3.3556\n",
      "[NN] epoch 40/50 loss=3.2550\n",
      "[NN] epoch 50/50 loss=3.1313\n",
      "[NN] epoch 10/50 loss=3.6386\n",
      "[NN] epoch 20/50 loss=3.5698\n",
      "[NN] epoch 30/50 loss=3.4926\n",
      "[NN] epoch 40/50 loss=3.3971\n",
      "[NN] epoch 50/50 loss=3.2789\n",
      "[NN] epoch 10/50 loss=3.5385\n",
      "[NN] epoch 20/50 loss=3.4474\n",
      "[NN] epoch 30/50 loss=3.3478\n",
      "[NN] epoch 40/50 loss=3.2331\n",
      "[NN] epoch 50/50 loss=3.1020\n",
      "[NN] epoch 10/50 loss=3.5036\n",
      "[NN] epoch 20/50 loss=3.4235\n",
      "[NN] epoch 30/50 loss=3.3436\n",
      "[NN] epoch 40/50 loss=3.2523\n",
      "[NN] epoch 50/50 loss=3.1382\n",
      "\n",
      "========== Scenario 1 ==========\n",
      "Linear  | Error mean=0.167, var=0.0004 | Value mean=3.122, var=0.0059\n",
      "NN      | Error mean=0.197, var=0.0003 | Value mean=3.104, var=0.0065\n",
      "\n",
      "========== Scenario 2 ==========\n",
      "Linear  | Error mean=0.375, var=0.0005 | Value mean=2.493, var=0.0029\n",
      "NN      | Error mean=0.215, var=0.0003 | Value mean=2.683, var=0.0027\n",
      "\n",
      "========== Scenario 3 ==========\n",
      "Linear  | Error mean=0.692, var=0.0008 | Value mean=1.287, var=0.0032\n",
      "NN      | Error mean=0.591, var=0.0008 | Value mean=1.468, var=0.0033\n"
     ]
    }
   ],
   "source": [
    "# 코드 실행 (스크립트용)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_experiments(K=4, p=10, n=2000, n_test=10000, n_repeats=20,\n",
    "                               epochs=50, lr=1e-3, smooth=True)\n",
    "    for scen, res in results.items():\n",
    "        print(\"\\n\" + \"=\"*10, f\"Scenario {scen}\", \"=\"*10)\n",
    "        for model_name, stats in res.items():\n",
    "            print(f\"{model_name:7s} | \"\n",
    "                  f\"Error mean={stats['Error_mean']:.3f}, var={stats['Error_var']:.4f} | \"\n",
    "                  f\"Value mean={stats['Value_mean']:.3f}, var={stats['Value_var']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def simulate_data_surv(\n",
    "    K,\n",
    "    n=2000,\n",
    "    p=10,\n",
    "    scenario=1,\n",
    "    seed=0,\n",
    "    main_effect=mu_default,  # μ(x)\n",
    "    uniform_low=-1.0,\n",
    "    uniform_high=1.0,\n",
    "    theta_cens=3.0,\n",
    "    smooth: bool = False,    \n",
    "):\n",
    "    \"\"\"\n",
    "    생존형 outcome (time-to-event)을 delta 기반 구조로 생성.\n",
    "    lambda_i = mu(x_i) + delta_{A_i}(x_i)\n",
    "    R_i ~ Exp(rate = exp(lambda_i))\n",
    "\n",
    "    Delta = 1 이면 event 발생, 0 이면 censoring\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 1) Covariates X\n",
    "    # ------------------------------------------\n",
    "    X = rng.uniform(uniform_low, uniform_high, size=(n, p))\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 2) Delta matrix: (n, K)\n",
    "    # ------------------------------------------\n",
    "    deltas = delta_scenario(X, scenario, K, smooth)\n",
    "    # ------------------------------------------\n",
    "    # 3) Main effect μ(x)\n",
    "    # ------------------------------------------\n",
    "    mu = main_effect(X)     # (n,)\n",
    "    mu_col = mu.reshape(-1, 1)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 4) True optimal arm = argmax survival time\n",
    "    #    생존시간 ~ 1 / exp(lambda) => lambda가 가장 작은 arm이 최적\n",
    "    # ------------------------------------------\n",
    "    lambda_mat = mu_col + deltas     # (n,K)\n",
    "    true_opt = np.argmin(lambda_mat, axis=1)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 5) 실제 치료 A: 균등 분배\n",
    "    # ------------------------------------------\n",
    "    base = n // K\n",
    "    counts = np.full(K, base, dtype=int)\n",
    "    counts[:(n % K)] += 1\n",
    "    A = np.repeat(np.arange(K), counts)\n",
    "    rng.shuffle(A)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 6) 개별 λ_i 및 rate = exp(λ_i)\n",
    "    # ------------------------------------------\n",
    "    lambdas = lambda_mat[np.arange(n), A]\n",
    "    rates = np.exp(lambdas)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 7) Event time R_true ~ Exp(rate)\n",
    "    # ------------------------------------------\n",
    "    R_true = rng.exponential(scale=1.0/rates)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 8) Censoring times\n",
    "    # ------------------------------------------\n",
    "    C = rng.exponential(scale=theta_cens, size=n)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 9) Observed times\n",
    "    # ------------------------------------------\n",
    "    T = np.minimum(R_true, C)\n",
    "    Delta = (R_true <= C).astype(int)\n",
    "\n",
    "    return X, A, T, Delta, true_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ad_linear_group_survival(\n",
    "    X, A, T, Delta, K,\n",
    "    lam=0.05,\n",
    "    step=1e-2,\n",
    "    max_iter=1000,\n",
    "    tol=1e-6,\n",
    "    pi=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    AD-learning linear model for survival outcome.\n",
    "    - X: (n, p) covariates\n",
    "    - A: (n,) treatment index in {0,...,K-1}\n",
    "    - T: (n,) observed time\n",
    "    - Delta: (n,) event indicator (1=event, 0=censored)\n",
    "    - K: number of treatments\n",
    "\n",
    "    Objective:\n",
    "        Cox partial negative log-likelihood with IPW + group L2 penalty on rows of B.\n",
    "\n",
    "    Hazard score:\n",
    "        F  = X @ B        # (n, K-1)\n",
    "        V  = simplex_vertices(K).numpy()    # (K, K-1)\n",
    "        scores = F @ V.T  # (n, K)\n",
    "        eta_i  = scores[i, A[i]]\n",
    "\n",
    "    Weights:\n",
    "        w_i = Delta_i / (n * pi_i)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: {\"B\": B}\n",
    "    V    : simplex vertices (K, K-1)\n",
    "    predict: function(X_new) -> argmin_k hazard score\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    V = simplex_vertices(K).numpy()  # (K, K-1)\n",
    "\n",
    "    if pi is None:\n",
    "        pi = np.full(n, 1.0 / K, dtype=float)\n",
    "    pi = np.asarray(pi, dtype=float)\n",
    "\n",
    "    # row-wise group soft-threshold\n",
    "    def _group_soft_threshold_rowwise(B, tau):\n",
    "        B_new = B.copy()\n",
    "        norms = np.linalg.norm(B_new, axis=1)  # (p,)\n",
    "        scale = np.maximum(0.0, 1.0 - tau / (norms + 1e-12))\n",
    "        B_new = scale[:, None] * B_new\n",
    "        return B_new\n",
    "\n",
    "    # Cox gradient helper: dL/deta (shape: (n,))\n",
    "    def _cox_grad_eta(eta, T, Delta, pi):\n",
    "        \"\"\"\n",
    "        eta: (n,) linear predictor for actually received treatment\n",
    "        T:   (n,) observed time\n",
    "        Delta: (n,) event indicator\n",
    "        pi:  (n,) propensity\n",
    "        Implements gradient of:\n",
    "            L = - (1/n) sum_{i:Delta_i=1} (1/pi_i) [ eta_i - log sum_{j:T_j>=T_i} exp(eta_j) ]\n",
    "        \"\"\"\n",
    "        n = len(eta)\n",
    "        # weights w_i = Delta_i / (n * pi_i)\n",
    "        w = Delta.astype(float) / (pi * n)\n",
    "\n",
    "        # sort by time ascending\n",
    "        order = np.argsort(T)\n",
    "        T_s = T[order]\n",
    "        eta_s = eta[order]\n",
    "        w_s = w[order]\n",
    "\n",
    "        exp_eta = np.exp(eta_s)\n",
    "\n",
    "        # cumulative risk set sums: R_i = sum_{j: T_j >= T_i} exp(eta_j)\n",
    "        # with T sorted ascending, risk set for i is j >= i\n",
    "        cum_exp_rev = np.cumsum(exp_eta[::-1])[::-1]  # (n,)\n",
    "\n",
    "        grad_s = -w_s.copy()  # start from -w_k term\n",
    "\n",
    "        # add risk-set contributions; only for events (w_s>0)\n",
    "        for i in range(n):\n",
    "            if w_s[i] == 0.0:\n",
    "                continue\n",
    "            denom = cum_exp_rev[i]  # R_i\n",
    "            contrib = w_s[i] * exp_eta[i:] / denom  # shape (n-i,)\n",
    "            grad_s[i:] += contrib\n",
    "\n",
    "        # unsort back\n",
    "        grad = np.zeros_like(grad_s)\n",
    "        grad[order] = grad_s\n",
    "        return grad  # shape (n,)\n",
    "\n",
    "    # initialize B\n",
    "    B = np.zeros((p, K-1), dtype=float)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # forward\n",
    "        F = X @ B                 # (n, K-1)\n",
    "        scores = F @ V.T          # (n, K)\n",
    "        eta = scores[np.arange(n), A]  # (n,)\n",
    "\n",
    "        # gradient wrt eta from Cox partial likelihood\n",
    "        grad_eta = _cox_grad_eta(eta, T, Delta, pi)   # (n,)\n",
    "\n",
    "        # backprop to scores: only column A gets grad_eta\n",
    "        grad_scores = np.zeros_like(scores)\n",
    "        grad_scores[np.arange(n), A] = grad_eta       # (n, K)\n",
    "\n",
    "        # backprop to F: scores = F @ V.T  => dL/dF = grad_scores @ V\n",
    "        grad_F = grad_scores @ V                      # (n, K-1)\n",
    "\n",
    "        # backprop to B: F = X @ B  => dL/dB = X^T @ grad_F\n",
    "        grad_B = X.T @ grad_F                         # (p, K-1)\n",
    "\n",
    "        # gradient step + group l2 proximal\n",
    "        B_next = B - step * grad_B\n",
    "        B_next = _group_soft_threshold_rowwise(B_next, tau=step * lam)\n",
    "\n",
    "        # convergence check\n",
    "        denom = max(1.0, np.linalg.norm(B, ord='fro'))\n",
    "        if np.linalg.norm(B_next - B, ord='fro') <= tol * denom:\n",
    "            B = B_next\n",
    "            break\n",
    "        B = B_next\n",
    "\n",
    "    def predict(X_new):\n",
    "        \"\"\"\n",
    "        Predict optimal treatment index for new X_new.\n",
    "        For survival outcome, hazard가 가장 작은 arm 선택 → argmin.\n",
    "        \"\"\"\n",
    "        F_new = X_new @ B            # (m, K-1)\n",
    "        scores_new = F_new @ V.T     # (m, K)\n",
    "        return scores_new.argmin(axis=1)\n",
    "\n",
    "    model = {\"B\": B}\n",
    "    return model, V, predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, p, out_dim, hidden=[128,128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = p\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def ad_nn_survival(X, A, T, Delta, K, epochs=50, lr=1e-3, pi=None):\n",
    "    \"\"\"\n",
    "    AD-learning 신경망 (생존 outcome, Cox partial likelihood)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X      : (n, p) numpy array, covariates\n",
    "    A      : (n,) numpy array, treatment index (0,...,K-1)\n",
    "    T      : (n,) numpy array, observed time\n",
    "    Delta  : (n,) numpy array, event indicator (1=event, 0=censor)\n",
    "    K      : number of treatments\n",
    "    pi     : (n,) treatment assignment prob; None이면 1/K로 가정\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model  : trained ADNet\n",
    "    V_np   : simplex vertices (K, K-1) as numpy array\n",
    "    predict: function(X_new) -> argmin_k (hazard score)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    A = np.asarray(A, dtype=np.int64)\n",
    "    T = np.asarray(T, dtype=np.float32)\n",
    "    Delta = np.asarray(Delta, dtype=np.float32)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    # simplex vertices\n",
    "    V = simplex_vertices(K).to(device)  # (K, K-1)\n",
    "\n",
    "    # tensors\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    A_t = torch.tensor(A, dtype=torch.long, device=device)\n",
    "    T_t = torch.tensor(T, dtype=torch.float32, device=device)\n",
    "    Delta_t = torch.tensor(Delta, dtype=torch.float32, device=device)\n",
    "\n",
    "    if pi is None:\n",
    "        pi = np.full(n, 1.0 / K, dtype=np.float32)\n",
    "    pi_t = torch.tensor(pi, dtype=torch.float32, device=device)\n",
    "\n",
    "    model = ADNet(p, K-1).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def cox_ipw_loss(eta, T_t, Delta_t, pi_t):\n",
    "        \"\"\"\n",
    "        eta : (n,) log-hazard for actually received treatment\n",
    "        \"\"\"\n",
    "        n = eta.shape[0]\n",
    "        # 정렬 (시간 오름차순)\n",
    "        order = torch.argsort(T_t)\n",
    "        T_s = T_t[order]\n",
    "        eta_s = eta[order]\n",
    "        Delta_s = Delta_t[order]\n",
    "        pi_s = pi_t[order]\n",
    "\n",
    "        exp_eta = torch.exp(eta_s)\n",
    "        # risk set 합: R_i = sum_{j: T_j >= T_i} exp(eta_j)\n",
    "        cum_exp_rev = torch.cumsum(exp_eta.flip(0), dim=0).flip(0)\n",
    "\n",
    "        # event만 골라서 계산\n",
    "        event_mask = (Delta_s > 0)\n",
    "        idx = torch.nonzero(event_mask, as_tuple=False).squeeze()\n",
    "\n",
    "        eta_evt = eta_s[idx]\n",
    "        log_risk_evt = torch.log(cum_exp_rev[idx])\n",
    "        w_evt = 1.0 / pi_s[idx]    # IPW\n",
    "\n",
    "        # - (1/n) * sum w_i [ eta_i - log R_i ]\n",
    "        loss = - (w_evt * (eta_evt - log_risk_evt)).sum() / n\n",
    "        return loss\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        f = model(X_t)              # (n, K-1)\n",
    "        scores = f @ V.T            # (n, K)\n",
    "        eta = scores[torch.arange(n), A_t]  # (n,)\n",
    "\n",
    "        loss = cox_ipw_loss(eta, T_t, Delta_t, pi_t)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"[AD-NN Survival] epoch {ep+1}/{epochs} loss={loss.item():.4f}\")\n",
    "\n",
    "    def predict(X_new):\n",
    "        X_new = np.asarray(X_new, dtype=np.float32)\n",
    "        X_new_t = torch.tensor(X_new, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            f_new = model(X_new_t)          # (m, K-1)\n",
    "            scores_new = f_new @ V.T        # (m, K)\n",
    "            # hazard가 가장 작은 arm 선택\n",
    "            pred = torch.argmin(scores_new, dim=1)\n",
    "        return pred.cpu().numpy()\n",
    "\n",
    "    return model, V.cpu().numpy(), predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_policy_survival(\n",
    "    predict_fn,\n",
    "    X,\n",
    "    A,\n",
    "    T,\n",
    "    Delta,\n",
    "    K,\n",
    "    scenario,\n",
    "    main_effect=mu_default,\n",
    "    smooth=False,\n",
    "    true_opt=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    생존 설정에서 정책 평가.\n",
    "    - error_rate: true_opt가 주어졌을 때 misclassification rate\n",
    "    - value    : 생성모형 기준 기대 생존시간의 평균\n",
    "                 E[ exp(-lambda_{d(X)}(X)) ]\n",
    "\n",
    "    lambda_k(x) = mu(x) + delta_k(x)\n",
    "    delta_k(x)  = delta_scenario(X, scenario, K, smooth)[...,k]\n",
    "    \"\"\"\n",
    "    # 1) 정책의 추천 arm\n",
    "    pred = predict_fn(X)\n",
    "\n",
    "    # 2) 에러율 (ground truth optimal arm과 비교)\n",
    "    error_rate = None if true_opt is None else np.mean(pred != true_opt)\n",
    "\n",
    "    # 3) 생성모형으로부터 lambda_k(x) 계산\n",
    "    deltas = delta_scenario(X, scenario, K, smooth)   # (n, K)\n",
    "    mu = main_effect(X)                               # (n,)\n",
    "    lambda_mat = mu[:, None] + deltas                 # (n, K)\n",
    "\n",
    "    # 4) 정책이 추천한 arm에 대한 기대 생존시간 = exp(-lambda)\n",
    "    #    rate = exp(lambda) 이므로 mean = 1 / rate = exp(-lambda)\n",
    "    chosen_lambda = lambda_mat[np.arange(X.shape[0]), pred]\n",
    "    value = np.mean(np.exp(-chosen_lambda))\n",
    "\n",
    "    return error_rate, value\n",
    "\n",
    "def run_experiments_survival(\n",
    "    K,\n",
    "    p=10,\n",
    "    n=2000,\n",
    "    n_test=10000,\n",
    "    scenario_list=[1, 2, 3],\n",
    "    n_repeats=20,\n",
    "    epochs=60,\n",
    "    lr=1e-3,\n",
    "    lam=0.05,\n",
    "    step=1e-2,\n",
    "    smooth=False,\n",
    "    theta_cens=3.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    생존 outcome용 실험 루프.\n",
    "    - 학습: 크기 n의 train 데이터\n",
    "    - 평가: 크기 n_test의 독립 test 데이터에서\n",
    "            생성모형 기준 value, error rate 측정\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for scenario in scenario_list:\n",
    "        err_lin_list, val_lin_list = [], []\n",
    "        err_nn_list,  val_nn_list  = [], []\n",
    "\n",
    "        for rep in range(n_repeats):\n",
    "            print(f\"[Scenario {scenario}] Repeat {rep+1}/{n_repeats}\")\n",
    "            # --------------------\n",
    "            # 1) Train set\n",
    "            # --------------------\n",
    "            train_seed = 42 + rep\n",
    "            X_tr, A_tr, T_tr, Delta_tr, opt_tr = simulate_data_surv(\n",
    "                K=K,\n",
    "                n=n,\n",
    "                p=p,\n",
    "                scenario=scenario,\n",
    "                seed=train_seed,\n",
    "                theta_cens=theta_cens,\n",
    "                smooth=smooth,\n",
    "            )\n",
    "\n",
    "            # --------------------\n",
    "            # 2) 모델 학습\n",
    "            # --------------------\n",
    "            # (1) Linear AD (survival)\n",
    "            lin_model, V_lin, lin_pred = ad_linear_group_survival(\n",
    "                X_tr, A_tr, T_tr, Delta_tr, K=K,\n",
    "                lam=lam, step=step, max_iter=1000, tol=1e-6, pi=None\n",
    "            )\n",
    "\n",
    "            # (2) NN AD (survival)\n",
    "            nn_model, V_nn, nn_pred = ad_nn_survival(\n",
    "                X_tr, A_tr, T_tr, Delta_tr, K=K,\n",
    "                epochs=epochs, lr=lr, pi=None\n",
    "            )\n",
    "\n",
    "            # --------------------\n",
    "            # 3) Independent Test set\n",
    "            # --------------------\n",
    "            test_seed = 1_000_000 + train_seed\n",
    "            X_te, A_te, T_te, Delta_te, opt_te = simulate_data_surv(\n",
    "                K=K,\n",
    "                n=n_test,\n",
    "                p=p,\n",
    "                scenario=scenario,\n",
    "                seed=test_seed,\n",
    "                theta_cens=theta_cens,\n",
    "                smooth=smooth,\n",
    "            )\n",
    "\n",
    "            # --------------------\n",
    "            # 4) Evaluate (생성모형 기반 value)\n",
    "            # --------------------\n",
    "            err_lin, val_lin = evaluate_policy_survival(\n",
    "                lin_pred, X_te, A_te, T_te, Delta_te,\n",
    "                K=K, scenario=scenario,\n",
    "                main_effect=mu_default,\n",
    "                smooth=smooth,\n",
    "                true_opt=opt_te,\n",
    "            )\n",
    "            err_nn,  val_nn  = evaluate_policy_survival(\n",
    "                nn_pred,  X_te, A_te, T_te, Delta_te,\n",
    "                K=K, scenario=scenario,\n",
    "                main_effect=mu_default,\n",
    "                smooth=smooth,\n",
    "                true_opt=opt_te,\n",
    "            )\n",
    "\n",
    "            err_lin_list.append(err_lin); val_lin_list.append(val_lin)\n",
    "            err_nn_list.append(err_nn);   val_nn_list.append(val_nn)\n",
    "\n",
    "        results[scenario] = {\n",
    "            \"Linear\": {\n",
    "                \"Error_mean\": float(np.mean(err_lin_list)),\n",
    "                \"Error_var\":  float(np.var(err_lin_list, ddof=1)),\n",
    "                \"Value_mean\": float(np.mean(val_lin_list)),\n",
    "                \"Value_var\":  float(np.var(val_lin_list, ddof=1)),\n",
    "            },\n",
    "            \"NN\": {\n",
    "                \"Error_mean\": float(np.mean(err_nn_list)),\n",
    "                \"Error_var\":  float(np.var(err_nn_list, ddof=1)),\n",
    "                \"Value_mean\": float(np.mean(val_nn_list)),\n",
    "                \"Value_var\":  float(np.var(val_nn_list, ddof=1)),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AD-NN Survival] epoch 10/50 loss=23.8971\n",
      "[AD-NN Survival] epoch 20/50 loss=23.6484\n",
      "[AD-NN Survival] epoch 30/50 loss=23.4753\n",
      "[AD-NN Survival] epoch 40/50 loss=23.4082\n",
      "[AD-NN Survival] epoch 50/50 loss=23.3442\n",
      "[AD-NN Survival] epoch 10/50 loss=24.4654\n",
      "[AD-NN Survival] epoch 20/50 loss=24.2012\n",
      "[AD-NN Survival] epoch 30/50 loss=24.0114\n",
      "[AD-NN Survival] epoch 40/50 loss=23.9433\n",
      "[AD-NN Survival] epoch 50/50 loss=23.8820\n",
      "[AD-NN Survival] epoch 10/50 loss=24.3403\n",
      "[AD-NN Survival] epoch 20/50 loss=24.0849\n",
      "[AD-NN Survival] epoch 30/50 loss=23.9210\n",
      "[AD-NN Survival] epoch 40/50 loss=23.8605\n",
      "[AD-NN Survival] epoch 50/50 loss=23.7979\n",
      "[AD-NN Survival] epoch 10/50 loss=24.3439\n",
      "[AD-NN Survival] epoch 20/50 loss=24.0959\n",
      "[AD-NN Survival] epoch 30/50 loss=23.9189\n",
      "[AD-NN Survival] epoch 40/50 loss=23.8529\n",
      "[AD-NN Survival] epoch 50/50 loss=23.7920\n",
      "[AD-NN Survival] epoch 10/50 loss=24.1126\n",
      "[AD-NN Survival] epoch 20/50 loss=23.8277\n",
      "[AD-NN Survival] epoch 30/50 loss=23.6358\n",
      "[AD-NN Survival] epoch 40/50 loss=23.5624\n",
      "[AD-NN Survival] epoch 50/50 loss=23.4994\n",
      "[AD-NN Survival] epoch 10/50 loss=24.5548\n",
      "[AD-NN Survival] epoch 20/50 loss=24.3072\n",
      "[AD-NN Survival] epoch 30/50 loss=24.1348\n",
      "[AD-NN Survival] epoch 40/50 loss=24.0650\n",
      "[AD-NN Survival] epoch 50/50 loss=24.0021\n",
      "[AD-NN Survival] epoch 10/50 loss=24.3001\n",
      "[AD-NN Survival] epoch 20/50 loss=24.0305\n",
      "[AD-NN Survival] epoch 30/50 loss=23.8425\n",
      "[AD-NN Survival] epoch 40/50 loss=23.7687\n",
      "[AD-NN Survival] epoch 50/50 loss=23.7103\n",
      "[AD-NN Survival] epoch 10/50 loss=24.7116\n",
      "[AD-NN Survival] epoch 20/50 loss=24.4436\n",
      "[AD-NN Survival] epoch 30/50 loss=24.2745\n",
      "[AD-NN Survival] epoch 40/50 loss=24.1977\n",
      "[AD-NN Survival] epoch 50/50 loss=24.1385\n",
      "[AD-NN Survival] epoch 10/50 loss=24.2555\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9929\n",
      "[AD-NN Survival] epoch 30/50 loss=23.8140\n",
      "[AD-NN Survival] epoch 40/50 loss=23.7402\n",
      "[AD-NN Survival] epoch 50/50 loss=23.6832\n",
      "[AD-NN Survival] epoch 10/50 loss=24.2546\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9874\n",
      "[AD-NN Survival] epoch 30/50 loss=23.7723\n",
      "[AD-NN Survival] epoch 40/50 loss=23.6827\n",
      "[AD-NN Survival] epoch 50/50 loss=23.6095\n",
      "[AD-NN Survival] epoch 10/50 loss=24.1616\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9193\n",
      "[AD-NN Survival] epoch 30/50 loss=23.7408\n",
      "[AD-NN Survival] epoch 40/50 loss=23.6597\n",
      "[AD-NN Survival] epoch 50/50 loss=23.5973\n",
      "[AD-NN Survival] epoch 10/50 loss=24.0016\n",
      "[AD-NN Survival] epoch 20/50 loss=23.7286\n",
      "[AD-NN Survival] epoch 30/50 loss=23.5356\n",
      "[AD-NN Survival] epoch 40/50 loss=23.4708\n",
      "[AD-NN Survival] epoch 50/50 loss=23.4104\n",
      "[AD-NN Survival] epoch 10/50 loss=24.2147\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9515\n",
      "[AD-NN Survival] epoch 30/50 loss=23.7910\n",
      "[AD-NN Survival] epoch 40/50 loss=23.7375\n",
      "[AD-NN Survival] epoch 50/50 loss=23.6867\n",
      "[AD-NN Survival] epoch 10/50 loss=24.1835\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9005\n",
      "[AD-NN Survival] epoch 30/50 loss=23.7081\n",
      "[AD-NN Survival] epoch 40/50 loss=23.6466\n",
      "[AD-NN Survival] epoch 50/50 loss=23.5907\n",
      "[AD-NN Survival] epoch 10/50 loss=24.0261\n",
      "[AD-NN Survival] epoch 20/50 loss=23.7844\n",
      "[AD-NN Survival] epoch 30/50 loss=23.5891\n",
      "[AD-NN Survival] epoch 40/50 loss=23.5064\n",
      "[AD-NN Survival] epoch 50/50 loss=23.4440\n",
      "[AD-NN Survival] epoch 10/50 loss=24.4429\n",
      "[AD-NN Survival] epoch 20/50 loss=24.1890\n",
      "[AD-NN Survival] epoch 30/50 loss=23.9539\n",
      "[AD-NN Survival] epoch 40/50 loss=23.8378\n",
      "[AD-NN Survival] epoch 50/50 loss=23.7651\n",
      "[AD-NN Survival] epoch 10/50 loss=23.9288\n",
      "[AD-NN Survival] epoch 20/50 loss=23.6622\n",
      "[AD-NN Survival] epoch 30/50 loss=23.4372\n",
      "[AD-NN Survival] epoch 40/50 loss=23.3573\n",
      "[AD-NN Survival] epoch 50/50 loss=23.3015\n",
      "[AD-NN Survival] epoch 10/50 loss=24.2717\n",
      "[AD-NN Survival] epoch 20/50 loss=23.9964\n",
      "[AD-NN Survival] epoch 30/50 loss=23.8482\n",
      "[AD-NN Survival] epoch 40/50 loss=23.7946\n",
      "[AD-NN Survival] epoch 50/50 loss=23.7352\n",
      "[AD-NN Survival] epoch 10/50 loss=24.5050\n",
      "[AD-NN Survival] epoch 20/50 loss=24.2320\n",
      "[AD-NN Survival] epoch 30/50 loss=24.0374\n",
      "[AD-NN Survival] epoch 40/50 loss=23.9672\n",
      "[AD-NN Survival] epoch 50/50 loss=23.8937\n",
      "[AD-NN Survival] epoch 10/50 loss=24.4665\n",
      "[AD-NN Survival] epoch 20/50 loss=24.2062\n",
      "[AD-NN Survival] epoch 30/50 loss=24.0420\n",
      "[AD-NN Survival] epoch 40/50 loss=23.9853\n",
      "[AD-NN Survival] epoch 50/50 loss=23.9238\n",
      "[AD-NN Survival] epoch 10/50 loss=21.9049\n",
      "[AD-NN Survival] epoch 20/50 loss=21.6417\n",
      "[AD-NN Survival] epoch 30/50 loss=21.4253\n",
      "[AD-NN Survival] epoch 40/50 loss=21.3324\n",
      "[AD-NN Survival] epoch 50/50 loss=21.2670\n",
      "[AD-NN Survival] epoch 10/50 loss=22.3694\n",
      "[AD-NN Survival] epoch 20/50 loss=22.1448\n",
      "[AD-NN Survival] epoch 30/50 loss=21.9290\n",
      "[AD-NN Survival] epoch 40/50 loss=21.7972\n",
      "[AD-NN Survival] epoch 50/50 loss=21.7419\n",
      "[AD-NN Survival] epoch 10/50 loss=22.3160\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0670\n",
      "[AD-NN Survival] epoch 30/50 loss=21.8278\n",
      "[AD-NN Survival] epoch 40/50 loss=21.7106\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6561\n",
      "[AD-NN Survival] epoch 10/50 loss=22.0647\n",
      "[AD-NN Survival] epoch 20/50 loss=21.8311\n",
      "[AD-NN Survival] epoch 30/50 loss=21.6105\n",
      "[AD-NN Survival] epoch 40/50 loss=21.4963\n",
      "[AD-NN Survival] epoch 50/50 loss=21.4345\n",
      "[AD-NN Survival] epoch 10/50 loss=22.0774\n",
      "[AD-NN Survival] epoch 20/50 loss=21.8253\n",
      "[AD-NN Survival] epoch 30/50 loss=21.5730\n",
      "[AD-NN Survival] epoch 40/50 loss=21.4188\n",
      "[AD-NN Survival] epoch 50/50 loss=21.3555\n",
      "[AD-NN Survival] epoch 10/50 loss=22.5854\n",
      "[AD-NN Survival] epoch 20/50 loss=22.3334\n",
      "[AD-NN Survival] epoch 30/50 loss=22.1376\n",
      "[AD-NN Survival] epoch 40/50 loss=22.0575\n",
      "[AD-NN Survival] epoch 50/50 loss=21.9976\n",
      "[AD-NN Survival] epoch 10/50 loss=22.1195\n",
      "[AD-NN Survival] epoch 20/50 loss=21.8976\n",
      "[AD-NN Survival] epoch 30/50 loss=21.6942\n",
      "[AD-NN Survival] epoch 40/50 loss=21.5952\n",
      "[AD-NN Survival] epoch 50/50 loss=21.5446\n",
      "[AD-NN Survival] epoch 10/50 loss=22.5163\n",
      "[AD-NN Survival] epoch 20/50 loss=22.2635\n",
      "[AD-NN Survival] epoch 30/50 loss=22.0381\n",
      "[AD-NN Survival] epoch 40/50 loss=21.9162\n",
      "[AD-NN Survival] epoch 50/50 loss=21.8566\n",
      "[AD-NN Survival] epoch 10/50 loss=22.4402\n",
      "[AD-NN Survival] epoch 20/50 loss=22.2101\n",
      "[AD-NN Survival] epoch 30/50 loss=22.0104\n",
      "[AD-NN Survival] epoch 40/50 loss=21.9294\n",
      "[AD-NN Survival] epoch 50/50 loss=21.8717\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2002\n",
      "[AD-NN Survival] epoch 20/50 loss=21.9815\n",
      "[AD-NN Survival] epoch 30/50 loss=21.7677\n",
      "[AD-NN Survival] epoch 40/50 loss=21.6511\n",
      "[AD-NN Survival] epoch 50/50 loss=21.5964\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2556\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0064\n",
      "[AD-NN Survival] epoch 30/50 loss=21.7656\n",
      "[AD-NN Survival] epoch 40/50 loss=21.6591\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6003\n",
      "[AD-NN Survival] epoch 10/50 loss=22.0104\n",
      "[AD-NN Survival] epoch 20/50 loss=21.7772\n",
      "[AD-NN Survival] epoch 30/50 loss=21.5560\n",
      "[AD-NN Survival] epoch 40/50 loss=21.4335\n",
      "[AD-NN Survival] epoch 50/50 loss=21.3684\n",
      "[AD-NN Survival] epoch 10/50 loss=22.3071\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0637\n",
      "[AD-NN Survival] epoch 30/50 loss=21.8819\n",
      "[AD-NN Survival] epoch 40/50 loss=21.8146\n",
      "[AD-NN Survival] epoch 50/50 loss=21.7460\n",
      "[AD-NN Survival] epoch 10/50 loss=22.3099\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0359\n",
      "[AD-NN Survival] epoch 30/50 loss=21.8128\n",
      "[AD-NN Survival] epoch 40/50 loss=21.7158\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6453\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2751\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0021\n",
      "[AD-NN Survival] epoch 30/50 loss=21.7732\n",
      "[AD-NN Survival] epoch 40/50 loss=21.6724\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6100\n",
      "[AD-NN Survival] epoch 10/50 loss=22.1944\n",
      "[AD-NN Survival] epoch 20/50 loss=21.9558\n",
      "[AD-NN Survival] epoch 30/50 loss=21.7325\n",
      "[AD-NN Survival] epoch 40/50 loss=21.6100\n",
      "[AD-NN Survival] epoch 50/50 loss=21.5512\n",
      "[AD-NN Survival] epoch 10/50 loss=21.8284\n",
      "[AD-NN Survival] epoch 20/50 loss=21.5651\n",
      "[AD-NN Survival] epoch 30/50 loss=21.3552\n",
      "[AD-NN Survival] epoch 40/50 loss=21.2661\n",
      "[AD-NN Survival] epoch 50/50 loss=21.2016\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2878\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0377\n",
      "[AD-NN Survival] epoch 30/50 loss=21.8124\n",
      "[AD-NN Survival] epoch 40/50 loss=21.7008\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6424\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2463\n",
      "[AD-NN Survival] epoch 20/50 loss=21.9962\n",
      "[AD-NN Survival] epoch 30/50 loss=21.8247\n",
      "[AD-NN Survival] epoch 40/50 loss=21.7533\n",
      "[AD-NN Survival] epoch 50/50 loss=21.6907\n",
      "[AD-NN Survival] epoch 10/50 loss=22.2676\n",
      "[AD-NN Survival] epoch 20/50 loss=22.0220\n",
      "[AD-NN Survival] epoch 30/50 loss=21.7772\n",
      "[AD-NN Survival] epoch 40/50 loss=21.6549\n",
      "[AD-NN Survival] epoch 50/50 loss=21.5936\n",
      "[AD-NN Survival] epoch 10/50 loss=22.8407\n",
      "[AD-NN Survival] epoch 20/50 loss=22.7699\n",
      "[AD-NN Survival] epoch 30/50 loss=22.7018\n",
      "[AD-NN Survival] epoch 40/50 loss=22.6231\n",
      "[AD-NN Survival] epoch 50/50 loss=22.5275\n",
      "[AD-NN Survival] epoch 10/50 loss=23.5016\n",
      "[AD-NN Survival] epoch 20/50 loss=23.4382\n",
      "[AD-NN Survival] epoch 30/50 loss=23.3751\n",
      "[AD-NN Survival] epoch 40/50 loss=23.2999\n",
      "[AD-NN Survival] epoch 50/50 loss=23.2073\n",
      "[AD-NN Survival] epoch 10/50 loss=23.1915\n",
      "[AD-NN Survival] epoch 20/50 loss=23.1230\n",
      "[AD-NN Survival] epoch 30/50 loss=23.0549\n",
      "[AD-NN Survival] epoch 40/50 loss=22.9699\n",
      "[AD-NN Survival] epoch 50/50 loss=22.8654\n",
      "[AD-NN Survival] epoch 10/50 loss=22.9512\n",
      "[AD-NN Survival] epoch 20/50 loss=22.8864\n",
      "[AD-NN Survival] epoch 30/50 loss=22.8205\n",
      "[AD-NN Survival] epoch 40/50 loss=22.7395\n",
      "[AD-NN Survival] epoch 50/50 loss=22.6408\n",
      "[AD-NN Survival] epoch 10/50 loss=23.0448\n",
      "[AD-NN Survival] epoch 20/50 loss=22.9804\n",
      "[AD-NN Survival] epoch 30/50 loss=22.9099\n",
      "[AD-NN Survival] epoch 40/50 loss=22.8191\n",
      "[AD-NN Survival] epoch 50/50 loss=22.7085\n",
      "[AD-NN Survival] epoch 10/50 loss=23.3832\n",
      "[AD-NN Survival] epoch 20/50 loss=23.3135\n",
      "[AD-NN Survival] epoch 30/50 loss=23.2361\n",
      "[AD-NN Survival] epoch 40/50 loss=23.1417\n",
      "[AD-NN Survival] epoch 50/50 loss=23.0282\n",
      "[AD-NN Survival] epoch 10/50 loss=23.5260\n",
      "[AD-NN Survival] epoch 20/50 loss=23.4643\n",
      "[AD-NN Survival] epoch 30/50 loss=23.3908\n",
      "[AD-NN Survival] epoch 40/50 loss=23.2982\n",
      "[AD-NN Survival] epoch 50/50 loss=23.1877\n",
      "[AD-NN Survival] epoch 10/50 loss=23.3132\n",
      "[AD-NN Survival] epoch 20/50 loss=23.2550\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1882\n",
      "[AD-NN Survival] epoch 40/50 loss=23.1036\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9989\n",
      "[AD-NN Survival] epoch 10/50 loss=23.3271\n",
      "[AD-NN Survival] epoch 20/50 loss=23.2650\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1909\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0949\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9802\n",
      "[AD-NN Survival] epoch 10/50 loss=23.2590\n",
      "[AD-NN Survival] epoch 20/50 loss=23.1814\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1065\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0209\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9218\n",
      "[AD-NN Survival] epoch 10/50 loss=23.2761\n",
      "[AD-NN Survival] epoch 20/50 loss=23.2153\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1505\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0697\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9692\n",
      "[AD-NN Survival] epoch 10/50 loss=22.9085\n",
      "[AD-NN Survival] epoch 20/50 loss=22.8490\n",
      "[AD-NN Survival] epoch 30/50 loss=22.7817\n",
      "[AD-NN Survival] epoch 40/50 loss=22.6977\n",
      "[AD-NN Survival] epoch 50/50 loss=22.5933\n",
      "[AD-NN Survival] epoch 10/50 loss=23.0531\n",
      "[AD-NN Survival] epoch 20/50 loss=22.9901\n",
      "[AD-NN Survival] epoch 30/50 loss=22.9224\n",
      "[AD-NN Survival] epoch 40/50 loss=22.8368\n",
      "[AD-NN Survival] epoch 50/50 loss=22.7307\n",
      "[AD-NN Survival] epoch 10/50 loss=23.2152\n",
      "[AD-NN Survival] epoch 20/50 loss=23.1557\n",
      "[AD-NN Survival] epoch 30/50 loss=23.0894\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0059\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9062\n",
      "[AD-NN Survival] epoch 10/50 loss=22.9094\n",
      "[AD-NN Survival] epoch 20/50 loss=22.8340\n",
      "[AD-NN Survival] epoch 30/50 loss=22.7549\n",
      "[AD-NN Survival] epoch 40/50 loss=22.6590\n",
      "[AD-NN Survival] epoch 50/50 loss=22.5477\n",
      "[AD-NN Survival] epoch 10/50 loss=23.4148\n",
      "[AD-NN Survival] epoch 20/50 loss=23.3456\n",
      "[AD-NN Survival] epoch 30/50 loss=23.2643\n",
      "[AD-NN Survival] epoch 40/50 loss=23.1603\n",
      "[AD-NN Survival] epoch 50/50 loss=23.0351\n",
      "[AD-NN Survival] epoch 10/50 loss=22.6763\n",
      "[AD-NN Survival] epoch 20/50 loss=22.6090\n",
      "[AD-NN Survival] epoch 30/50 loss=22.5338\n",
      "[AD-NN Survival] epoch 40/50 loss=22.4395\n",
      "[AD-NN Survival] epoch 50/50 loss=22.3247\n",
      "[AD-NN Survival] epoch 10/50 loss=23.2389\n",
      "[AD-NN Survival] epoch 20/50 loss=23.1818\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1183\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0362\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9311\n",
      "[AD-NN Survival] epoch 10/50 loss=23.2633\n",
      "[AD-NN Survival] epoch 20/50 loss=23.2034\n",
      "[AD-NN Survival] epoch 30/50 loss=23.1357\n",
      "[AD-NN Survival] epoch 40/50 loss=23.0513\n",
      "[AD-NN Survival] epoch 50/50 loss=22.9491\n",
      "[AD-NN Survival] epoch 10/50 loss=23.5034\n",
      "[AD-NN Survival] epoch 20/50 loss=23.4381\n",
      "[AD-NN Survival] epoch 30/50 loss=23.3720\n",
      "[AD-NN Survival] epoch 40/50 loss=23.2938\n",
      "[AD-NN Survival] epoch 50/50 loss=23.1990\n",
      "\n",
      "========== Scenario 1 ==========\n",
      "Linear  | Error mean=0.092, var=0.0004 | Value mean=0.912, var=0.0005\n",
      "NN      | Error mean=0.163, var=0.0003 | Value mean=0.897, var=0.0006\n",
      "\n",
      "========== Scenario 2 ==========\n",
      "Linear  | Error mean=0.236, var=0.0003 | Value mean=2.017, var=0.0004\n",
      "NN      | Error mean=0.180, var=0.0003 | Value mean=1.997, var=0.0017\n",
      "\n",
      "========== Scenario 3 ==========\n",
      "Linear  | Error mean=0.676, var=0.0013 | Value mean=0.664, var=0.0033\n",
      "NN      | Error mean=0.566, var=0.0006 | Value mean=0.760, var=0.0006\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    res_surv = run_experiments_survival(\n",
    "        K=4, p=10, n=2000, n_test=10000,\n",
    "        n_repeats=20, epochs=50, lr=1e-3,\n",
    "        lam=0.05, step=1e-2,\n",
    "        smooth=True, theta_cens=3.0,\n",
    "    )\n",
    "\n",
    "    for scen, res in res_surv.items():\n",
    "        print(\"\\n\" + \"=\"*10, f\"Scenario {scen}\", \"=\"*10)\n",
    "        for model_name, stats in res.items():\n",
    "            print(f\"{model_name:7s} | \"\n",
    "                  f\"Error mean={stats['Error_mean']:.3f}, var={stats['Error_var']:.4f} | \"\n",
    "                  f\"Value mean={stats['Value_mean']:.3f}, var={stats['Value_var']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Obtaining dependency information for ucimlrepo from https://files.pythonhosted.org/packages/3b/07/1252560194df2b4fad1cb3c46081b948331c63eb1bb0b97620d508d12a53/ucimlrepo-0.0.7-py3-none-any.whl.metadata\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from ucimlrepo) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/heejun/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "aids_clinical_trials_group_study_175 = fetch_ucirepo(id=890) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df = aids_clinical_trials_group_study_175.data.features \n",
    "#y = aids_clinical_trials_group_study_175.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "COVS = [\"age\",\"wtkg\",\"cd80\",\"karnof\",\n",
    "        \"gender\",\"race\",\"homo\",\"drugs\",\"symptom\",\"str2\",\"hemo\"]\n",
    "\n",
    "def actg_to_arrays(df: pd.DataFrame, covs=COVS, keep_R_pos=False, R_log=False):\n",
    "    \"\"\"\n",
    "    ACTG175 데이터프레임 -> (X, A, R, meta)\n",
    "    trt 열이 이미 {0,1,2,3}로 코딩되어 있다고 가정\n",
    "    \"\"\"\n",
    "    # 치료: 이미 0,1,2,3\n",
    "    A = df[\"trt\"].to_numpy().astype(int)\n",
    "\n",
    "    # Reward: cd420 - cd40\n",
    "    R = df[\"cd420\"].astype(float) - df[\"cd40\"].astype(float)\n",
    "\n",
    "    # X: 지정된 공변량\n",
    "    X = df[covs].to_numpy(dtype=float)\n",
    "\n",
    "    # 옵션: R > 0만 유지\n",
    "    if keep_R_pos:\n",
    "        mask = (R > 0)\n",
    "        X, A, R = X[mask], A[mask], R[mask]\n",
    "        if R_log:\n",
    "            R = np.log1p(R)\n",
    "            # if minmax:\n",
    "            #     z = (z - z.min()) / (z.max() - z.min() + 1e-12)\n",
    "\n",
    "\n",
    "    meta = {\"covs\": covs, \"K\": len(np.unique(A)), \"n\": len(A)}\n",
    "    return X, A, R, meta\n",
    "\n",
    "X, A, R, meta = actg_to_arrays(df, keep_R_pos=True, R_log=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# -----------------------\n",
    "# 평가 함수 (균등 무작위 배정 가정)\n",
    "# -----------------------\n",
    "def value_uniform(pred, A, R, K):\n",
    "    pred = np.asarray(pred).reshape(-1)\n",
    "    A    = np.asarray(A, dtype=int).reshape(-1)\n",
    "    R    = np.asarray(R, dtype=float).reshape(-1)\n",
    "    assert pred.shape == A.shape == R.shape, \"pred/A/R length mismatch\"\n",
    "    return np.mean((pred == A).astype(float) * R * K)\n",
    "\n",
    "# -----------------------\n",
    "# 유틸: 예측함수 안전화\n",
    "# -----------------------\n",
    "def _safe_predict(predict_fn, X_te):\n",
    "    pred = predict_fn(X_te)\n",
    "    pred = np.asarray(pred)\n",
    "    if pred.ndim != 1:\n",
    "        pred = pred.reshape(-1)\n",
    "    return pred.astype(int)\n",
    "\n",
    "\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, p, out_dim, hidden=[128,128,128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = p\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 한 번의 5-fold CV에서 value 계산 (층화 + 견고)\n",
    "# -----------------------\n",
    "def cv5_value_once(train_and_build_predict, X, A, R, K, seed, verbose=False):\n",
    "    \"\"\"\n",
    "    train_and_build_predict: (X_tr, A_tr, R_tr, K) -> predict_fn\n",
    "    반환: 이번 5-fold에서의 평균 value (fold 크기 가중 평균)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    A = np.asarray(A, dtype=int)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    vals, sizes = [], []\n",
    "\n",
    "    for fold_idx, (tr_idx, te_idx) in enumerate(skf.split(X, A), start=1):\n",
    "        X_tr, A_tr, R_tr = X[tr_idx], A[tr_idx], R[tr_idx]\n",
    "        X_te, A_te, R_te = X[te_idx], A[te_idx], R[te_idx]\n",
    "\n",
    "        try:\n",
    "            predict_fn = train_and_build_predict(X_tr, A_tr, R_tr, K)\n",
    "            pred_te = _safe_predict(predict_fn, X_te)\n",
    "            v = value_uniform(pred_te, A_te, R_te, K)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                uniq, cnts = np.unique(A_tr, return_counts=True)\n",
    "                msg = (f\"[cv5] fold={fold_idx} failed | \"\n",
    "                       f\"train sizes per arm={dict(zip(uniq.tolist(), cnts.tolist()))} | \"\n",
    "                       f\"err: {repr(e)}\")\n",
    "                print(msg)\n",
    "            raise\n",
    "        vals.append(v)\n",
    "        sizes.append(len(te_idx))\n",
    "\n",
    "    vals, sizes = np.asarray(vals), np.asarray(sizes)\n",
    "    return float(np.sum(vals * sizes) / np.sum(sizes))\n",
    "\n",
    "# -----------------------\n",
    "# 5-fold CV 반복\n",
    "# -----------------------\n",
    "def repeated_cv_value(train_and_build_predict, X, A, R, K,\n",
    "                      n_repeats=1000, base_seed=2025, verbose=False):\n",
    "    values = np.zeros(n_repeats, dtype=float)\n",
    "    for r in range(n_repeats):\n",
    "        seed = base_seed + r\n",
    "        values[r] = cv5_value_once(train_and_build_predict, X, A, R, K, seed, verbose=verbose)\n",
    "    return {\n",
    "        \"values\": values,\n",
    "        \"mean\": float(values.mean()),\n",
    "        \"std\": float(values.std(ddof=1)),\n",
    "        \"n_repeats\": int(n_repeats)\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# 빌더 (네 ad_* 시그니처에 맞춰 타입 강제)\n",
    "# -----------------------\n",
    "def build_predict_ad_linear(Xtr, Atr, Rtr, K, alpha=1.0):\n",
    "    Xtr = np.asarray(Xtr, dtype=float)\n",
    "    Atr = np.asarray(Atr, dtype=int)\n",
    "    Rtr = np.asarray(Rtr, dtype=float)\n",
    "    model, V, predict = ad_linear_group(Xtr, Atr, Rtr, K=K, alpha=alpha)\n",
    "    return predict  # predict(X_new) -> np.int64 1D 기대\n",
    "\n",
    "def build_predict_ad_nn(Xtr, Atr, Rtr, K, epochs=60, lr=1e-3):\n",
    "    Xtr = np.asarray(Xtr, dtype=float)\n",
    "    Atr = np.asarray(Atr, dtype=int)\n",
    "    Rtr = np.asarray(Rtr, dtype=float)\n",
    "    model, V, predict = ad_nn(Xtr, Atr, Rtr, K=K, epochs=epochs, lr=lr)\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2557: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "/Users/heejun/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:2556: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] epoch 10/10 loss=18796.9355\n",
      "[NN] epoch 10/10 loss=17912.4238\n",
      "[NN] epoch 10/10 loss=17274.2891\n",
      "[NN] epoch 10/10 loss=17051.3652\n",
      "[NN] epoch 10/10 loss=18886.9531\n",
      "[NN] epoch 10/10 loss=18451.3809\n",
      "[NN] epoch 10/10 loss=18242.2344\n",
      "[NN] epoch 10/10 loss=17231.3945\n",
      "[NN] epoch 10/10 loss=18076.0762\n",
      "[NN] epoch 10/10 loss=17908.6250\n",
      "[NN] epoch 10/10 loss=18852.1387\n",
      "[NN] epoch 10/10 loss=16411.7656\n",
      "[NN] epoch 10/10 loss=18580.0566\n",
      "[NN] epoch 10/10 loss=18397.4023\n",
      "[NN] epoch 10/10 loss=17741.8027\n",
      "[NN] epoch 10/10 loss=18266.9004\n",
      "[NN] epoch 10/10 loss=17710.6758\n",
      "[NN] epoch 10/10 loss=18671.6074\n",
      "[NN] epoch 10/10 loss=17836.8125\n",
      "[NN] epoch 10/10 loss=17313.8574\n",
      "[NN] epoch 10/10 loss=17974.7852\n",
      "[NN] epoch 10/10 loss=18244.7207\n",
      "[NN] epoch 10/10 loss=17314.4727\n",
      "[NN] epoch 10/10 loss=18339.7930\n",
      "[NN] epoch 10/10 loss=18109.1895\n",
      "[NN] epoch 10/10 loss=18344.3320\n",
      "[NN] epoch 10/10 loss=18033.3652\n",
      "[NN] epoch 10/10 loss=17833.1016\n",
      "[NN] epoch 10/10 loss=18043.0527\n",
      "[NN] epoch 10/10 loss=17654.2871\n",
      "[NN] epoch 10/10 loss=18043.5000\n",
      "[NN] epoch 10/10 loss=17958.4102\n",
      "[NN] epoch 10/10 loss=18097.5371\n",
      "[NN] epoch 10/10 loss=18485.1016\n",
      "[NN] epoch 10/10 loss=17185.1133\n",
      "[NN] epoch 10/10 loss=17314.0527\n",
      "[NN] epoch 10/10 loss=18906.1250\n",
      "[NN] epoch 10/10 loss=17029.6406\n",
      "[NN] epoch 10/10 loss=18611.9512\n",
      "[NN] epoch 10/10 loss=18076.6074\n",
      "[NN] epoch 10/10 loss=18250.4727\n",
      "[NN] epoch 10/10 loss=17780.7676\n",
      "[NN] epoch 10/10 loss=18088.3770\n",
      "[NN] epoch 10/10 loss=17544.9062\n",
      "[NN] epoch 10/10 loss=18191.6426\n",
      "[NN] epoch 10/10 loss=17148.0684\n",
      "[NN] epoch 10/10 loss=18775.5449\n",
      "[NN] epoch 10/10 loss=17944.7305\n",
      "[NN] epoch 10/10 loss=18867.9473\n",
      "[NN] epoch 10/10 loss=17060.2734\n",
      "[NN] epoch 10/10 loss=18457.7520\n",
      "[NN] epoch 10/10 loss=18574.2324\n",
      "[NN] epoch 10/10 loss=16983.7188\n",
      "[NN] epoch 10/10 loss=18327.6582\n",
      "[NN] epoch 10/10 loss=17342.9102\n",
      "[NN] epoch 10/10 loss=17875.2402\n",
      "[NN] epoch 10/10 loss=17673.4004\n",
      "[NN] epoch 10/10 loss=18561.3047\n",
      "[NN] epoch 10/10 loss=17530.9297\n",
      "[NN] epoch 10/10 loss=18122.5996\n",
      "[NN] epoch 10/10 loss=17763.3398\n",
      "[NN] epoch 10/10 loss=17681.0312\n",
      "[NN] epoch 10/10 loss=17935.1191\n",
      "[NN] epoch 10/10 loss=17974.2363\n",
      "[NN] epoch 10/10 loss=18286.0410\n",
      "[NN] epoch 10/10 loss=18026.6738\n",
      "[NN] epoch 10/10 loss=18085.0645\n",
      "[NN] epoch 10/10 loss=17885.5977\n",
      "[NN] epoch 10/10 loss=18008.7715\n",
      "[NN] epoch 10/10 loss=17839.3027\n",
      "[NN] epoch 10/10 loss=18210.9434\n",
      "[NN] epoch 10/10 loss=18164.6074\n",
      "[NN] epoch 10/10 loss=17509.3301\n",
      "[NN] epoch 10/10 loss=17202.6055\n",
      "[NN] epoch 10/10 loss=18674.8984\n",
      "[NN] epoch 10/10 loss=17514.4277\n",
      "[NN] epoch 10/10 loss=18192.5312\n",
      "[NN] epoch 10/10 loss=18168.8047\n",
      "[NN] epoch 10/10 loss=18347.0781\n",
      "[NN] epoch 10/10 loss=17713.2578\n",
      "[NN] epoch 10/10 loss=17665.2695\n",
      "[NN] epoch 10/10 loss=17421.2324\n",
      "[NN] epoch 10/10 loss=17726.1777\n",
      "[NN] epoch 10/10 loss=18504.4922\n",
      "[NN] epoch 10/10 loss=18470.8516\n",
      "[NN] epoch 10/10 loss=17975.4453\n",
      "[NN] epoch 10/10 loss=17917.2363\n",
      "[NN] epoch 10/10 loss=17507.0898\n",
      "[NN] epoch 10/10 loss=18321.8711\n",
      "[NN] epoch 10/10 loss=17978.9609\n",
      "[NN] epoch 10/10 loss=18066.5469\n",
      "[NN] epoch 10/10 loss=18093.5098\n",
      "[NN] epoch 10/10 loss=17384.8730\n",
      "[NN] epoch 10/10 loss=17703.0918\n",
      "[NN] epoch 10/10 loss=18513.7520\n",
      "[NN] epoch 10/10 loss=17854.8516\n",
      "[NN] epoch 10/10 loss=17578.3945\n",
      "[NN] epoch 10/10 loss=17424.0566\n",
      "[NN] epoch 10/10 loss=18339.0527\n",
      "[NN] epoch 10/10 loss=18707.5820\n",
      "[NN] epoch 10/10 loss=18144.3105\n",
      "[NN] epoch 10/10 loss=18259.7969\n",
      "[NN] epoch 10/10 loss=18474.1680\n",
      "[NN] epoch 10/10 loss=17910.8496\n",
      "[NN] epoch 10/10 loss=17084.0840\n",
      "[NN] epoch 10/10 loss=17763.8574\n",
      "[NN] epoch 10/10 loss=18069.2871\n",
      "[NN] epoch 10/10 loss=18309.4453\n",
      "[NN] epoch 10/10 loss=18043.0801\n",
      "[NN] epoch 10/10 loss=17586.3340\n",
      "[NN] epoch 10/10 loss=18026.9375\n",
      "[NN] epoch 10/10 loss=18375.2422\n",
      "[NN] epoch 10/10 loss=17388.1875\n",
      "[NN] epoch 10/10 loss=18016.6895\n",
      "[NN] epoch 10/10 loss=17804.5566\n",
      "[NN] epoch 10/10 loss=18620.5801\n",
      "[NN] epoch 10/10 loss=16781.9766\n",
      "[NN] epoch 10/10 loss=18678.2402\n",
      "[NN] epoch 10/10 loss=18202.9102\n",
      "[NN] epoch 10/10 loss=17493.8633\n",
      "[NN] epoch 10/10 loss=17533.3418\n",
      "[NN] epoch 10/10 loss=17422.8164\n",
      "[NN] epoch 10/10 loss=18148.5996\n",
      "[NN] epoch 10/10 loss=18211.7871\n",
      "[NN] epoch 10/10 loss=18443.9902\n",
      "[NN] epoch 10/10 loss=18289.8242\n",
      "[NN] epoch 10/10 loss=17655.6660\n",
      "[NN] epoch 10/10 loss=17365.3887\n",
      "[NN] epoch 10/10 loss=18038.3164\n",
      "[NN] epoch 10/10 loss=18441.0410\n",
      "[NN] epoch 10/10 loss=18149.5488\n",
      "[NN] epoch 10/10 loss=18162.7461\n",
      "[NN] epoch 10/10 loss=18226.8535\n",
      "[NN] epoch 10/10 loss=17986.6230\n",
      "[NN] epoch 10/10 loss=17503.1719\n",
      "[NN] epoch 10/10 loss=17314.9180\n",
      "[NN] epoch 10/10 loss=18290.7129\n",
      "[NN] epoch 10/10 loss=18143.5332\n",
      "[NN] epoch 10/10 loss=18404.4863\n",
      "[NN] epoch 10/10 loss=17632.0918\n",
      "[NN] epoch 10/10 loss=18663.7832\n",
      "[NN] epoch 10/10 loss=17803.0332\n",
      "[NN] epoch 10/10 loss=18238.8398\n",
      "[NN] epoch 10/10 loss=18657.2539\n",
      "[NN] epoch 10/10 loss=16431.5586\n",
      "[NN] epoch 10/10 loss=17682.4668\n",
      "[NN] epoch 10/10 loss=18125.4492\n",
      "[NN] epoch 10/10 loss=18126.8457\n",
      "[NN] epoch 10/10 loss=18128.5645\n",
      "[NN] epoch 10/10 loss=17756.7539\n",
      "[NN] epoch 10/10 loss=17868.4805\n",
      "[NN] epoch 10/10 loss=16312.0732\n",
      "[NN] epoch 10/10 loss=18814.3906\n",
      "[NN] epoch 10/10 loss=18666.7910\n",
      "[NN] epoch 10/10 loss=18189.9414\n",
      "[NN] epoch 10/10 loss=18111.2871\n",
      "[NN] epoch 10/10 loss=18757.3203\n",
      "[NN] epoch 10/10 loss=17170.7285\n",
      "[NN] epoch 10/10 loss=17571.1738\n",
      "[NN] epoch 10/10 loss=18205.7207\n",
      "[NN] epoch 10/10 loss=18525.2559\n",
      "[NN] epoch 10/10 loss=18028.1406\n",
      "[NN] epoch 10/10 loss=18000.3359\n",
      "[NN] epoch 10/10 loss=18318.5879\n",
      "[NN] epoch 10/10 loss=16940.3281\n",
      "[NN] epoch 10/10 loss=18491.7227\n",
      "[NN] epoch 10/10 loss=18794.4531\n",
      "[NN] epoch 10/10 loss=17855.4590\n",
      "[NN] epoch 10/10 loss=17084.3672\n",
      "[NN] epoch 10/10 loss=17648.3867\n",
      "[NN] epoch 10/10 loss=18166.7402\n",
      "[NN] epoch 10/10 loss=18798.0527\n",
      "[NN] epoch 10/10 loss=18355.5723\n",
      "[NN] epoch 10/10 loss=17245.7090\n",
      "[NN] epoch 10/10 loss=17264.1758\n",
      "[NN] epoch 10/10 loss=17934.5332\n",
      "[NN] epoch 10/10 loss=17786.6074\n",
      "[NN] epoch 10/10 loss=18557.1035\n",
      "[NN] epoch 10/10 loss=18322.1523\n",
      "[NN] epoch 10/10 loss=17134.2734\n",
      "[NN] epoch 10/10 loss=17843.7109\n",
      "[NN] epoch 10/10 loss=18947.9277\n",
      "[NN] epoch 10/10 loss=18313.2852\n",
      "[NN] epoch 10/10 loss=17355.3477\n",
      "[NN] epoch 10/10 loss=17441.5527\n",
      "[NN] epoch 10/10 loss=17691.9082\n",
      "[NN] epoch 10/10 loss=18155.4043\n",
      "[NN] epoch 10/10 loss=18360.7676\n",
      "[NN] epoch 10/10 loss=17847.6973\n",
      "[NN] epoch 10/10 loss=17833.9941\n",
      "[NN] epoch 10/10 loss=18556.9570\n",
      "[NN] epoch 10/10 loss=17125.2129\n",
      "[NN] epoch 10/10 loss=17253.6230\n",
      "[NN] epoch 10/10 loss=18253.8066\n",
      "[NN] epoch 10/10 loss=18690.5254\n",
      "[NN] epoch 10/10 loss=18154.0137\n",
      "[NN] epoch 10/10 loss=18576.0156\n",
      "[NN] epoch 10/10 loss=17972.3320\n",
      "[NN] epoch 10/10 loss=18755.9844\n",
      "[NN] epoch 10/10 loss=16572.0996\n",
      "[NN] epoch 10/10 loss=18132.7676\n",
      "[NN] epoch 10/10 loss=18598.5020\n",
      "[NN] epoch 10/10 loss=16903.8340\n",
      "[NN] epoch 10/10 loss=18047.2715\n",
      "[NN] epoch 10/10 loss=18144.9121\n",
      "[NN] epoch 10/10 loss=18236.8340\n",
      "[NN] epoch 10/10 loss=17023.3555\n",
      "[NN] epoch 10/10 loss=18177.0371\n",
      "[NN] epoch 10/10 loss=19007.5801\n",
      "[NN] epoch 10/10 loss=17482.3105\n",
      "[NN] epoch 10/10 loss=18125.8770\n",
      "[NN] epoch 10/10 loss=17226.5449\n",
      "[NN] epoch 10/10 loss=17322.5957\n",
      "[NN] epoch 10/10 loss=18551.4180\n",
      "[NN] epoch 10/10 loss=18591.8145\n",
      "[NN] epoch 10/10 loss=18274.0996\n",
      "[NN] epoch 10/10 loss=18183.8340\n",
      "[NN] epoch 10/10 loss=18288.3594\n",
      "[NN] epoch 10/10 loss=18013.9434\n",
      "[NN] epoch 10/10 loss=17116.7891\n",
      "[NN] epoch 10/10 loss=17205.0957\n",
      "[NN] epoch 10/10 loss=18555.6836\n",
      "[NN] epoch 10/10 loss=18389.1133\n",
      "[NN] epoch 10/10 loss=17912.3066\n",
      "[NN] epoch 10/10 loss=17846.2188\n",
      "[NN] epoch 10/10 loss=17616.3613\n",
      "[NN] epoch 10/10 loss=17328.2715\n",
      "[NN] epoch 10/10 loss=18618.7734\n",
      "[NN] epoch 10/10 loss=17494.7871\n",
      "[NN] epoch 10/10 loss=18809.1895\n",
      "[NN] epoch 10/10 loss=18245.7305\n",
      "[NN] epoch 10/10 loss=18429.1523\n",
      "[NN] epoch 10/10 loss=18375.4883\n",
      "[NN] epoch 10/10 loss=16563.6934\n",
      "[NN] epoch 10/10 loss=18126.1914\n",
      "[NN] epoch 10/10 loss=17203.0156\n",
      "[NN] epoch 10/10 loss=18385.2930\n",
      "[NN] epoch 10/10 loss=18266.2734\n",
      "[NN] epoch 10/10 loss=18381.4297\n",
      "[NN] epoch 10/10 loss=17798.2832\n",
      "[NN] epoch 10/10 loss=19001.0508\n",
      "[NN] epoch 10/10 loss=18130.5781\n",
      "[NN] epoch 10/10 loss=17500.3457\n",
      "[NN] epoch 10/10 loss=17531.5039\n",
      "[NN] epoch 10/10 loss=17812.2617\n",
      "[NN] epoch 10/10 loss=18539.6582\n",
      "[NN] epoch 10/10 loss=18625.6641\n",
      "[NN] epoch 10/10 loss=18160.9316\n",
      "[NN] epoch 10/10 loss=17262.6133\n",
      "[NN] epoch 10/10 loss=17209.2266\n",
      "[NN] epoch 10/10 loss=18067.7676\n",
      "[NN] epoch 10/10 loss=18393.2969\n",
      "[NN] epoch 10/10 loss=18287.5723\n",
      "[NN] epoch 10/10 loss=17364.4043\n",
      "[NN] epoch 10/10 loss=17774.9688\n",
      "[NN] epoch 10/10 loss=17839.1191\n",
      "[NN] epoch 10/10 loss=18087.9258\n",
      "[NN] epoch 10/10 loss=18120.3672\n",
      "[NN] epoch 10/10 loss=16992.7520\n",
      "[NN] epoch 10/10 loss=18780.3398\n",
      "[NN] epoch 10/10 loss=17607.2402\n",
      "[NN] epoch 10/10 loss=18388.4531\n",
      "[NN] epoch 10/10 loss=18713.2676\n",
      "[NN] epoch 10/10 loss=17718.2422\n",
      "[NN] epoch 10/10 loss=17495.1660\n",
      "[NN] epoch 10/10 loss=18350.9238\n",
      "[NN] epoch 10/10 loss=18238.8203\n",
      "[NN] epoch 10/10 loss=18053.5820\n",
      "[NN] epoch 10/10 loss=17785.2598\n",
      "[NN] epoch 10/10 loss=17327.2695\n",
      "[NN] epoch 10/10 loss=19155.6133\n",
      "[NN] epoch 10/10 loss=18522.0820\n",
      "[NN] epoch 10/10 loss=17789.9805\n",
      "[NN] epoch 10/10 loss=16965.2793\n",
      "[NN] epoch 10/10 loss=17449.5820\n",
      "[NN] epoch 10/10 loss=17874.8574\n",
      "[NN] epoch 10/10 loss=18092.0039\n",
      "[NN] epoch 10/10 loss=18255.1250\n",
      "[NN] epoch 10/10 loss=18236.3691\n",
      "[NN] epoch 10/10 loss=17271.8340\n",
      "[NN] epoch 10/10 loss=17581.0586\n",
      "[NN] epoch 10/10 loss=18629.8828\n",
      "[NN] epoch 10/10 loss=17979.2207\n",
      "[NN] epoch 10/10 loss=16940.3828\n",
      "[NN] epoch 10/10 loss=18683.9473\n",
      "[NN] epoch 10/10 loss=17879.3418\n",
      "[NN] epoch 10/10 loss=18573.1895\n",
      "[NN] epoch 10/10 loss=17694.8887\n",
      "[NN] epoch 10/10 loss=17801.5410\n",
      "[NN] epoch 10/10 loss=18029.3418\n",
      "[NN] epoch 10/10 loss=18511.9863\n",
      "[NN] epoch 10/10 loss=17106.1523\n",
      "[NN] epoch 10/10 loss=19119.5996\n",
      "[NN] epoch 10/10 loss=17876.5859\n",
      "[NN] epoch 10/10 loss=17345.1660\n",
      "[NN] epoch 10/10 loss=18457.5312\n",
      "[NN] epoch 10/10 loss=17198.7695\n",
      "[NN] epoch 10/10 loss=18141.7109\n",
      "[NN] epoch 10/10 loss=18108.9375\n",
      "[NN] epoch 10/10 loss=17785.8984\n",
      "[NN] epoch 10/10 loss=18726.2383\n",
      "[NN] epoch 10/10 loss=18042.2793\n",
      "[NN] epoch 10/10 loss=18486.2617\n",
      "[NN] epoch 10/10 loss=16695.3965\n",
      "[NN] epoch 10/10 loss=17828.4004\n",
      "[NN] epoch 10/10 loss=18941.2891\n",
      "[NN] epoch 10/10 loss=16699.4746\n",
      "[NN] epoch 10/10 loss=17194.8496\n",
      "[NN] epoch 10/10 loss=19061.2988\n",
      "[NN] epoch 10/10 loss=17999.4316\n",
      "[NN] epoch 10/10 loss=17378.6738\n",
      "[NN] epoch 10/10 loss=18258.8398\n",
      "[NN] epoch 10/10 loss=18666.8613\n",
      "[NN] epoch 10/10 loss=17750.2324\n",
      "[NN] epoch 10/10 loss=17827.7129\n",
      "[NN] epoch 10/10 loss=17919.4355\n",
      "[NN] epoch 10/10 loss=18856.0703\n",
      "[NN] epoch 10/10 loss=16894.4648\n",
      "[NN] epoch 10/10 loss=17676.3477\n",
      "[NN] epoch 10/10 loss=18451.5215\n",
      "[NN] epoch 10/10 loss=17941.1055\n",
      "[NN] epoch 10/10 loss=18761.2461\n",
      "[NN] epoch 10/10 loss=17608.0938\n",
      "[NN] epoch 10/10 loss=18646.9453\n",
      "[NN] epoch 10/10 loss=16891.8398\n",
      "[NN] epoch 10/10 loss=18086.1113\n",
      "[NN] epoch 10/10 loss=17235.9160\n",
      "[NN] epoch 10/10 loss=18223.8770\n",
      "[NN] epoch 10/10 loss=17879.7969\n",
      "[NN] epoch 10/10 loss=18580.8730\n",
      "[NN] epoch 10/10 loss=17058.1816\n",
      "[NN] epoch 10/10 loss=17030.1426\n",
      "[NN] epoch 10/10 loss=18889.0996\n",
      "[NN] epoch 10/10 loss=18323.7188\n",
      "[NN] epoch 10/10 loss=18440.9531\n",
      "[NN] epoch 10/10 loss=17788.6973\n",
      "[NN] epoch 10/10 loss=17905.8359\n",
      "[NN] epoch 10/10 loss=17872.5684\n",
      "[NN] epoch 10/10 loss=18076.1426\n",
      "[NN] epoch 10/10 loss=18196.2969\n",
      "[NN] epoch 10/10 loss=18497.2773\n",
      "[NN] epoch 10/10 loss=18500.5059\n",
      "[NN] epoch 10/10 loss=18631.1836\n",
      "[NN] epoch 10/10 loss=16966.7070\n",
      "[NN] epoch 10/10 loss=17245.6777\n",
      "[NN] epoch 10/10 loss=18216.5820\n",
      "[NN] epoch 10/10 loss=18658.1016\n",
      "[NN] epoch 10/10 loss=17665.1719\n",
      "[NN] epoch 10/10 loss=17147.6836\n",
      "[NN] epoch 10/10 loss=18294.5703\n",
      "[NN] epoch 10/10 loss=16795.2070\n",
      "[NN] epoch 10/10 loss=18686.8828\n",
      "[NN] epoch 10/10 loss=18358.2988\n",
      "[NN] epoch 10/10 loss=18773.7422\n",
      "[NN] epoch 10/10 loss=17272.5781\n",
      "[NN] epoch 10/10 loss=18564.3379\n",
      "[NN] epoch 10/10 loss=18754.4922\n",
      "[NN] epoch 10/10 loss=18064.4102\n",
      "[NN] epoch 10/10 loss=17674.7402\n",
      "[NN] epoch 10/10 loss=16719.5703\n",
      "[NN] epoch 10/10 loss=17905.0801\n",
      "[NN] epoch 10/10 loss=17861.0645\n",
      "[NN] epoch 10/10 loss=19241.4082\n",
      "[NN] epoch 10/10 loss=16794.1523\n",
      "[NN] epoch 10/10 loss=18201.8867\n",
      "[NN] epoch 10/10 loss=17155.1113\n",
      "[NN] epoch 10/10 loss=18346.9551\n",
      "[NN] epoch 10/10 loss=18684.8203\n",
      "[NN] epoch 10/10 loss=17768.8750\n",
      "[NN] epoch 10/10 loss=17851.9043\n",
      "[NN] epoch 10/10 loss=18610.4824\n",
      "[NN] epoch 10/10 loss=17865.2988\n",
      "[NN] epoch 10/10 loss=18139.9570\n",
      "[NN] epoch 10/10 loss=16943.8301\n",
      "[NN] epoch 10/10 loss=18267.6211\n",
      "[NN] epoch 10/10 loss=17006.0566\n",
      "[NN] epoch 10/10 loss=18264.7207\n",
      "[NN] epoch 10/10 loss=18445.0781\n",
      "[NN] epoch 10/10 loss=18083.4160\n",
      "[NN] epoch 10/10 loss=17954.5020\n",
      "[NN] epoch 10/10 loss=17352.1426\n",
      "[NN] epoch 10/10 loss=17947.1250\n",
      "[NN] epoch 10/10 loss=18984.3340\n",
      "[NN] epoch 10/10 loss=16897.4023\n",
      "[NN] epoch 10/10 loss=18675.3828\n",
      "[NN] epoch 10/10 loss=18211.6602\n",
      "[NN] epoch 10/10 loss=18610.5801\n",
      "[NN] epoch 10/10 loss=17752.3457\n",
      "[NN] epoch 10/10 loss=18007.7012\n",
      "[NN] epoch 10/10 loss=17437.0293\n",
      "[NN] epoch 10/10 loss=18751.0918\n",
      "[NN] epoch 10/10 loss=17922.7969\n",
      "[NN] epoch 10/10 loss=17073.3887\n",
      "[NN] epoch 10/10 loss=18148.2637\n",
      "[NN] epoch 10/10 loss=18101.5684\n",
      "[NN] epoch 10/10 loss=18919.2930\n",
      "[NN] epoch 10/10 loss=17904.6328\n",
      "[NN] epoch 10/10 loss=16420.2031\n",
      "[NN] epoch 10/10 loss=17784.9922\n",
      "[NN] epoch 10/10 loss=18911.4902\n",
      "[NN] epoch 10/10 loss=18688.0391\n",
      "[NN] epoch 10/10 loss=16917.4355\n",
      "[NN] epoch 10/10 loss=18413.4766\n",
      "[NN] epoch 10/10 loss=17394.0801\n",
      "[NN] epoch 10/10 loss=18310.4141\n",
      "[NN] epoch 10/10 loss=17848.5664\n",
      "[NN] epoch 10/10 loss=18251.7598\n",
      "[NN] epoch 10/10 loss=18076.7500\n",
      "[NN] epoch 10/10 loss=18207.8477\n",
      "[NN] epoch 10/10 loss=17404.7656\n",
      "[NN] epoch 10/10 loss=17393.7402\n",
      "[NN] epoch 10/10 loss=17463.7969\n",
      "[NN] epoch 10/10 loss=18160.7402\n",
      "[NN] epoch 10/10 loss=17893.9102\n",
      "[NN] epoch 10/10 loss=18965.5957\n",
      "[NN] epoch 10/10 loss=18333.6465\n",
      "[NN] epoch 10/10 loss=17009.9453\n",
      "[NN] epoch 10/10 loss=18206.4961\n",
      "[NN] epoch 10/10 loss=17771.9043\n",
      "[NN] epoch 10/10 loss=18448.6660\n",
      "[NN] epoch 10/10 loss=17166.3203\n",
      "[NN] epoch 10/10 loss=18021.6309\n",
      "[NN] epoch 10/10 loss=18325.2930\n",
      "[NN] epoch 10/10 loss=18665.3496\n",
      "[NN] epoch 10/10 loss=17687.0762\n",
      "[NN] epoch 10/10 loss=17039.2812\n",
      "[NN] epoch 10/10 loss=17356.8555\n",
      "[NN] epoch 10/10 loss=18515.0254\n",
      "[NN] epoch 10/10 loss=18914.3125\n",
      "[NN] epoch 10/10 loss=18052.2871\n",
      "[NN] epoch 10/10 loss=19028.0137\n",
      "[NN] epoch 10/10 loss=17410.0898\n",
      "[NN] epoch 10/10 loss=17941.6680\n",
      "[NN] epoch 10/10 loss=18157.1992\n",
      "[NN] epoch 10/10 loss=17397.7480\n",
      "[NN] epoch 10/10 loss=18943.4023\n",
      "[NN] epoch 10/10 loss=18585.7578\n",
      "[NN] epoch 10/10 loss=17623.8906\n",
      "[NN] epoch 10/10 loss=16945.2578\n",
      "[NN] epoch 10/10 loss=17738.8594\n",
      "[NN] epoch 10/10 loss=17603.5527\n",
      "[NN] epoch 10/10 loss=17748.9004\n",
      "[NN] epoch 10/10 loss=18787.8262\n",
      "[NN] epoch 10/10 loss=17896.5508\n",
      "[NN] epoch 10/10 loss=17811.3633\n",
      "[NN] epoch 10/10 loss=18288.1816\n",
      "[NN] epoch 10/10 loss=17934.7285\n",
      "[NN] epoch 10/10 loss=17473.9375\n",
      "[NN] epoch 10/10 loss=18321.6191\n",
      "[NN] epoch 10/10 loss=17787.6602\n",
      "[NN] epoch 10/10 loss=16820.3691\n",
      "[NN] epoch 10/10 loss=18197.5332\n",
      "[NN] epoch 10/10 loss=18658.5352\n",
      "[NN] epoch 10/10 loss=18433.2207\n",
      "[NN] epoch 10/10 loss=17642.3906\n",
      "[NN] epoch 10/10 loss=17829.1621\n",
      "[NN] epoch 10/10 loss=17066.4102\n",
      "[NN] epoch 10/10 loss=18316.2637\n",
      "[NN] epoch 10/10 loss=18059.0332\n",
      "[NN] epoch 10/10 loss=18503.9219\n",
      "[NN] epoch 10/10 loss=18241.8301\n",
      "[NN] epoch 10/10 loss=18521.0820\n",
      "[NN] epoch 10/10 loss=17647.0723\n",
      "[NN] epoch 10/10 loss=18127.3105\n",
      "[NN] epoch 10/10 loss=17423.2461\n",
      "[NN] epoch 10/10 loss=18597.2246\n",
      "[NN] epoch 10/10 loss=18387.7344\n",
      "[NN] epoch 10/10 loss=17618.4863\n",
      "[NN] epoch 10/10 loss=18017.6230\n",
      "[NN] epoch 10/10 loss=17302.3340\n",
      "[NN] epoch 10/10 loss=18649.2539\n",
      "[NN] epoch 10/10 loss=18709.9766\n",
      "[NN] epoch 10/10 loss=17197.4141\n",
      "[NN] epoch 10/10 loss=17384.4238\n",
      "[NN] epoch 10/10 loss=17918.6562\n",
      "[NN] epoch 10/10 loss=16701.0176\n",
      "[NN] epoch 10/10 loss=18100.0898\n",
      "[NN] epoch 10/10 loss=17450.6328\n",
      "[NN] epoch 10/10 loss=18977.9141\n",
      "[NN] epoch 10/10 loss=18579.6426\n",
      "[NN] epoch 10/10 loss=18272.4395\n",
      "[NN] epoch 10/10 loss=18846.9180\n",
      "[NN] epoch 10/10 loss=17223.3789\n",
      "[NN] epoch 10/10 loss=17703.7812\n",
      "[NN] epoch 10/10 loss=17752.8379\n",
      "[NN] epoch 10/10 loss=17714.1152\n",
      "[NN] epoch 10/10 loss=17955.8203\n",
      "[NN] epoch 10/10 loss=17594.3301\n",
      "[NN] epoch 10/10 loss=18373.2695\n",
      "[NN] epoch 10/10 loss=18307.5137\n",
      "[NN] epoch 10/10 loss=17719.0078\n",
      "[NN] epoch 10/10 loss=18410.0039\n",
      "[NN] epoch 10/10 loss=18392.9316\n",
      "[NN] epoch 10/10 loss=17466.6035\n",
      "[NN] epoch 10/10 loss=17834.1309\n",
      "[NN] epoch 10/10 loss=18316.2051\n",
      "[NN] epoch 10/10 loss=18101.4492\n",
      "[NN] epoch 10/10 loss=18569.1133\n",
      "[NN] epoch 10/10 loss=17133.9668\n",
      "[NN] epoch 10/10 loss=17716.3867\n",
      "[Linear] mean value = 138.12921768707483 std = 10.469106726754566\n",
      "[NN]     mean value = 146.55095238095237 std = 0.19431434748752777\n"
     ]
    }
   ],
   "source": [
    "lin_res = repeated_cv_value(\n",
    "    lambda Xtr,Atr,Rtr,K: build_predict_ad_linear(Xtr,Atr,Rtr,K,alpha=1.0),\n",
    "    X, A, R, K=meta[\"K\"], n_repeats=100, base_seed=2025, verbose=True\n",
    ")\n",
    "nn_res  = repeated_cv_value(\n",
    "    lambda Xtr,Atr,Rtr,K: build_predict_ad_nn(Xtr,Atr,Rtr,K,epochs=10,lr=1e-3),\n",
    "    X, A, R, K=meta[\"K\"], n_repeats=100, base_seed=2025, verbose=True\n",
    ")\n",
    "\n",
    "print(\"[Linear] mean value =\", lin_res[\"mean\"], \"std =\", lin_res[\"std\"])\n",
    "print(\"[NN]     mean value =\", nn_res[\"mean\"],  \"std =\", nn_res[\"std\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연속형 표준화 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "COVS = [\"age\",\"wtkg\",\"cd80\",\"karnof\", \"cd40\",\n",
    "        \"gender\",\"race\",\"homo\",\"drugs\",\"symptom\",\"str2\",\"hemo\"]\n",
    "\n",
    "def _infer_continuous_idx(df: pd.DataFrame, covs):\n",
    "    \"\"\"\n",
    "    연속형 열 추정 규칙:\n",
    "      - dtype이 수치형이고\n",
    "      - 고유값 개수 > 6 이고\n",
    "      - {0,1} 이진집합이 아닌 경우\n",
    "    \"\"\"\n",
    "    cont_idx = []\n",
    "    for j, c in enumerate(covs):\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            uniq = pd.unique(s.dropna())\n",
    "            # 이진 {0,1}이면 제외\n",
    "            if set(np.unique(uniq)).issubset({0, 1}):\n",
    "                continue\n",
    "            if len(uniq) > 6:\n",
    "                cont_idx.append(j)\n",
    "    return cont_idx\n",
    "\n",
    "def transform_reward_softplus(R, tau=10.0):\n",
    "    \"\"\"\n",
    "    R>=0는 거의 보존, R<0는 0+로 스무스하게 압축.\n",
    "    tau 작을수록 ReLU에 근접(음수 더 얇게), 클수록 완만.\n",
    "    \"\"\"\n",
    "    R = np.asarray(R, dtype=float)\n",
    "    return tau * np.log1p(np.exp(R / tau))\n",
    "\n",
    "def actg_to_arrays(df: pd.DataFrame, covs=COVS, keep_R_pos=False, R_log=False, neg_scal=False):\n",
    "    \"\"\"\n",
    "    ACTG175 데이터프레임 -> (X, A, R, meta)\n",
    "    trt 열이 이미 {0,1,2,3}로 코딩되어 있다고 가정\n",
    "    \"\"\"\n",
    "    # 치료: 이미 0,1,2,3\n",
    "    A = df[\"trt\"].to_numpy().astype(int)\n",
    "\n",
    "    # Reward: cd420 - cd40\n",
    "    R = df[\"cd420\"].astype(float) - df[\"cd40\"].astype(float)\n",
    "\n",
    "    # X: 지정된 공변량\n",
    "    X = df[covs].to_numpy(dtype=float)\n",
    "\n",
    "    # 옵션: R > 0만 유지\n",
    "    if keep_R_pos:\n",
    "        mask = (R > 0)\n",
    "        X, A, R = X[mask], A[mask], R[mask]\n",
    "        if R_log:\n",
    "            R = np.log1p(R)\n",
    "    else:\n",
    "        if neg_scal:\n",
    "            R = transform_reward_softplus(R)\n",
    "\n",
    "    # 연속형 열 인덱스 추정 (스케일링은 CV 내부에서 수행)\n",
    "    cont_idx = _infer_continuous_idx(df, covs)\n",
    "    meta = {\"covs\": covs, \"K\": len(np.unique(A)), \"n\": len(A), \"cont_idx\": cont_idx}\n",
    "    return X, A, R, meta\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 평가 함수 (균등 무작위 배정 가정)\n",
    "# -----------------------\n",
    "def value_uniform(pred, A, R, K):\n",
    "    pred = np.asarray(pred).reshape(-1)\n",
    "    A    = np.asarray(A, dtype=int).reshape(-1)\n",
    "    R    = np.asarray(R, dtype=float).reshape(-1)\n",
    "    assert pred.shape == A.shape == R.shape, \"pred/A/R length mismatch\"\n",
    "    return np.mean((pred == A).astype(float) * R * K)\n",
    "\n",
    "# -----------------------\n",
    "# 유틸: 예측함수 안전화\n",
    "# -----------------------\n",
    "def _safe_predict(predict_fn, X_te):\n",
    "    pred = predict_fn(X_te)\n",
    "    pred = np.asarray(pred)\n",
    "    if pred.ndim != 1:\n",
    "        pred = pred.reshape(-1)\n",
    "    return pred.astype(int)\n",
    "\n",
    "class ADNet(nn.Module):\n",
    "    def __init__(self, p, out_dim, hidden=[256,256]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = p\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# -----------------------\n",
    "# 한 번의 5-fold CV에서 value 계산 (층화 + 견고)\n",
    "# -----------------------\n",
    "def cv5_value_once(train_and_build_predict, X, A, R, K, seed,\n",
    "                   verbose=False, standardize=False, cont_idx=None):\n",
    "    \"\"\"\n",
    "    train_and_build_predict: (X_tr, A_tr, R_tr, K) -> predict_fn\n",
    "    standardize=True 이고 cont_idx가 주어지면,\n",
    "    각 fold의 train으로 StandardScaler를 fit 후 train/test에 동일 변환을 적용(연속형 열만).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    A = np.asarray(A, dtype=int)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    vals, sizes = [], []\n",
    "\n",
    "    for fold_idx, (tr_idx, te_idx) in enumerate(skf.split(X, A), start=1):\n",
    "        X_tr, A_tr, R_tr = X[tr_idx].copy(), A[tr_idx], R[tr_idx]\n",
    "        X_te, A_te, R_te = X[te_idx].copy(), A[te_idx], R[te_idx]\n",
    "\n",
    "        # --- 표준화(연속형 열만): train fit → train/test transform ---\n",
    "        if standardize and cont_idx:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_tr[:, cont_idx])\n",
    "            X_tr[:, cont_idx] = scaler.transform(X_tr[:, cont_idx])\n",
    "            X_te[:, cont_idx] = scaler.transform(X_te[:, cont_idx])\n",
    "\n",
    "        try:\n",
    "            predict_fn = train_and_build_predict(X_tr, A_tr, R_tr, K)\n",
    "            pred_te = _safe_predict(predict_fn, X_te)\n",
    "            v = value_uniform(pred_te, A_te, R_te, K)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                uniq, cnts = np.unique(A_tr, return_counts=True)\n",
    "                msg = (f\"[cv5] fold={fold_idx} failed | \"\n",
    "                       f\"train sizes per arm={dict(zip(uniq.tolist(), cnts.tolist()))} | \"\n",
    "                       f\"err: {repr(e)}\")\n",
    "                print(msg)\n",
    "            raise\n",
    "        vals.append(v)\n",
    "        sizes.append(len(te_idx))\n",
    "\n",
    "    vals, sizes = np.asarray(vals), np.asarray(sizes)\n",
    "    return float(np.sum(vals * sizes) / np.sum(sizes))\n",
    "\n",
    "# -----------------------\n",
    "# 5-fold CV 반복\n",
    "# -----------------------\n",
    "def repeated_cv_value(train_and_build_predict, X, A, R, K,\n",
    "                      n_repeats=1000, base_seed=2025, verbose=False,\n",
    "                      standardize=False, cont_idx=None):\n",
    "    values = np.zeros(n_repeats, dtype=float)\n",
    "    for r in range(n_repeats):\n",
    "        seed = base_seed + r\n",
    "        values[r] = cv5_value_once(\n",
    "            train_and_build_predict, X, A, R, K, seed, verbose=verbose,\n",
    "            standardize=standardize, cont_idx=cont_idx\n",
    "        )\n",
    "    return {\n",
    "        \"values\": values,\n",
    "        \"mean\": float(values.mean()),\n",
    "        \"std\": float(values.std(ddof=1)),\n",
    "        \"n_repeats\": int(n_repeats)\n",
    "    }\n",
    "\n",
    "def build_predict_ad_linear(Xtr, Atr, Rtr, K, alpha=1.0):\n",
    "    Xtr = np.asarray(Xtr, dtype=float)\n",
    "    Atr = np.asarray(Atr, dtype=int)\n",
    "    Rtr = np.asarray(Rtr, dtype=float)\n",
    "    model, V, predict = ad_linear(Xtr, Atr, Rtr, K=K, alpha=alpha)\n",
    "    return predict  # predict(X_new) -> np.int64 1D 기대\n",
    "\n",
    "def build_predict_ad_nn(Xtr, Atr, Rtr, K, epochs=60, lr=1e-3):\n",
    "    Xtr = np.asarray(Xtr, dtype=float)\n",
    "    Atr = np.asarray(Atr, dtype=int)\n",
    "    Rtr = np.asarray(Rtr, dtype=float)\n",
    "    model, V, predict = ad_nn(Xtr, Atr, Rtr, K=K, epochs=epochs, lr=lr)\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] epoch 10/10 loss=10677.3604\n",
      "[NN] epoch 10/10 loss=9849.8711\n",
      "[NN] epoch 10/10 loss=9703.1777\n",
      "[NN] epoch 10/10 loss=9881.1904\n",
      "[NN] epoch 10/10 loss=10262.9131\n",
      "[NN] epoch 10/10 loss=9518.0430\n",
      "[NN] epoch 10/10 loss=10556.7725\n",
      "[NN] epoch 10/10 loss=10053.8975\n",
      "[NN] epoch 10/10 loss=10354.4355\n",
      "[NN] epoch 10/10 loss=9883.3379\n",
      "[NN] epoch 10/10 loss=9761.1182\n",
      "[NN] epoch 10/10 loss=9488.1016\n",
      "[NN] epoch 10/10 loss=10296.9326\n",
      "[NN] epoch 10/10 loss=10220.3965\n",
      "[NN] epoch 10/10 loss=10595.5605\n",
      "[NN] epoch 10/10 loss=9612.6104\n",
      "[NN] epoch 10/10 loss=9933.5830\n",
      "[NN] epoch 10/10 loss=10306.8066\n",
      "[NN] epoch 10/10 loss=10569.2803\n",
      "[NN] epoch 10/10 loss=9894.6211\n",
      "[NN] epoch 10/10 loss=10155.5713\n",
      "[NN] epoch 10/10 loss=10505.4102\n",
      "[NN] epoch 10/10 loss=9651.4551\n",
      "[NN] epoch 10/10 loss=10042.7080\n",
      "[NN] epoch 10/10 loss=10032.8369\n",
      "[NN] epoch 10/10 loss=9768.3633\n",
      "[NN] epoch 10/10 loss=10307.8809\n",
      "[NN] epoch 10/10 loss=10048.7969\n",
      "[NN] epoch 10/10 loss=10446.2188\n",
      "[NN] epoch 10/10 loss=9805.1670\n",
      "[NN] epoch 10/10 loss=10134.6992\n",
      "[NN] epoch 10/10 loss=9974.2363\n",
      "[NN] epoch 10/10 loss=10065.6221\n",
      "[NN] epoch 10/10 loss=9914.0889\n",
      "[NN] epoch 10/10 loss=10281.2568\n",
      "[NN] epoch 10/10 loss=10589.0625\n",
      "[NN] epoch 10/10 loss=9946.3447\n",
      "[NN] epoch 10/10 loss=10262.4766\n",
      "[NN] epoch 10/10 loss=9681.2451\n",
      "[NN] epoch 10/10 loss=9876.1865\n",
      "[NN] epoch 10/10 loss=9986.0596\n",
      "[NN] epoch 10/10 loss=10459.9629\n",
      "[NN] epoch 10/10 loss=9600.6074\n",
      "[NN] epoch 10/10 loss=10240.3018\n",
      "[NN] epoch 10/10 loss=10050.5635\n",
      "[NN] epoch 10/10 loss=10065.9717\n",
      "[NN] epoch 10/10 loss=10127.8955\n",
      "[NN] epoch 10/10 loss=10237.7041\n",
      "[NN] epoch 10/10 loss=10498.0312\n",
      "[NN] epoch 10/10 loss=9414.8203\n",
      "[NN] epoch 10/10 loss=10177.6816\n",
      "[NN] epoch 10/10 loss=10412.4102\n",
      "[NN] epoch 10/10 loss=9743.9170\n",
      "[NN] epoch 10/10 loss=9936.2568\n",
      "[NN] epoch 10/10 loss=10108.6621\n",
      "[NN] epoch 10/10 loss=10096.6748\n",
      "[NN] epoch 10/10 loss=10531.8750\n",
      "[NN] epoch 10/10 loss=9452.7471\n",
      "[NN] epoch 10/10 loss=10130.1230\n",
      "[NN] epoch 10/10 loss=10185.3193\n",
      "[NN] epoch 10/10 loss=10394.8906\n",
      "[NN] epoch 10/10 loss=10218.5889\n",
      "[NN] epoch 10/10 loss=9438.6562\n",
      "[NN] epoch 10/10 loss=10036.2734\n",
      "[NN] epoch 10/10 loss=10286.4541\n",
      "[NN] epoch 10/10 loss=10769.5869\n",
      "[NN] epoch 10/10 loss=9907.2920\n",
      "[NN] epoch 10/10 loss=10536.5771\n",
      "[NN] epoch 10/10 loss=9354.2480\n",
      "[NN] epoch 10/10 loss=9812.2422\n",
      "[NN] epoch 10/10 loss=10500.7373\n",
      "[NN] epoch 10/10 loss=9676.5859\n",
      "[NN] epoch 10/10 loss=10335.2705\n",
      "[NN] epoch 10/10 loss=10226.8438\n",
      "[NN] epoch 10/10 loss=9629.7959\n",
      "[NN] epoch 10/10 loss=10384.5596\n",
      "[NN] epoch 10/10 loss=10237.3789\n",
      "[NN] epoch 10/10 loss=9952.0713\n",
      "[NN] epoch 10/10 loss=9885.7637\n",
      "[NN] epoch 10/10 loss=9917.8271\n",
      "[NN] epoch 10/10 loss=9983.7725\n",
      "[NN] epoch 10/10 loss=10032.7500\n",
      "[NN] epoch 10/10 loss=9991.8887\n",
      "[NN] epoch 10/10 loss=9912.8252\n",
      "[NN] epoch 10/10 loss=10454.9951\n",
      "[NN] epoch 10/10 loss=10440.9668\n",
      "[NN] epoch 10/10 loss=10171.8506\n",
      "[NN] epoch 10/10 loss=9679.1299\n",
      "[NN] epoch 10/10 loss=9957.0967\n",
      "[NN] epoch 10/10 loss=10129.5166\n",
      "[NN] epoch 10/10 loss=9570.6514\n",
      "[NN] epoch 10/10 loss=10468.2920\n",
      "[NN] epoch 10/10 loss=10373.3945\n",
      "[NN] epoch 10/10 loss=10006.9248\n",
      "[NN] epoch 10/10 loss=9963.1299\n",
      "[NN] epoch 10/10 loss=9966.9717\n",
      "[NN] epoch 10/10 loss=10108.8555\n",
      "[NN] epoch 10/10 loss=10303.3154\n",
      "[NN] epoch 10/10 loss=10286.3574\n",
      "[NN] epoch 10/10 loss=9688.4219\n",
      "[NN] epoch 10/10 loss=10147.6250\n",
      "[NN] epoch 10/10 loss=10280.2090\n",
      "[NN] epoch 10/10 loss=9900.9092\n",
      "[NN] epoch 10/10 loss=9617.8193\n",
      "[NN] epoch 10/10 loss=10437.0752\n",
      "[NN] epoch 10/10 loss=9795.5371\n",
      "[NN] epoch 10/10 loss=10393.9648\n",
      "[NN] epoch 10/10 loss=9561.4062\n",
      "[NN] epoch 10/10 loss=10130.6670\n",
      "[NN] epoch 10/10 loss=10485.6436\n",
      "[NN] epoch 10/10 loss=10147.1201\n",
      "[NN] epoch 10/10 loss=9698.8203\n",
      "[NN] epoch 10/10 loss=9990.4014\n",
      "[NN] epoch 10/10 loss=10276.0527\n",
      "[NN] epoch 10/10 loss=10223.2363\n",
      "[NN] epoch 10/10 loss=10165.1016\n",
      "[NN] epoch 10/10 loss=10070.5322\n",
      "[NN] epoch 10/10 loss=9860.4922\n",
      "[NN] epoch 10/10 loss=10589.8770\n",
      "[NN] epoch 10/10 loss=9657.3457\n",
      "[NN] epoch 10/10 loss=10424.1416\n",
      "[NN] epoch 10/10 loss=10025.1172\n",
      "[NN] epoch 10/10 loss=10143.3369\n",
      "[NN] epoch 10/10 loss=9931.7373\n",
      "[NN] epoch 10/10 loss=9826.7881\n",
      "[NN] epoch 10/10 loss=10333.9072\n",
      "[NN] epoch 10/10 loss=9882.8672\n",
      "[NN] epoch 10/10 loss=10188.8701\n",
      "[NN] epoch 10/10 loss=10077.5234\n",
      "[NN] epoch 10/10 loss=9873.0596\n",
      "[NN] epoch 10/10 loss=9837.7275\n",
      "[NN] epoch 10/10 loss=10038.6191\n",
      "[NN] epoch 10/10 loss=9664.3633\n",
      "[NN] epoch 10/10 loss=10596.8838\n",
      "[NN] epoch 10/10 loss=10211.5039\n",
      "[NN] epoch 10/10 loss=10560.4609\n",
      "[NN] epoch 10/10 loss=9426.5127\n",
      "[NN] epoch 10/10 loss=10350.6006\n",
      "[NN] epoch 10/10 loss=9665.7500\n",
      "[NN] epoch 10/10 loss=10350.5879\n",
      "[NN] epoch 10/10 loss=9984.3926\n",
      "[NN] epoch 10/10 loss=10383.2627\n",
      "[NN] epoch 10/10 loss=10409.6367\n",
      "[NN] epoch 10/10 loss=9656.6641\n",
      "[NN] epoch 10/10 loss=9932.1973\n",
      "[NN] epoch 10/10 loss=9905.7207\n",
      "[NN] epoch 10/10 loss=10353.0625\n",
      "[NN] epoch 10/10 loss=9753.3135\n",
      "[NN] epoch 10/10 loss=10496.6543\n",
      "[NN] epoch 10/10 loss=9862.4111\n",
      "[NN] epoch 10/10 loss=10201.5391\n",
      "[NN] epoch 10/10 loss=9739.9785\n",
      "[NN] epoch 10/10 loss=10019.9326\n",
      "[NN] epoch 10/10 loss=10189.6455\n",
      "[NN] epoch 10/10 loss=10197.7363\n",
      "[NN] epoch 10/10 loss=10110.1348\n",
      "[NN] epoch 10/10 loss=9908.5166\n",
      "[NN] epoch 10/10 loss=10069.3340\n",
      "[NN] epoch 10/10 loss=9822.4463\n",
      "[NN] epoch 10/10 loss=10465.3086\n",
      "[NN] epoch 10/10 loss=9756.5566\n",
      "[NN] epoch 10/10 loss=9552.4365\n",
      "[NN] epoch 10/10 loss=10891.9111\n",
      "[NN] epoch 10/10 loss=10547.1094\n",
      "[NN] epoch 10/10 loss=9606.4326\n",
      "[NN] epoch 10/10 loss=10169.7520\n",
      "[NN] epoch 10/10 loss=10070.6367\n",
      "[NN] epoch 10/10 loss=10017.8867\n",
      "[NN] epoch 10/10 loss=9927.8174\n",
      "[NN] epoch 10/10 loss=10180.0537\n",
      "[NN] epoch 10/10 loss=9611.3242\n",
      "[NN] epoch 10/10 loss=10354.4219\n",
      "[NN] epoch 10/10 loss=10315.6396\n",
      "[NN] epoch 10/10 loss=10082.7637\n",
      "[NN] epoch 10/10 loss=10017.1309\n",
      "[NN] epoch 10/10 loss=10344.6094\n",
      "[NN] epoch 10/10 loss=10534.1953\n",
      "[NN] epoch 10/10 loss=9697.5137\n",
      "[NN] epoch 10/10 loss=9887.8818\n",
      "[NN] epoch 10/10 loss=9912.8174\n",
      "[NN] epoch 10/10 loss=10506.2549\n",
      "[NN] epoch 10/10 loss=9593.7656\n",
      "[NN] epoch 10/10 loss=9907.3662\n",
      "[NN] epoch 10/10 loss=9935.1387\n",
      "[NN] epoch 10/10 loss=10455.7070\n",
      "[NN] epoch 10/10 loss=10615.0879\n",
      "[NN] epoch 10/10 loss=9991.5762\n",
      "[NN] epoch 10/10 loss=9527.1914\n",
      "[NN] epoch 10/10 loss=10158.9570\n",
      "[NN] epoch 10/10 loss=10113.0898\n",
      "[NN] epoch 10/10 loss=9929.3906\n",
      "[NN] epoch 10/10 loss=9497.3643\n",
      "[NN] epoch 10/10 loss=10399.6816\n",
      "[NN] epoch 10/10 loss=10492.3818\n",
      "[NN] epoch 10/10 loss=10073.4785\n",
      "[NN] epoch 10/10 loss=9683.0654\n",
      "[NN] epoch 10/10 loss=10306.3438\n",
      "[NN] epoch 10/10 loss=10790.1416\n",
      "[NN] epoch 10/10 loss=9743.7217\n",
      "[NN] epoch 10/10 loss=9848.5342\n",
      "[NN] epoch 10/10 loss=10446.3994\n",
      "[NN] epoch 10/10 loss=10049.1436\n",
      "[NN] epoch 10/10 loss=10199.0342\n",
      "[NN] epoch 10/10 loss=10182.5977\n",
      "[NN] epoch 10/10 loss=9523.9072\n",
      "[NN] epoch 10/10 loss=10190.9814\n",
      "[NN] epoch 10/10 loss=9587.0234\n",
      "[NN] epoch 10/10 loss=10209.0752\n",
      "[NN] epoch 10/10 loss=9875.8076\n",
      "[NN] epoch 10/10 loss=10527.5244\n",
      "[NN] epoch 10/10 loss=10015.7012\n",
      "[NN] epoch 10/10 loss=10071.7852\n",
      "[NN] epoch 10/10 loss=9860.0303\n",
      "[NN] epoch 10/10 loss=10055.3672\n",
      "[NN] epoch 10/10 loss=10379.2031\n",
      "[NN] epoch 10/10 loss=10082.3359\n",
      "[NN] epoch 10/10 loss=10301.4004\n",
      "[NN] epoch 10/10 loss=10614.1953\n",
      "[NN] epoch 10/10 loss=9479.0078\n",
      "[NN] epoch 10/10 loss=9910.0498\n",
      "[NN] epoch 10/10 loss=10185.8496\n",
      "[NN] epoch 10/10 loss=10691.5205\n",
      "[NN] epoch 10/10 loss=10242.2188\n",
      "[NN] epoch 10/10 loss=9105.6162\n",
      "[NN] epoch 10/10 loss=10160.3584\n",
      "[NN] epoch 10/10 loss=10390.0303\n",
      "[NN] epoch 10/10 loss=9810.1016\n",
      "[NN] epoch 10/10 loss=10095.4951\n",
      "[NN] epoch 10/10 loss=9826.9482\n",
      "[NN] epoch 10/10 loss=10274.3555\n",
      "[NN] epoch 10/10 loss=10492.9414\n",
      "[NN] epoch 10/10 loss=10172.9590\n",
      "[NN] epoch 10/10 loss=9952.5693\n",
      "[NN] epoch 10/10 loss=10563.7666\n",
      "[NN] epoch 10/10 loss=9188.4639\n",
      "[NN] epoch 10/10 loss=9717.6572\n",
      "[NN] epoch 10/10 loss=10483.6641\n",
      "[NN] epoch 10/10 loss=10589.8262\n",
      "[NN] epoch 10/10 loss=10153.7354\n",
      "[NN] epoch 10/10 loss=9417.8066\n",
      "[NN] epoch 10/10 loss=9762.7930\n",
      "[NN] epoch 10/10 loss=10144.1299\n",
      "[NN] epoch 10/10 loss=10271.8789\n",
      "[NN] epoch 10/10 loss=9938.9639\n",
      "[NN] epoch 10/10 loss=10228.5098\n",
      "[NN] epoch 10/10 loss=10250.2412\n",
      "[NN] epoch 10/10 loss=10464.4229\n",
      "[NN] epoch 10/10 loss=9496.0166\n",
      "[NN] epoch 10/10 loss=9799.1807\n",
      "[NN] epoch 10/10 loss=10376.1074\n",
      "[NN] epoch 10/10 loss=9944.8369\n",
      "[NN] epoch 10/10 loss=10050.8818\n",
      "[NN] epoch 10/10 loss=9836.5098\n",
      "[NN] epoch 10/10 loss=10545.3301\n",
      "[NN] epoch 10/10 loss=9966.5674\n",
      "[NN] epoch 10/10 loss=9601.8213\n",
      "[NN] epoch 10/10 loss=10051.0635\n",
      "[NN] epoch 10/10 loss=10221.1221\n",
      "[NN] epoch 10/10 loss=9893.3818\n",
      "[NN] epoch 10/10 loss=10600.2510\n",
      "[NN] epoch 10/10 loss=9603.7920\n",
      "[NN] epoch 10/10 loss=9998.2275\n",
      "[NN] epoch 10/10 loss=10426.2715\n",
      "[NN] epoch 10/10 loss=10283.2529\n",
      "[NN] epoch 10/10 loss=10079.6074\n",
      "[NN] epoch 10/10 loss=10377.7783\n",
      "[NN] epoch 10/10 loss=10315.2227\n",
      "[NN] epoch 10/10 loss=9710.5488\n",
      "[NN] epoch 10/10 loss=9795.3623\n",
      "[NN] epoch 10/10 loss=10166.6836\n",
      "[NN] epoch 10/10 loss=9770.6436\n",
      "[NN] epoch 10/10 loss=10464.7852\n",
      "[NN] epoch 10/10 loss=9678.3076\n",
      "[NN] epoch 10/10 loss=10403.3223\n",
      "[NN] epoch 10/10 loss=10045.1904\n",
      "[NN] epoch 10/10 loss=10402.0059\n",
      "[NN] epoch 10/10 loss=10379.2070\n",
      "[NN] epoch 10/10 loss=9560.1973\n",
      "[NN] epoch 10/10 loss=10193.0645\n",
      "[NN] epoch 10/10 loss=9825.3076\n",
      "[NN] epoch 10/10 loss=9649.4258\n",
      "[NN] epoch 10/10 loss=9974.8232\n",
      "[NN] epoch 10/10 loss=10262.7217\n",
      "[NN] epoch 10/10 loss=10377.5098\n",
      "[NN] epoch 10/10 loss=10100.3994\n",
      "[NN] epoch 10/10 loss=9642.1016\n",
      "[NN] epoch 10/10 loss=10535.0029\n",
      "[NN] epoch 10/10 loss=10440.9150\n",
      "[NN] epoch 10/10 loss=9640.9248\n",
      "[NN] epoch 10/10 loss=10076.5850\n",
      "[NN] epoch 10/10 loss=9276.6895\n",
      "[NN] epoch 10/10 loss=10076.6719\n",
      "[NN] epoch 10/10 loss=10412.9375\n",
      "[NN] epoch 10/10 loss=10109.0205\n",
      "[NN] epoch 10/10 loss=10504.7451\n",
      "[NN] epoch 10/10 loss=10414.9062\n",
      "[NN] epoch 10/10 loss=9417.6650\n",
      "[NN] epoch 10/10 loss=10252.5410\n",
      "[NN] epoch 10/10 loss=10313.6738\n",
      "[NN] epoch 10/10 loss=9957.7773\n",
      "[NN] epoch 10/10 loss=10615.7256\n",
      "[NN] epoch 10/10 loss=9364.2383\n",
      "[NN] epoch 10/10 loss=9990.7695\n",
      "[NN] epoch 10/10 loss=10560.1523\n",
      "[NN] epoch 10/10 loss=9845.8613\n",
      "[NN] epoch 10/10 loss=10087.4600\n",
      "[NN] epoch 10/10 loss=10329.9873\n",
      "[NN] epoch 10/10 loss=9851.5781\n",
      "[NN] epoch 10/10 loss=10693.4414\n",
      "[NN] epoch 10/10 loss=9425.0879\n",
      "[NN] epoch 10/10 loss=10150.6270\n",
      "[NN] epoch 10/10 loss=10130.1973\n",
      "[NN] epoch 10/10 loss=10123.9922\n",
      "[NN] epoch 10/10 loss=10340.2725\n",
      "[NN] epoch 10/10 loss=9653.9473\n",
      "[NN] epoch 10/10 loss=9793.7188\n",
      "[NN] epoch 10/10 loss=9935.7705\n",
      "[NN] epoch 10/10 loss=10283.5645\n",
      "[NN] epoch 10/10 loss=9930.4092\n",
      "[NN] epoch 10/10 loss=10435.3232\n",
      "[NN] epoch 10/10 loss=10064.2412\n",
      "[NN] epoch 10/10 loss=10397.3223\n",
      "[NN] epoch 10/10 loss=10015.2100\n",
      "[NN] epoch 10/10 loss=10155.0957\n",
      "[NN] epoch 10/10 loss=9722.2510\n",
      "[NN] epoch 10/10 loss=10171.7383\n",
      "[NN] epoch 10/10 loss=10272.4082\n",
      "[NN] epoch 10/10 loss=9584.8379\n",
      "[NN] epoch 10/10 loss=10473.0898\n",
      "[NN] epoch 10/10 loss=9882.7969\n",
      "[NN] epoch 10/10 loss=9395.8652\n",
      "[NN] epoch 10/10 loss=10582.3955\n",
      "[NN] epoch 10/10 loss=10158.7578\n",
      "[NN] epoch 10/10 loss=9859.3604\n",
      "[NN] epoch 10/10 loss=10381.2510\n",
      "[NN] epoch 10/10 loss=10600.4902\n",
      "[NN] epoch 10/10 loss=9580.8867\n",
      "[NN] epoch 10/10 loss=9957.7148\n",
      "[NN] epoch 10/10 loss=10209.9990\n",
      "[NN] epoch 10/10 loss=10031.1211\n",
      "[NN] epoch 10/10 loss=9401.2988\n",
      "[NN] epoch 10/10 loss=10252.0723\n",
      "[NN] epoch 10/10 loss=10576.3916\n",
      "[NN] epoch 10/10 loss=9822.0840\n",
      "[NN] epoch 10/10 loss=10304.5117\n",
      "[NN] epoch 10/10 loss=10306.1699\n",
      "[NN] epoch 10/10 loss=10079.3652\n",
      "[NN] epoch 10/10 loss=10182.9961\n",
      "[NN] epoch 10/10 loss=9445.6895\n",
      "[NN] epoch 10/10 loss=10360.6328\n",
      "[NN] epoch 10/10 loss=10225.9336\n",
      "[NN] epoch 10/10 loss=10661.6377\n",
      "[NN] epoch 10/10 loss=9647.6191\n",
      "[NN] epoch 10/10 loss=9805.8115\n",
      "[NN] epoch 10/10 loss=10049.4229\n",
      "[NN] epoch 10/10 loss=10442.1709\n",
      "[NN] epoch 10/10 loss=9306.6357\n",
      "[NN] epoch 10/10 loss=9874.1699\n",
      "[NN] epoch 10/10 loss=10091.7148\n",
      "[NN] epoch 10/10 loss=10631.6406\n",
      "[NN] epoch 10/10 loss=10154.7705\n",
      "[NN] epoch 10/10 loss=9993.1211\n",
      "[NN] epoch 10/10 loss=9914.3848\n",
      "[NN] epoch 10/10 loss=10180.5215\n",
      "[NN] epoch 10/10 loss=10114.2021\n",
      "[NN] epoch 10/10 loss=9666.4707\n",
      "[NN] epoch 10/10 loss=10172.9688\n",
      "[NN] epoch 10/10 loss=9768.3389\n",
      "[NN] epoch 10/10 loss=10590.9756\n",
      "[NN] epoch 10/10 loss=10176.1592\n",
      "[NN] epoch 10/10 loss=10357.5244\n",
      "[NN] epoch 10/10 loss=9891.2383\n",
      "[NN] epoch 10/10 loss=10087.8564\n",
      "[NN] epoch 10/10 loss=9903.2061\n",
      "[NN] epoch 10/10 loss=10115.3760\n",
      "[NN] epoch 10/10 loss=10160.5176\n",
      "[NN] epoch 10/10 loss=9915.8555\n",
      "[NN] epoch 10/10 loss=10598.7539\n",
      "[NN] epoch 10/10 loss=9565.7539\n",
      "[NN] epoch 10/10 loss=10132.6143\n",
      "[NN] epoch 10/10 loss=9742.4482\n",
      "[NN] epoch 10/10 loss=9841.2158\n",
      "[NN] epoch 10/10 loss=10036.2480\n",
      "[NN] epoch 10/10 loss=10441.8887\n",
      "[NN] epoch 10/10 loss=10312.3262\n",
      "[NN] epoch 10/10 loss=10097.8496\n",
      "[NN] epoch 10/10 loss=9713.8154\n",
      "[NN] epoch 10/10 loss=10170.7227\n",
      "[NN] epoch 10/10 loss=9689.3496\n",
      "[NN] epoch 10/10 loss=10674.9775\n",
      "[NN] epoch 10/10 loss=10128.7070\n",
      "[NN] epoch 10/10 loss=10264.1680\n",
      "[NN] epoch 10/10 loss=9821.9180\n",
      "[NN] epoch 10/10 loss=10092.5801\n",
      "[NN] epoch 10/10 loss=10070.1611\n",
      "[NN] epoch 10/10 loss=10165.6514\n",
      "[NN] epoch 10/10 loss=10444.7227\n",
      "[NN] epoch 10/10 loss=9552.7041\n",
      "[NN] epoch 10/10 loss=9816.1465\n",
      "[NN] epoch 10/10 loss=10370.3818\n",
      "[NN] epoch 10/10 loss=10760.7695\n",
      "[NN] epoch 10/10 loss=9748.3623\n",
      "[NN] epoch 10/10 loss=9900.5205\n",
      "[NN] epoch 10/10 loss=9748.5352\n",
      "[NN] epoch 10/10 loss=10201.3740\n",
      "[NN] epoch 10/10 loss=9832.3936\n",
      "[NN] epoch 10/10 loss=10321.6357\n",
      "[NN] epoch 10/10 loss=10240.3809\n",
      "[NN] epoch 10/10 loss=9810.8926\n",
      "[NN] epoch 10/10 loss=10171.4404\n",
      "[NN] epoch 10/10 loss=10397.1084\n",
      "[NN] epoch 10/10 loss=9920.3682\n",
      "[NN] epoch 10/10 loss=10093.0342\n",
      "[NN] epoch 10/10 loss=10325.1025\n",
      "[NN] epoch 10/10 loss=9612.5283\n",
      "[NN] epoch 10/10 loss=9846.9102\n",
      "[NN] epoch 10/10 loss=9905.3984\n",
      "[NN] epoch 10/10 loss=10132.7275\n",
      "[NN] epoch 10/10 loss=10022.7305\n",
      "[NN] epoch 10/10 loss=10450.9600\n",
      "[NN] epoch 10/10 loss=10270.8057\n",
      "[NN] epoch 10/10 loss=9913.6533\n",
      "[NN] epoch 10/10 loss=10067.7188\n",
      "[NN] epoch 10/10 loss=9484.9502\n",
      "[NN] epoch 10/10 loss=10643.2217\n",
      "[NN] epoch 10/10 loss=9869.0078\n",
      "[NN] epoch 10/10 loss=10250.8945\n",
      "[NN] epoch 10/10 loss=9963.8613\n",
      "[NN] epoch 10/10 loss=10204.1914\n",
      "[NN] epoch 10/10 loss=10050.7803\n",
      "[NN] epoch 10/10 loss=10417.6064\n",
      "[NN] epoch 10/10 loss=9701.8516\n",
      "[NN] epoch 10/10 loss=10148.6406\n",
      "[NN] epoch 10/10 loss=9789.1289\n",
      "[NN] epoch 10/10 loss=10310.2676\n",
      "[NN] epoch 10/10 loss=9620.3066\n",
      "[NN] epoch 10/10 loss=10226.4268\n",
      "[NN] epoch 10/10 loss=9735.4043\n",
      "[NN] epoch 10/10 loss=10060.6572\n",
      "[NN] epoch 10/10 loss=10745.8477\n",
      "[NN] epoch 10/10 loss=10772.8652\n",
      "[NN] epoch 10/10 loss=9886.6475\n",
      "[NN] epoch 10/10 loss=10427.7607\n",
      "[NN] epoch 10/10 loss=9940.1016\n",
      "[NN] epoch 10/10 loss=9325.1650\n",
      "[NN] epoch 10/10 loss=9822.6934\n",
      "[NN] epoch 10/10 loss=10251.6602\n",
      "[NN] epoch 10/10 loss=10185.5068\n",
      "[NN] epoch 10/10 loss=9397.8506\n",
      "[NN] epoch 10/10 loss=10712.2031\n",
      "[NN] epoch 10/10 loss=10163.8740\n",
      "[NN] epoch 10/10 loss=9891.1514\n",
      "[NN] epoch 10/10 loss=10344.4111\n",
      "[NN] epoch 10/10 loss=9983.0625\n",
      "[NN] epoch 10/10 loss=9999.8496\n",
      "[NN] epoch 10/10 loss=10271.8457\n",
      "[NN] epoch 10/10 loss=9646.5391\n",
      "[NN] epoch 10/10 loss=10415.0098\n",
      "[NN] epoch 10/10 loss=10178.8604\n",
      "[NN] epoch 10/10 loss=9848.9287\n",
      "[NN] epoch 10/10 loss=9548.8545\n",
      "[NN] epoch 10/10 loss=10541.7871\n",
      "[NN] epoch 10/10 loss=9780.5156\n",
      "[NN] epoch 10/10 loss=10221.4697\n",
      "[NN] epoch 10/10 loss=10276.7422\n",
      "[NN] epoch 10/10 loss=10363.4111\n",
      "[NN] epoch 10/10 loss=9431.4756\n",
      "[NN] epoch 10/10 loss=10291.7188\n",
      "[NN] epoch 10/10 loss=10240.0908\n",
      "[NN] epoch 10/10 loss=10051.2070\n",
      "[NN] epoch 10/10 loss=9982.4932\n",
      "[NN] epoch 10/10 loss=10075.7695\n",
      "[NN] epoch 10/10 loss=10128.3896\n",
      "[NN] epoch 10/10 loss=10303.2803\n",
      "[NN] epoch 10/10 loss=9871.6973\n",
      "[NN] epoch 10/10 loss=10068.8467\n",
      "[NN] epoch 10/10 loss=10360.5947\n",
      "[NN] epoch 10/10 loss=9866.0820\n",
      "[NN] epoch 10/10 loss=10285.3691\n",
      "[NN] epoch 10/10 loss=9780.2148\n",
      "[NN] epoch 10/10 loss=10086.4854\n",
      "[NN] epoch 10/10 loss=9927.8682\n",
      "[NN] epoch 10/10 loss=10051.6797\n",
      "[NN] epoch 10/10 loss=10605.4834\n",
      "[NN] epoch 10/10 loss=9704.7031\n",
      "[NN] epoch 10/10 loss=10597.2100\n",
      "[NN] epoch 10/10 loss=9334.3486\n",
      "[NN] epoch 10/10 loss=10425.7188\n",
      "[NN] epoch 10/10 loss=9900.0879\n",
      "[NN] epoch 10/10 loss=10134.1094\n",
      "[NN] epoch 10/10 loss=9880.9727\n",
      "[NN] epoch 10/10 loss=9984.6865\n",
      "[NN] epoch 10/10 loss=10848.6846\n",
      "[NN] epoch 10/10 loss=9952.9004\n",
      "[NN] epoch 10/10 loss=9719.8115\n",
      "[NN] epoch 10/10 loss=9855.7354\n",
      "[NN] epoch 10/10 loss=10368.3506\n",
      "[NN] epoch 10/10 loss=9700.7617\n",
      "[NN] epoch 10/10 loss=9707.3721\n",
      "[NN] epoch 10/10 loss=10718.6904\n",
      "[Linear] mean value = 76.83159504412426 std = 2.232722308924553\n",
      "[NN]     mean value = 81.21547348877851 std = 6.6990611570507735e-15\n"
     ]
    }
   ],
   "source": [
    "X, A, R, meta = actg_to_arrays(df, keep_R_pos=False, R_log=False, neg_scal=True)\n",
    "\n",
    "lin_res = repeated_cv_value(\n",
    "    lambda Xtr,Atr,Rtr,K: build_predict_ad_linear(Xtr,Atr,Rtr,K,alpha=1.0),\n",
    "    X, A, R, K=meta[\"K\"], n_repeats=100, base_seed=2025, verbose=True,\n",
    "    standardize=True, cont_idx=meta[\"cont_idx\"]\n",
    ")\n",
    "nn_res  = repeated_cv_value(\n",
    "    lambda Xtr,Atr,Rtr,K: build_predict_ad_nn(Xtr,Atr,Rtr,K,epochs=10,lr=1e-3),\n",
    "    X, A, R, K=meta[\"K\"], n_repeats=100, base_seed=2025, verbose=True,\n",
    "    standardize=True, cont_idx=meta[\"cont_idx\"]\n",
    ")\n",
    "print(\"[Linear] mean value =\", lin_res[\"mean\"], \"std =\", lin_res[\"std\"])\n",
    "print(\"[NN]     mean value =\", nn_res[\"mean\"],  \"std =\", nn_res[\"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment 0: 532 patients (24.87%)\n",
      "Treatment 1: 522 patients (24.40%)\n",
      "Treatment 2: 524 patients (24.50%)\n",
      "Treatment 3: 561 patients (26.23%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X, A, R, meta = actg_to_arrays(df, keep_R_pos=False, R_log=False, neg_scal=True)\n",
    "\n",
    "# 치료 그룹 분포\n",
    "unique, counts = np.unique(A, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Treatment {u}: {c} patients ({c/len(A):.2%})\")\n",
    "\n",
    "plt.bar(unique, counts, tick_label=[f\"A={u}\" for u in unique])\n",
    "plt.title(\"Distribution of Treatment Assignments (A)\")\n",
    "plt.xlabel(\"Treatment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Summary Statistics\n",
      "n=2139, mean=56.79, std=84.51, min=0.00, max=860.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(\"R Summary Statistics\")\n",
    "print(f\"n={len(R)}, mean={np.mean(R):.2f}, std={np.std(R):.2f}, \"\n",
    "      f\"min={np.min(R):.2f}, max={np.max(R):.2f}\")\n",
    "\n",
    "# 히스토그램\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(R, bins=40, kde=True)\n",
    "plt.title(\"Distribution of Reward (R = cd420 - cd40)\")\n",
    "plt.xlabel(\"Reward R\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Summary Statistics\n",
      "n=1176, mean=102.15, std=91.69, min=1.00, max=860.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, A, R, meta = actg_to_arrays(df, keep_R_pos=True, R_log=False, neg_scal=False)\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"R Summary Statistics\")\n",
    "print(f\"n={len(R)}, mean={np.mean(R):.2f}, std={np.std(R):.2f}, \"\n",
    "      f\"min={np.min(R):.2f}, max={np.max(R):.2f}\")\n",
    "\n",
    "# 히스토그램\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(R, bins=40, kde=True)\n",
    "plt.title(\"Distribution of Reward (R = cd420 - cd40)\")\n",
    "plt.xlabel(\"Reward R\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
